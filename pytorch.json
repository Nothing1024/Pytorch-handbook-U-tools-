[{"name": "torch.is_tensor", "type": "torch", "path": "stable/generated/torch.is_tensor.html#torch.is_tensor", "desc": "Returns True if obj is a PyTorch tensor."}, {"name": "torch.is_storage", "type": "torch", "path": "stable/generated/torch.is_storage.html#torch.is_storage", "desc": "Returns True if obj is a PyTorch storage object."}, {"name": "torch.is_complex", "type": "torch", "path": "stable/generated/torch.is_complex.html#torch.is_complex", "desc": "Returns True if the data type of input is a complex data type i.e., one of torch.complex64 and torch.complex128 "}, {"name": "torch.is_conj", "type": "torch", "path": "stable/generated/torch.is_conj.html#torch.is_conj", "desc": "Returns True if the input is a conjugated tensor, i.e. its conjugate bit is set to True "}, {"name": "torch.is_floating_point", "type": "torch", "path": "stable/generated/torch.is_floating_point.html#torch.is_floating_point", "desc": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64 torch.float32 torch.float16 and torch.bfloat16 "}, {"name": "torch.is_nonzero", "type": "torch", "path": "stable/generated/torch.is_nonzero.html#torch.is_nonzero", "desc": "Returns True if the input is a single element tensor which is not equal to zero after type conversions."}, {"name": "torch.set_default_dtype", "type": "torch", "path": "stable/generated/torch.set_default_dtype.html#torch.set_default_dtype", "desc": "Sets the default floating point dtype to d "}, {"name": "torch.get_default_dtype", "type": "torch", "path": "stable/generated/torch.get_default_dtype.html#torch.get_default_dtype", "desc": "Get the current default floating point torch.dtype "}, {"name": "torch.set_default_tensor_type", "type": "torch", "path": "stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type", "desc": "Sets the default torch.Tensor type to floating point tensor type t "}, {"name": "torch.numel", "type": "torch", "path": "stable/generated/torch.numel.html#torch.numel", "desc": "Returns the total number of elements in the input tensor."}, {"name": "torch.set_printoptions", "type": "torch", "path": "stable/generated/torch.set_printoptions.html#torch.set_printoptions", "desc": "Set options for printing."}, {"name": "torch.set_flush_denormal", "type": "torch", "path": "stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal", "desc": "Disables denormal floating numbers on CPU."}, {"name": "torch.tensor", "type": "torch", "path": "stable/generated/torch.tensor.html#torch.tensor", "desc": "Constructs a tensor with no autograd history (also known as a \u201cleaf tensor\u201d, see Autograd mechanics ) by copying data "}, {"name": "torch.sparse_coo_tensor", "type": "torch", "path": "stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor", "desc": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices "}, {"name": "torch.asarray", "type": "torch", "path": "stable/generated/torch.asarray.html#torch.asarray", "desc": "Converts obj to a tensor."}, {"name": "torch.as_tensor", "type": "torch", "path": "stable/generated/torch.as_tensor.html#torch.as_tensor", "desc": "Converts data into a tensor, sharing data and preserving autograd history if possible."}, {"name": "torch.as_strided", "type": "torch", "path": "stable/generated/torch.as_strided.html#torch.as_strided", "desc": "Create a view of an existing torch.Tensor input with specified size stride and storage_offset "}, {"name": "torch.from_numpy", "type": "torch", "path": "stable/generated/torch.from_numpy.html#torch.from_numpy", "desc": "Creates a Tensor from a numpy.ndarray "}, {"name": "torch.frombuffer", "type": "torch", "path": "stable/generated/torch.frombuffer.html#torch.frombuffer", "desc": "Creates a 1-dimensional Tensor from an object that implements the Python buffer protocol."}, {"name": "torch.zeros", "type": "torch", "path": "stable/generated/torch.zeros.html#torch.zeros", "desc": "Returns a tensor filled with the scalar value 0 with the shape defined by the variable argument size "}, {"name": "torch.zeros_like", "type": "torch", "path": "stable/generated/torch.zeros_like.html#torch.zeros_like", "desc": "Returns a tensor filled with the scalar value 0 with the same size as input "}, {"name": "torch.ones", "type": "torch", "path": "stable/generated/torch.ones.html#torch.ones", "desc": "Returns a tensor filled with the scalar value 1 with the shape defined by the variable argument size "}, {"name": "torch.ones_like", "type": "torch", "path": "stable/generated/torch.ones_like.html#torch.ones_like", "desc": "Returns a tensor filled with the scalar value 1 with the same size as input "}, {"name": "torch.arange", "type": "torch", "path": "stable/generated/torch.arange.html#torch.arange", "desc": "Returns a 1-D tensor of size \u2308 end \u2212 start step \u2309 \\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil \u2308 step end \u2212 start \u200b \u2309 with values from the interval [start, end) taken with common difference step beginning from start "}, {"name": "torch.range", "type": "torch", "path": "stable/generated/torch.range.html#torch.range", "desc": "Returns a 1-D tensor of size \u230a end \u2212 start step \u230b + 1 \\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1 \u230a step end \u2212 start \u200b \u230b + 1 with values from start to end with step step "}, {"name": "torch.linspace", "type": "torch", "path": "stable/generated/torch.linspace.html#torch.linspace", "desc": "Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end inclusive."}, {"name": "torch.logspace", "type": "torch", "path": "stable/generated/torch.logspace.html#torch.logspace", "desc": "Creates a one-dimensional tensor of size steps whose values are evenly spaced from base start {{\\text{{base}}}}^{{\\text{{start}}}} base start to base end {{\\text{{base}}}}^{{\\text{{end}}}} base end inclusive, on a logarithmic scale with base base "}, {"name": "torch.eye", "type": "torch", "path": "stable/generated/torch.eye.html#torch.eye", "desc": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere."}, {"name": "torch.empty", "type": "torch", "path": "stable/generated/torch.empty.html#torch.empty", "desc": "Returns a tensor filled with uninitialized data."}, {"name": "torch.empty_like", "type": "torch", "path": "stable/generated/torch.empty_like.html#torch.empty_like", "desc": "Returns an uninitialized tensor with the same size as input "}, {"name": "torch.empty_strided", "type": "torch", "path": "stable/generated/torch.empty_strided.html#torch.empty_strided", "desc": "Creates a tensor with the specified size and stride and filled with undefined data."}, {"name": "torch.full", "type": "torch", "path": "stable/generated/torch.full.html#torch.full", "desc": "Creates a tensor of size size filled with fill_value "}, {"name": "torch.full_like", "type": "torch", "path": "stable/generated/torch.full_like.html#torch.full_like", "desc": "Returns a tensor with the same size as input filled with fill_value "}, {"name": "torch.quantize_per_tensor", "type": "torch", "path": "stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor", "desc": "Converts a float tensor to a quantized tensor with given scale and zero point."}, {"name": "torch.quantize_per_channel", "type": "torch", "path": "stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel", "desc": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points."}, {"name": "torch.dequantize", "type": "torch", "path": "stable/generated/torch.dequantize.html#torch.dequantize", "desc": "Returns an fp32 Tensor by dequantizing a quantized Tensor"}, {"name": "torch.complex", "type": "torch", "path": "stable/generated/torch.complex.html#torch.complex", "desc": "Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag "}, {"name": "torch.polar", "type": "torch", "path": "stable/generated/torch.polar.html#torch.polar", "desc": "Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle "}, {"name": "torch.heaviside", "type": "torch", "path": "stable/generated/torch.heaviside.html#torch.heaviside", "desc": "Computes the Heaviside step function for each element in input "}, {"name": "torch.adjoint", "type": "torch", "path": "stable/generated/torch.adjoint.html#torch.adjoint", "desc": "Returns a view of the tensor conjugated and with the last two dimensions transposed."}, {"name": "torch.argwhere", "type": "torch", "path": "stable/generated/torch.argwhere.html#torch.argwhere", "desc": "Returns a tensor containing the indices of all non-zero elements of input "}, {"name": "torch.cat", "type": "torch", "path": "stable/generated/torch.cat.html#torch.cat", "desc": "Concatenates the given sequence of seq tensors in the given dimension."}, {"name": "torch.concat", "type": "torch", "path": "stable/generated/torch.concat.html#torch.concat", "desc": "Alias of torch.cat() "}, {"name": "torch.conj", "type": "torch", "path": "stable/generated/torch.conj.html#torch.conj", "desc": "Returns a view of input with a flipped conjugate bit."}, {"name": "torch.chunk", "type": "torch", "path": "stable/generated/torch.chunk.html#torch.chunk", "desc": "Attempts to split a tensor into the specified number of chunks."}, {"name": "torch.dsplit", "type": "torch", "path": "stable/generated/torch.dsplit.html#torch.dsplit", "desc": "Splits input a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections "}, {"name": "torch.column_stack", "type": "torch", "path": "stable/generated/torch.column_stack.html#torch.column_stack", "desc": "Creates a new tensor by horizontally stacking the tensors in tensors "}, {"name": "torch.dstack", "type": "torch", "path": "stable/generated/torch.dstack.html#torch.dstack", "desc": "Stack tensors in sequence depthwise (along third axis)."}, {"name": "torch.gather", "type": "torch", "path": "stable/generated/torch.gather.html#torch.gather", "desc": "Gathers values along an axis specified by dim "}, {"name": "torch.hsplit", "type": "torch", "path": "stable/generated/torch.hsplit.html#torch.hsplit", "desc": "Splits input a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections "}, {"name": "torch.hstack", "type": "torch", "path": "stable/generated/torch.hstack.html#torch.hstack", "desc": "Stack tensors in sequence horizontally (column wise)."}, {"name": "torch.index_add", "type": "torch", "path": "stable/generated/torch.index_add.html#torch.index_add", "desc": "See index_add_() for function description."}, {"name": "torch.index_select", "type": "torch", "path": "stable/generated/torch.index_select.html#torch.index_select", "desc": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor "}, {"name": "torch.masked_select", "type": "torch", "path": "stable/generated/torch.masked_select.html#torch.masked_select", "desc": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor "}, {"name": "torch.movedim", "type": "torch", "path": "stable/generated/torch.movedim.html#torch.movedim", "desc": "Moves the dimension(s) of input at the position(s) in source to the position(s) in destination "}, {"name": "torch.moveaxis", "type": "torch", "path": "stable/generated/torch.moveaxis.html#torch.moveaxis", "desc": "Alias for torch.movedim() "}, {"name": "torch.narrow", "type": "torch", "path": "stable/generated/torch.narrow.html#torch.narrow", "desc": "Returns a new tensor that is a narrowed version of input tensor."}, {"name": "torch.nonzero", "type": "torch", "path": "stable/generated/torch.nonzero.html#torch.nonzero", "desc": ""}, {"name": "torch.permute", "type": "torch", "path": "stable/generated/torch.permute.html#torch.permute", "desc": "Returns a view of the original tensor input with its dimensions permuted."}, {"name": "torch.reshape", "type": "torch", "path": "stable/generated/torch.reshape.html#torch.reshape", "desc": "Returns a tensor with the same data and number of elements as input but with the specified shape."}, {"name": "torch.row_stack", "type": "torch", "path": "stable/generated/torch.row_stack.html#torch.row_stack", "desc": "Alias of torch.vstack() "}, {"name": "torch.select", "type": "torch", "path": "stable/generated/torch.select.html#torch.select", "desc": "Slices the input tensor along the selected dimension at the given index."}, {"name": "torch.scatter", "type": "torch", "path": "stable/generated/torch.scatter.html#torch.scatter", "desc": "Out-of-place version of torch.Tensor.scatter_()"}, {"name": "torch.diagonal_scatter", "type": "torch", "path": "stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter", "desc": "Embeds the values of the src tensor into input along the diagonal elements of input with respect to dim1 and dim2 "}, {"name": "torch.select_scatter", "type": "torch", "path": "stable/generated/torch.select_scatter.html#torch.select_scatter", "desc": "Embeds the values of the src tensor into input at the given index."}, {"name": "torch.slice_scatter", "type": "torch", "path": "stable/generated/torch.slice_scatter.html#torch.slice_scatter", "desc": "Embeds the values of the src tensor into input at the given dimension."}, {"name": "torch.scatter_add", "type": "torch", "path": "stable/generated/torch.scatter_add.html#torch.scatter_add", "desc": "Out-of-place version of torch.Tensor.scatter_add_()"}, {"name": "torch.scatter_reduce", "type": "torch", "path": "stable/generated/torch.scatter_reduce.html#torch.scatter_reduce", "desc": "Reduces all values from the input tensor to the indices specified in the index tensor."}, {"name": "torch.split", "type": "torch", "path": "stable/generated/torch.split.html#torch.split", "desc": "Splits the tensor into chunks."}, {"name": "torch.squeeze", "type": "torch", "path": "stable/generated/torch.squeeze.html#torch.squeeze", "desc": "Returns a tensor with all the dimensions of input of size 1 removed."}, {"name": "torch.stack", "type": "torch", "path": "stable/generated/torch.stack.html#torch.stack", "desc": "Concatenates a sequence of tensors along a new dimension."}, {"name": "torch.swapaxes", "type": "torch", "path": "stable/generated/torch.swapaxes.html#torch.swapaxes", "desc": "Alias for torch.transpose() "}, {"name": "torch.swapdims", "type": "torch", "path": "stable/generated/torch.swapdims.html#torch.swapdims", "desc": "Alias for torch.transpose() "}, {"name": "torch.t", "type": "torch", "path": "stable/generated/torch.t.html#torch.t", "desc": "Expects input to be <= 2-D tensor and transposes dimensions 0 and 1."}, {"name": "torch.take", "type": "torch", "path": "stable/generated/torch.take.html#torch.take", "desc": "Returns a new tensor with the elements of input at the given indices."}, {"name": "torch.take_along_dim", "type": "torch", "path": "stable/generated/torch.take_along_dim.html#torch.take_along_dim", "desc": "Selects values from input at the 1-dimensional indices from indices along the given dim "}, {"name": "torch.tensor_split", "type": "torch", "path": "stable/generated/torch.tensor_split.html#torch.tensor_split", "desc": "Splits a tensor into multiple sub-tensors, all of which are views of input along dimension dim according to the indices or number of sections specified by indices_or_sections "}, {"name": "torch.tile", "type": "torch", "path": "stable/generated/torch.tile.html#torch.tile", "desc": "Constructs a tensor by repeating the elements of input "}, {"name": "torch.transpose", "type": "torch", "path": "stable/generated/torch.transpose.html#torch.transpose", "desc": "Returns a tensor that is a transposed version of input "}, {"name": "torch.unbind", "type": "torch", "path": "stable/generated/torch.unbind.html#torch.unbind", "desc": "Removes a tensor dimension."}, {"name": "torch.unsqueeze", "type": "torch", "path": "stable/generated/torch.unsqueeze.html#torch.unsqueeze", "desc": "Returns a new tensor with a dimension of size one inserted at the specified position."}, {"name": "torch.vsplit", "type": "torch", "path": "stable/generated/torch.vsplit.html#torch.vsplit", "desc": "Splits input a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections "}, {"name": "torch.vstack", "type": "torch", "path": "stable/generated/torch.vstack.html#torch.vstack", "desc": "Stack tensors in sequence vertically (row wise)."}, {"name": "torch.where", "type": "torch", "path": "stable/generated/torch.where.html#torch.where", "desc": "Return a tensor of elements selected from either x or y depending on condition "}, {"name": "torch.Generator", "type": "torch", "path": "stable/generated/torch.Generator.html#torch.Generator", "desc": "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers."}, {"name": "torch.seed", "type": "torch", "path": "stable/generated/torch.seed.html#torch.seed", "desc": "Sets the seed for generating random numbers to a non-deterministic random number."}, {"name": "torch.manual_seed", "type": "torch", "path": "stable/generated/torch.manual_seed.html#torch.manual_seed", "desc": "Sets the seed for generating random numbers."}, {"name": "torch.initial_seed", "type": "torch", "path": "stable/generated/torch.initial_seed.html#torch.initial_seed", "desc": "Returns the initial seed for generating random numbers as a Python long "}, {"name": "torch.get_rng_state", "type": "torch", "path": "stable/generated/torch.get_rng_state.html#torch.get_rng_state", "desc": "Returns the random number generator state as a torch.ByteTensor "}, {"name": "torch.set_rng_state", "type": "torch", "path": "stable/generated/torch.set_rng_state.html#torch.set_rng_state", "desc": "Sets the random number generator state."}, {"name": "torch.bernoulli", "type": "torch", "path": "stable/generated/torch.bernoulli.html#torch.bernoulli", "desc": "Draws binary random numbers (0 or 1) from a Bernoulli distribution."}, {"name": "torch.multinomial", "type": "torch", "path": "stable/generated/torch.multinomial.html#torch.multinomial", "desc": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input "}, {"name": "torch.normal", "type": "torch", "path": "stable/generated/torch.normal.html#torch.normal", "desc": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given."}, {"name": "torch.poisson", "type": "torch", "path": "stable/generated/torch.poisson.html#torch.poisson", "desc": "Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,"}, {"name": "torch.rand", "type": "torch", "path": "stable/generated/torch.rand.html#torch.rand", "desc": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0 1 ) [0, 1) [ 0 1 )"}, {"name": "torch.rand_like", "type": "torch", "path": "stable/generated/torch.rand_like.html#torch.rand_like", "desc": "Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [ 0 1 ) [0, 1) [ 0 1 ) "}, {"name": "torch.randint", "type": "torch", "path": "stable/generated/torch.randint.html#torch.randint", "desc": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive)."}, {"name": "torch.randint_like", "type": "torch", "path": "stable/generated/torch.randint_like.html#torch.randint_like", "desc": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive)."}, {"name": "torch.randn", "type": "torch", "path": "stable/generated/torch.randn.html#torch.randn", "desc": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution)."}, {"name": "torch.randn_like", "type": "torch", "path": "stable/generated/torch.randn_like.html#torch.randn_like", "desc": "Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1."}, {"name": "torch.randperm", "type": "torch", "path": "stable/generated/torch.randperm.html#torch.randperm", "desc": "Returns a random permutation of integers from 0 to n - 1 "}, {"name": "torch.quasirandom.SobolEngine", "type": "torch", "path": "stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine", "desc": "The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences."}, {"name": "torch.save", "type": "torch", "path": "stable/generated/torch.save.html#torch.save", "desc": "Saves an object to a disk file."}, {"name": "torch.load", "type": "torch", "path": "stable/generated/torch.load.html#torch.load", "desc": "Loads an object saved with torch.save() from a file."}, {"name": "torch.get_num_threads", "type": "torch", "path": "stable/generated/torch.get_num_threads.html#torch.get_num_threads", "desc": "Returns the number of threads used for parallelizing CPU operations"}, {"name": "torch.set_num_threads", "type": "torch", "path": "stable/generated/torch.set_num_threads.html#torch.set_num_threads", "desc": "Sets the number of threads used for intraop parallelism on CPU."}, {"name": "torch.get_num_interop_threads", "type": "torch", "path": "stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads", "desc": "Returns the number of threads used for inter-op parallelism on CPU (e.g."}, {"name": "torch.set_num_interop_threads", "type": "torch", "path": "stable/generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads", "desc": "Sets the number of threads used for interop parallelism (e.g."}, {"name": "torch.no_grad", "type": "torch", "path": "stable/generated/torch.no_grad.html#torch.no_grad", "desc": "Context-manager that disabled gradient calculation."}, {"name": "torch.enable_grad", "type": "torch", "path": "stable/generated/torch.enable_grad.html#torch.enable_grad", "desc": "Context-manager that enables gradient calculation."}, {"name": "torch.set_grad_enabled", "type": "torch", "path": "stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled", "desc": "Context-manager that sets gradient calculation to on or off."}, {"name": "torch.is_grad_enabled", "type": "torch", "path": "stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled", "desc": "Returns True if grad mode is currently enabled."}, {"name": "torch.inference_mode", "type": "torch", "path": "stable/generated/torch.inference_mode.html#torch.inference_mode", "desc": "Context-manager that enables or disables inference mode"}, {"name": "torch.is_inference_mode_enabled", "type": "torch", "path": "stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled", "desc": "Returns True if inference mode is currently enabled."}, {"name": "torch.abs", "type": "torch", "path": "stable/generated/torch.abs.html#torch.abs", "desc": "Computes the absolute value of each element in input "}, {"name": "torch.absolute", "type": "torch", "path": "stable/generated/torch.absolute.html#torch.absolute", "desc": "Alias for torch.abs()"}, {"name": "torch.acos", "type": "torch", "path": "stable/generated/torch.acos.html#torch.acos", "desc": "Computes the inverse cosine of each element in input "}, {"name": "torch.arccos", "type": "torch", "path": "stable/generated/torch.arccos.html#torch.arccos", "desc": "Alias for torch.acos() "}, {"name": "torch.acosh", "type": "torch", "path": "stable/generated/torch.acosh.html#torch.acosh", "desc": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input "}, {"name": "torch.arccosh", "type": "torch", "path": "stable/generated/torch.arccosh.html#torch.arccosh", "desc": "Alias for torch.acosh() "}, {"name": "torch.add", "type": "torch", "path": "stable/generated/torch.add.html#torch.add", "desc": "Adds other scaled by alpha to input "}, {"name": "torch.addcdiv", "type": "torch", "path": "stable/generated/torch.addcdiv.html#torch.addcdiv", "desc": "Performs the element-wise division of tensor1 by tensor2 multiply the result by the scalar value and add it to input "}, {"name": "torch.addcmul", "type": "torch", "path": "stable/generated/torch.addcmul.html#torch.addcmul", "desc": "Performs the element-wise multiplication of tensor1 by tensor2 multiply the result by the scalar value and add it to input "}, {"name": "torch.angle", "type": "torch", "path": "stable/generated/torch.angle.html#torch.angle", "desc": "Computes the element-wise angle (in radians) of the given input tensor."}, {"name": "torch.asin", "type": "torch", "path": "stable/generated/torch.asin.html#torch.asin", "desc": "Returns a new tensor with the arcsine of the elements of input "}, {"name": "torch.arcsin", "type": "torch", "path": "stable/generated/torch.arcsin.html#torch.arcsin", "desc": "Alias for torch.asin() "}, {"name": "torch.asinh", "type": "torch", "path": "stable/generated/torch.asinh.html#torch.asinh", "desc": "Returns a new tensor with the inverse hyperbolic sine of the elements of input "}, {"name": "torch.arcsinh", "type": "torch", "path": "stable/generated/torch.arcsinh.html#torch.arcsinh", "desc": "Alias for torch.asinh() "}, {"name": "torch.atan", "type": "torch", "path": "stable/generated/torch.atan.html#torch.atan", "desc": "Returns a new tensor with the arctangent of the elements of input "}, {"name": "torch.arctan", "type": "torch", "path": "stable/generated/torch.arctan.html#torch.arctan", "desc": "Alias for torch.atan() "}, {"name": "torch.atanh", "type": "torch", "path": "stable/generated/torch.atanh.html#torch.atanh", "desc": "Returns a new tensor with the inverse hyperbolic tangent of the elements of input "}, {"name": "torch.arctanh", "type": "torch", "path": "stable/generated/torch.arctanh.html#torch.arctanh", "desc": "Alias for torch.atanh() "}, {"name": "torch.atan2", "type": "torch", "path": "stable/generated/torch.atan2.html#torch.atan2", "desc": "Element-wise arctangent of input i / other i \\text{input}_{i} / \\text{other}_{i} input i \u200b / other i \u200b with consideration of the quadrant."}, {"name": "torch.arctan2", "type": "torch", "path": "stable/generated/torch.arctan2.html#torch.arctan2", "desc": "Alias for torch.atan2() "}, {"name": "torch.bitwise_not", "type": "torch", "path": "stable/generated/torch.bitwise_not.html#torch.bitwise_not", "desc": "Computes the bitwise NOT of the given input tensor."}, {"name": "torch.bitwise_and", "type": "torch", "path": "stable/generated/torch.bitwise_and.html#torch.bitwise_and", "desc": "Computes the bitwise AND of input and other "}, {"name": "torch.bitwise_or", "type": "torch", "path": "stable/generated/torch.bitwise_or.html#torch.bitwise_or", "desc": "Computes the bitwise OR of input and other "}, {"name": "torch.bitwise_xor", "type": "torch", "path": "stable/generated/torch.bitwise_xor.html#torch.bitwise_xor", "desc": "Computes the bitwise XOR of input and other "}, {"name": "torch.bitwise_left_shift", "type": "torch", "path": "stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift", "desc": "Computes the left arithmetic shift of input by other bits."}, {"name": "torch.bitwise_right_shift", "type": "torch", "path": "stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift", "desc": "Computes the right arithmetic shift of input by other bits."}, {"name": "torch.ceil", "type": "torch", "path": "stable/generated/torch.ceil.html#torch.ceil", "desc": "Returns a new tensor with the ceil of the elements of input the smallest integer greater than or equal to each element."}, {"name": "torch.clamp", "type": "torch", "path": "stable/generated/torch.clamp.html#torch.clamp", "desc": "Clamps all elements in input into the range [ min max ] "}, {"name": "torch.clip", "type": "torch", "path": "stable/generated/torch.clip.html#torch.clip", "desc": "Alias for torch.clamp() "}, {"name": "torch.conj_physical", "type": "torch", "path": "stable/generated/torch.conj_physical.html#torch.conj_physical", "desc": "Computes the element-wise conjugate of the given input tensor."}, {"name": "torch.copysign", "type": "torch", "path": "stable/generated/torch.copysign.html#torch.copysign", "desc": "Create a new floating-point tensor with the magnitude of input and the sign of other elementwise."}, {"name": "torch.cos", "type": "torch", "path": "stable/generated/torch.cos.html#torch.cos", "desc": "Returns a new tensor with the cosine of the elements of input "}, {"name": "torch.cosh", "type": "torch", "path": "stable/generated/torch.cosh.html#torch.cosh", "desc": "Returns a new tensor with the hyperbolic cosine of the elements of input "}, {"name": "torch.deg2rad", "type": "torch", "path": "stable/generated/torch.deg2rad.html#torch.deg2rad", "desc": "Returns a new tensor with each of the elements of input converted from angles in degrees to radians."}, {"name": "torch.div", "type": "torch", "path": "stable/generated/torch.div.html#torch.div", "desc": "Divides each element of the input input by the corresponding element of other "}, {"name": "torch.divide", "type": "torch", "path": "stable/generated/torch.divide.html#torch.divide", "desc": "Alias for torch.div() "}, {"name": "torch.digamma", "type": "torch", "path": "stable/generated/torch.digamma.html#torch.digamma", "desc": "Alias for torch.special.digamma() "}, {"name": "torch.erf", "type": "torch", "path": "stable/generated/torch.erf.html#torch.erf", "desc": "Alias for torch.special.erf() "}, {"name": "torch.erfc", "type": "torch", "path": "stable/generated/torch.erfc.html#torch.erfc", "desc": "Alias for torch.special.erfc() "}, {"name": "torch.erfinv", "type": "torch", "path": "stable/generated/torch.erfinv.html#torch.erfinv", "desc": "Alias for torch.special.erfinv() "}, {"name": "torch.exp", "type": "torch", "path": "stable/generated/torch.exp.html#torch.exp", "desc": "Returns a new tensor with the exponential of the elements of the input tensor input "}, {"name": "torch.exp2", "type": "torch", "path": "stable/generated/torch.exp2.html#torch.exp2", "desc": "Alias for torch.special.exp2() "}, {"name": "torch.expm1", "type": "torch", "path": "stable/generated/torch.expm1.html#torch.expm1", "desc": "Alias for torch.special.expm1() "}, {"name": "torch.fake_quantize_per_channel_affine", "type": "torch", "path": "stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine", "desc": "Returns a new tensor with the data in input fake quantized per channel using scale zero_point quant_min and quant_max across the channel specified by axis "}, {"name": "torch.fake_quantize_per_tensor_affine", "type": "torch", "path": "stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine", "desc": "Returns a new tensor with the data in input fake quantized using scale zero_point quant_min and quant_max "}, {"name": "torch.fix", "type": "torch", "path": "stable/generated/torch.fix.html#torch.fix", "desc": "Alias for torch.trunc()"}, {"name": "torch.float_power", "type": "torch", "path": "stable/generated/torch.float_power.html#torch.float_power", "desc": "Raises input to the power of exponent elementwise, in double precision."}, {"name": "torch.floor", "type": "torch", "path": "stable/generated/torch.floor.html#torch.floor", "desc": "Returns a new tensor with the floor of the elements of input the largest integer less than or equal to each element."}, {"name": "torch.floor_divide", "type": "torch", "path": "stable/generated/torch.floor_divide.html#torch.floor_divide", "desc": ""}, {"name": "torch.fmod", "type": "torch", "path": "stable/generated/torch.fmod.html#torch.fmod", "desc": "Applies C++\u2019s std::fmod entrywise."}, {"name": "torch.frac", "type": "torch", "path": "stable/generated/torch.frac.html#torch.frac", "desc": "Computes the fractional portion of each element in input "}, {"name": "torch.frexp", "type": "torch", "path": "stable/generated/torch.frexp.html#torch.frexp", "desc": "Decomposes input into mantissa and exponent tensors such that input = mantissa \u00d7 2 exponent \\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}} input = mantissa \u00d7 2 exponent "}, {"name": "torch.gradient", "type": "torch", "path": "stable/generated/torch.gradient.html#torch.gradient", "desc": "Estimates the gradient of a function g : R n \u2192 R g : \\mathbb{R}^n \\rightarrow \\mathbb{R} g : R n \u2192 R in one or more dimensions using the second-order accurate central differences method "}, {"name": "torch.imag", "type": "torch", "path": "stable/generated/torch.imag.html#torch.imag", "desc": "Returns a new tensor containing imaginary values of the self tensor."}, {"name": "torch.ldexp", "type": "torch", "path": "stable/generated/torch.ldexp.html#torch.ldexp", "desc": "Multiplies input by 2**:attr: other "}, {"name": "torch.lerp", "type": "torch", "path": "stable/generated/torch.lerp.html#torch.lerp", "desc": "Does a linear interpolation of two tensors start (given by input ) and end based on a scalar or tensor weight and returns the resulting out tensor."}, {"name": "torch.lgamma", "type": "torch", "path": "stable/generated/torch.lgamma.html#torch.lgamma", "desc": "Computes the natural logarithm of the absolute value of the gamma function on input "}, {"name": "torch.log", "type": "torch", "path": "stable/generated/torch.log.html#torch.log", "desc": "Returns a new tensor with the natural logarithm of the elements of input "}, {"name": "torch.log10", "type": "torch", "path": "stable/generated/torch.log10.html#torch.log10", "desc": "Returns a new tensor with the logarithm to the base 10 of the elements of input "}, {"name": "torch.log1p", "type": "torch", "path": "stable/generated/torch.log1p.html#torch.log1p", "desc": "Returns a new tensor with the natural logarithm of (1 + input )."}, {"name": "torch.log2", "type": "torch", "path": "stable/generated/torch.log2.html#torch.log2", "desc": "Returns a new tensor with the logarithm to the base 2 of the elements of input "}, {"name": "torch.logaddexp", "type": "torch", "path": "stable/generated/torch.logaddexp.html#torch.logaddexp", "desc": "Logarithm of the sum of exponentiations of the inputs."}, {"name": "torch.logaddexp2", "type": "torch", "path": "stable/generated/torch.logaddexp2.html#torch.logaddexp2", "desc": "Logarithm of the sum of exponentiations of the inputs in base-2."}, {"name": "torch.logical_and", "type": "torch", "path": "stable/generated/torch.logical_and.html#torch.logical_and", "desc": "Computes the element-wise logical AND of the given input tensors."}, {"name": "torch.logical_not", "type": "torch", "path": "stable/generated/torch.logical_not.html#torch.logical_not", "desc": "Computes the element-wise logical NOT of the given input tensor."}, {"name": "torch.logical_or", "type": "torch", "path": "stable/generated/torch.logical_or.html#torch.logical_or", "desc": "Computes the element-wise logical OR of the given input tensors."}, {"name": "torch.logical_xor", "type": "torch", "path": "stable/generated/torch.logical_xor.html#torch.logical_xor", "desc": "Computes the element-wise logical XOR of the given input tensors."}, {"name": "torch.logit", "type": "torch", "path": "stable/generated/torch.logit.html#torch.logit", "desc": "Alias for torch.special.logit() "}, {"name": "torch.hypot", "type": "torch", "path": "stable/generated/torch.hypot.html#torch.hypot", "desc": "Given the legs of a right triangle, return its hypotenuse."}, {"name": "torch.i0", "type": "torch", "path": "stable/generated/torch.i0.html#torch.i0", "desc": "Alias for torch.special.i0() "}, {"name": "torch.igamma", "type": "torch", "path": "stable/generated/torch.igamma.html#torch.igamma", "desc": "Alias for torch.special.gammainc() "}, {"name": "torch.igammac", "type": "torch", "path": "stable/generated/torch.igammac.html#torch.igammac", "desc": "Alias for torch.special.gammaincc() "}, {"name": "torch.mul", "type": "torch", "path": "stable/generated/torch.mul.html#torch.mul", "desc": "Multiplies input by other "}, {"name": "torch.multiply", "type": "torch", "path": "stable/generated/torch.multiply.html#torch.multiply", "desc": "Alias for torch.mul() "}, {"name": "torch.mvlgamma", "type": "torch", "path": "stable/generated/torch.mvlgamma.html#torch.mvlgamma", "desc": "Alias for torch.special.multigammaln() "}, {"name": "torch.nan_to_num", "type": "torch", "path": "stable/generated/torch.nan_to_num.html#torch.nan_to_num", "desc": "Replaces NaN positive infinity, and negative infinity values in input with the values specified by nan posinf and neginf respectively."}, {"name": "torch.neg", "type": "torch", "path": "stable/generated/torch.neg.html#torch.neg", "desc": "Returns a new tensor with the negative of the elements of input "}, {"name": "torch.negative", "type": "torch", "path": "stable/generated/torch.negative.html#torch.negative", "desc": "Alias for torch.neg()"}, {"name": "torch.nextafter", "type": "torch", "path": "stable/generated/torch.nextafter.html#torch.nextafter", "desc": "Return the next floating-point value after input towards other elementwise."}, {"name": "torch.polygamma", "type": "torch", "path": "stable/generated/torch.polygamma.html#torch.polygamma", "desc": "Alias for torch.special.polygamma() "}, {"name": "torch.positive", "type": "torch", "path": "stable/generated/torch.positive.html#torch.positive", "desc": "Returns input "}, {"name": "torch.pow", "type": "torch", "path": "stable/generated/torch.pow.html#torch.pow", "desc": "Takes the power of each element in input with exponent and returns a tensor with the result."}, {"name": "torch.quantized_batch_norm", "type": "torch", "path": "stable/generated/torch.quantized_batch_norm.html#torch.quantized_batch_norm", "desc": "Applies batch normalization on a 4D (NCHW) quantized tensor."}, {"name": "torch.quantized_max_pool1d", "type": "torch", "path": "stable/generated/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d", "desc": "Applies a 1D max pooling over an input quantized tensor composed of several input planes."}, {"name": "torch.quantized_max_pool2d", "type": "torch", "path": "stable/generated/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d", "desc": "Applies a 2D max pooling over an input quantized tensor composed of several input planes."}, {"name": "torch.rad2deg", "type": "torch", "path": "stable/generated/torch.rad2deg.html#torch.rad2deg", "desc": "Returns a new tensor with each of the elements of input converted from angles in radians to degrees."}, {"name": "torch.real", "type": "torch", "path": "stable/generated/torch.real.html#torch.real", "desc": "Returns a new tensor containing real values of the self tensor."}, {"name": "torch.reciprocal", "type": "torch", "path": "stable/generated/torch.reciprocal.html#torch.reciprocal", "desc": "Returns a new tensor with the reciprocal of the elements of input"}, {"name": "torch.remainder", "type": "torch", "path": "stable/generated/torch.remainder.html#torch.remainder", "desc": "Computes Python\u2019s modulus operation entrywise."}, {"name": "torch.round", "type": "torch", "path": "stable/generated/torch.round.html#torch.round", "desc": "Rounds elements of input to the nearest integer."}, {"name": "torch.rsqrt", "type": "torch", "path": "stable/generated/torch.rsqrt.html#torch.rsqrt", "desc": "Returns a new tensor with the reciprocal of the square-root of each of the elements of input "}, {"name": "torch.sigmoid", "type": "torch", "path": "stable/generated/torch.sigmoid.html#torch.sigmoid", "desc": "Alias for torch.special.expit() "}, {"name": "torch.sign", "type": "torch", "path": "stable/generated/torch.sign.html#torch.sign", "desc": "Returns a new tensor with the signs of the elements of input "}, {"name": "torch.sgn", "type": "torch", "path": "stable/generated/torch.sgn.html#torch.sgn", "desc": "This function is an extension of torch.sign() to complex tensors."}, {"name": "torch.signbit", "type": "torch", "path": "stable/generated/torch.signbit.html#torch.signbit", "desc": "Tests if each element of input has its sign bit set (is less than zero) or not."}, {"name": "torch.sin", "type": "torch", "path": "stable/generated/torch.sin.html#torch.sin", "desc": "Returns a new tensor with the sine of the elements of input "}, {"name": "torch.sinc", "type": "torch", "path": "stable/generated/torch.sinc.html#torch.sinc", "desc": "Alias for torch.special.sinc() "}, {"name": "torch.sinh", "type": "torch", "path": "stable/generated/torch.sinh.html#torch.sinh", "desc": "Returns a new tensor with the hyperbolic sine of the elements of input "}, {"name": "torch.sqrt", "type": "torch", "path": "stable/generated/torch.sqrt.html#torch.sqrt", "desc": "Returns a new tensor with the square-root of the elements of input "}, {"name": "torch.square", "type": "torch", "path": "stable/generated/torch.square.html#torch.square", "desc": "Returns a new tensor with the square of the elements of input "}, {"name": "torch.sub", "type": "torch", "path": "stable/generated/torch.sub.html#torch.sub", "desc": "Subtracts other scaled by alpha from input "}, {"name": "torch.subtract", "type": "torch", "path": "stable/generated/torch.subtract.html#torch.subtract", "desc": "Alias for torch.sub() "}, {"name": "torch.tan", "type": "torch", "path": "stable/generated/torch.tan.html#torch.tan", "desc": "Returns a new tensor with the tangent of the elements of input "}, {"name": "torch.tanh", "type": "torch", "path": "stable/generated/torch.tanh.html#torch.tanh", "desc": "Returns a new tensor with the hyperbolic tangent of the elements of input "}, {"name": "torch.true_divide", "type": "torch", "path": "stable/generated/torch.true_divide.html#torch.true_divide", "desc": "Alias for torch.div() with rounding_mode=None "}, {"name": "torch.trunc", "type": "torch", "path": "stable/generated/torch.trunc.html#torch.trunc", "desc": "Returns a new tensor with the truncated integer values of the elements of input "}, {"name": "torch.xlogy", "type": "torch", "path": "stable/generated/torch.xlogy.html#torch.xlogy", "desc": "Alias for torch.special.xlogy() "}, {"name": "torch.argmax", "type": "torch", "path": "stable/generated/torch.argmax.html#torch.argmax", "desc": "Returns the indices of the maximum value of all elements in the input tensor."}, {"name": "torch.argmin", "type": "torch", "path": "stable/generated/torch.argmin.html#torch.argmin", "desc": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension"}, {"name": "torch.amax", "type": "torch", "path": "stable/generated/torch.amax.html#torch.amax", "desc": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim "}, {"name": "torch.amin", "type": "torch", "path": "stable/generated/torch.amin.html#torch.amin", "desc": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim "}, {"name": "torch.aminmax", "type": "torch", "path": "stable/generated/torch.aminmax.html#torch.aminmax", "desc": "Computes the minimum and maximum values of the input tensor."}, {"name": "torch.all", "type": "torch", "path": "stable/generated/torch.all.html#torch.all", "desc": "Tests if all elements in input evaluate to True "}, {"name": "torch.any", "type": "torch", "path": "stable/generated/torch.any.html#torch.any", "desc": "Tests if any element in input evaluates to True "}, {"name": "torch.max", "type": "torch", "path": "stable/generated/torch.max.html#torch.max", "desc": "Returns the maximum value of all elements in the input tensor."}, {"name": "torch.min", "type": "torch", "path": "stable/generated/torch.min.html#torch.min", "desc": "Returns the minimum value of all elements in the input tensor."}, {"name": "torch.dist", "type": "torch", "path": "stable/generated/torch.dist.html#torch.dist", "desc": "Returns the p-norm of ( input - other )"}, {"name": "torch.logsumexp", "type": "torch", "path": "stable/generated/torch.logsumexp.html#torch.logsumexp", "desc": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim "}, {"name": "torch.mean", "type": "torch", "path": "stable/generated/torch.mean.html#torch.mean", "desc": "Returns the mean value of all elements in the input tensor."}, {"name": "torch.nanmean", "type": "torch", "path": "stable/generated/torch.nanmean.html#torch.nanmean", "desc": "Computes the mean of all non-NaN elements along the specified dimensions."}, {"name": "torch.median", "type": "torch", "path": "stable/generated/torch.median.html#torch.median", "desc": "Returns the median of the values in input "}, {"name": "torch.nanmedian", "type": "torch", "path": "stable/generated/torch.nanmedian.html#torch.nanmedian", "desc": "Returns the median of the values in input ignoring NaN values."}, {"name": "torch.mode", "type": "torch", "path": "stable/generated/torch.mode.html#torch.mode", "desc": "Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim i.e. a value which appears most often in that row, and indices is the index location of each mode value found."}, {"name": "torch.norm", "type": "torch", "path": "stable/generated/torch.norm.html#torch.norm", "desc": "Returns the matrix norm or vector norm of a given tensor."}, {"name": "torch.nansum", "type": "torch", "path": "stable/generated/torch.nansum.html#torch.nansum", "desc": "Returns the sum of all elements, treating Not a Numbers (NaNs) as zero."}, {"name": "torch.prod", "type": "torch", "path": "stable/generated/torch.prod.html#torch.prod", "desc": "Returns the product of all elements in the input tensor."}, {"name": "torch.quantile", "type": "torch", "path": "stable/generated/torch.quantile.html#torch.quantile", "desc": "Computes the q-th quantiles of each row of the input tensor along the dimension dim "}, {"name": "torch.nanquantile", "type": "torch", "path": "stable/generated/torch.nanquantile.html#torch.nanquantile", "desc": "This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist."}, {"name": "torch.std", "type": "torch", "path": "stable/generated/torch.std.html#torch.std", "desc": "If unbiased is True Bessel\u2019s correction will be used."}, {"name": "torch.std_mean", "type": "torch", "path": "stable/generated/torch.std_mean.html#torch.std_mean", "desc": "If unbiased is True Bessel\u2019s correction will be used to calculate the standard deviation."}, {"name": "torch.sum", "type": "torch", "path": "stable/generated/torch.sum.html#torch.sum", "desc": "Returns the sum of all elements in the input tensor."}, {"name": "torch.unique", "type": "torch", "path": "stable/generated/torch.unique.html#torch.unique", "desc": "Returns the unique elements of the input tensor."}, {"name": "torch.unique_consecutive", "type": "torch", "path": "stable/generated/torch.unique_consecutive.html#torch.unique_consecutive", "desc": "Eliminates all but the first element from every consecutive group of equivalent elements."}, {"name": "torch.var", "type": "torch", "path": "stable/generated/torch.var.html#torch.var", "desc": "If unbiased is True Bessel\u2019s correction will be used."}, {"name": "torch.var_mean", "type": "torch", "path": "stable/generated/torch.var_mean.html#torch.var_mean", "desc": "If unbiased is True Bessel\u2019s correction will be used to calculate the variance."}, {"name": "torch.count_nonzero", "type": "torch", "path": "stable/generated/torch.count_nonzero.html#torch.count_nonzero", "desc": "Counts the number of non-zero values in the tensor input along the given dim "}, {"name": "torch.allclose", "type": "torch", "path": "stable/generated/torch.allclose.html#torch.allclose", "desc": "This function checks if all input and other satisfy the condition:"}, {"name": "torch.argsort", "type": "torch", "path": "stable/generated/torch.argsort.html#torch.argsort", "desc": "Returns the indices that sort a tensor along a given dimension in ascending order by value."}, {"name": "torch.eq", "type": "torch", "path": "stable/generated/torch.eq.html#torch.eq", "desc": "Computes element-wise equality"}, {"name": "torch.equal", "type": "torch", "path": "stable/generated/torch.equal.html#torch.equal", "desc": "True if two tensors have the same size and elements, False otherwise."}, {"name": "torch.ge", "type": "torch", "path": "stable/generated/torch.ge.html#torch.ge", "desc": "Computes input \u2265 other \\text{input} \\geq \\text{other} input \u2265 other element-wise."}, {"name": "torch.greater_equal", "type": "torch", "path": "stable/generated/torch.greater_equal.html#torch.greater_equal", "desc": "Alias for torch.ge() "}, {"name": "torch.gt", "type": "torch", "path": "stable/generated/torch.gt.html#torch.gt", "desc": "Computes input > other \\text{input} > \\text{other} input > other element-wise."}, {"name": "torch.greater", "type": "torch", "path": "stable/generated/torch.greater.html#torch.greater", "desc": "Alias for torch.gt() "}, {"name": "torch.isclose", "type": "torch", "path": "stable/generated/torch.isclose.html#torch.isclose", "desc": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other "}, {"name": "torch.isfinite", "type": "torch", "path": "stable/generated/torch.isfinite.html#torch.isfinite", "desc": "Returns a new tensor with boolean elements representing if each element is finite or not."}, {"name": "torch.isin", "type": "torch", "path": "stable/generated/torch.isin.html#torch.isin", "desc": "Tests if each element of elements is in test_elements "}, {"name": "torch.isinf", "type": "torch", "path": "stable/generated/torch.isinf.html#torch.isinf", "desc": "Tests if each element of input is infinite (positive or negative infinity) or not."}, {"name": "torch.isposinf", "type": "torch", "path": "stable/generated/torch.isposinf.html#torch.isposinf", "desc": "Tests if each element of input is positive infinity or not."}, {"name": "torch.isneginf", "type": "torch", "path": "stable/generated/torch.isneginf.html#torch.isneginf", "desc": "Tests if each element of input is negative infinity or not."}, {"name": "torch.isnan", "type": "torch", "path": "stable/generated/torch.isnan.html#torch.isnan", "desc": "Returns a new tensor with boolean elements representing if each element of input is NaN or not."}, {"name": "torch.isreal", "type": "torch", "path": "stable/generated/torch.isreal.html#torch.isreal", "desc": "Returns a new tensor with boolean elements representing if each element of input is real-valued or not."}, {"name": "torch.kthvalue", "type": "torch", "path": "stable/generated/torch.kthvalue.html#torch.kthvalue", "desc": "Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim "}, {"name": "torch.le", "type": "torch", "path": "stable/generated/torch.le.html#torch.le", "desc": "Computes input \u2264 other \\text{input} \\leq \\text{other} input \u2264 other element-wise."}, {"name": "torch.less_equal", "type": "torch", "path": "stable/generated/torch.less_equal.html#torch.less_equal", "desc": "Alias for torch.le() "}, {"name": "torch.lt", "type": "torch", "path": "stable/generated/torch.lt.html#torch.lt", "desc": "Computes input < other \\text{input} < \\text{other} input < other element-wise."}, {"name": "torch.less", "type": "torch", "path": "stable/generated/torch.less.html#torch.less", "desc": "Alias for torch.lt() "}, {"name": "torch.maximum", "type": "torch", "path": "stable/generated/torch.maximum.html#torch.maximum", "desc": "Computes the element-wise maximum of input and other "}, {"name": "torch.minimum", "type": "torch", "path": "stable/generated/torch.minimum.html#torch.minimum", "desc": "Computes the element-wise minimum of input and other "}, {"name": "torch.fmax", "type": "torch", "path": "stable/generated/torch.fmax.html#torch.fmax", "desc": "Computes the element-wise maximum of input and other "}, {"name": "torch.fmin", "type": "torch", "path": "stable/generated/torch.fmin.html#torch.fmin", "desc": "Computes the element-wise minimum of input and other "}, {"name": "torch.ne", "type": "torch", "path": "stable/generated/torch.ne.html#torch.ne", "desc": "Computes input \u2260 other \\text{input} \\neq \\text{other} input \ue020 = other element-wise."}, {"name": "torch.not_equal", "type": "torch", "path": "stable/generated/torch.not_equal.html#torch.not_equal", "desc": "Alias for torch.ne() "}, {"name": "torch.sort", "type": "torch", "path": "stable/generated/torch.sort.html#torch.sort", "desc": "Sorts the elements of the input tensor along a given dimension in ascending order by value."}, {"name": "torch.topk", "type": "torch", "path": "stable/generated/torch.topk.html#torch.topk", "desc": "Returns the k largest elements of the given input tensor along a given dimension."}, {"name": "torch.msort", "type": "torch", "path": "stable/generated/torch.msort.html#torch.msort", "desc": "Sorts the elements of the input tensor along its first dimension in ascending order by value."}, {"name": "torch.stft", "type": "torch", "path": "stable/generated/torch.stft.html#torch.stft", "desc": "Short-time Fourier transform (STFT)."}, {"name": "torch.istft", "type": "torch", "path": "stable/generated/torch.istft.html#torch.istft", "desc": "Inverse short time Fourier Transform."}, {"name": "torch.bartlett_window", "type": "torch", "path": "stable/generated/torch.bartlett_window.html#torch.bartlett_window", "desc": "Bartlett window function."}, {"name": "torch.blackman_window", "type": "torch", "path": "stable/generated/torch.blackman_window.html#torch.blackman_window", "desc": "Blackman window function."}, {"name": "torch.hamming_window", "type": "torch", "path": "stable/generated/torch.hamming_window.html#torch.hamming_window", "desc": "Hamming window function."}, {"name": "torch.hann_window", "type": "torch", "path": "stable/generated/torch.hann_window.html#torch.hann_window", "desc": "Hann window function."}, {"name": "torch.kaiser_window", "type": "torch", "path": "stable/generated/torch.kaiser_window.html#torch.kaiser_window", "desc": "Computes the Kaiser window with window length window_length and shape parameter beta "}, {"name": "torch.atleast_1d", "type": "torch", "path": "stable/generated/torch.atleast_1d.html#torch.atleast_1d", "desc": "Returns a 1-dimensional view of each input tensor with zero dimensions."}, {"name": "torch.atleast_2d", "type": "torch", "path": "stable/generated/torch.atleast_2d.html#torch.atleast_2d", "desc": "Returns a 2-dimensional view of each input tensor with zero dimensions."}, {"name": "torch.atleast_3d", "type": "torch", "path": "stable/generated/torch.atleast_3d.html#torch.atleast_3d", "desc": "Returns a 3-dimensional view of each input tensor with zero dimensions."}, {"name": "torch.bincount", "type": "torch", "path": "stable/generated/torch.bincount.html#torch.bincount", "desc": "Count the frequency of each value in an array of non-negative ints."}, {"name": "torch.block_diag", "type": "torch", "path": "stable/generated/torch.block_diag.html#torch.block_diag", "desc": "Create a block diagonal matrix from provided tensors."}, {"name": "torch.broadcast_tensors", "type": "torch", "path": "stable/generated/torch.broadcast_tensors.html#torch.broadcast_tensors", "desc": "Broadcasts the given tensors according to Broadcasting semantics "}, {"name": "torch.broadcast_to", "type": "torch", "path": "stable/generated/torch.broadcast_to.html#torch.broadcast_to", "desc": "Broadcasts input to the shape shape "}, {"name": "torch.broadcast_shapes", "type": "torch", "path": "stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes", "desc": "Similar to broadcast_tensors() but for shapes."}, {"name": "torch.bucketize", "type": "torch", "path": "stable/generated/torch.bucketize.html#torch.bucketize", "desc": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries "}, {"name": "torch.cartesian_prod", "type": "torch", "path": "stable/generated/torch.cartesian_prod.html#torch.cartesian_prod", "desc": "Do cartesian product of the given sequence of tensors."}, {"name": "torch.cdist", "type": "torch", "path": "stable/generated/torch.cdist.html#torch.cdist", "desc": "Computes batched the p-norm distance between each pair of the two collections of row vectors."}, {"name": "torch.clone", "type": "torch", "path": "stable/generated/torch.clone.html#torch.clone", "desc": "Returns a copy of input "}, {"name": "torch.combinations", "type": "torch", "path": "stable/generated/torch.combinations.html#torch.combinations", "desc": "Compute combinations of length r r r of the given tensor."}, {"name": "torch.corrcoef", "type": "torch", "path": "stable/generated/torch.corrcoef.html#torch.corrcoef", "desc": "Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the input matrix, where rows are the variables and columns are the observations."}, {"name": "torch.cov", "type": "torch", "path": "stable/generated/torch.cov.html#torch.cov", "desc": "Estimates the covariance matrix of the variables given by the input matrix, where rows are the variables and columns are the observations."}, {"name": "torch.cross", "type": "torch", "path": "stable/generated/torch.cross.html#torch.cross", "desc": "Returns the cross product of vectors in dimension dim of input and other "}, {"name": "torch.cummax", "type": "torch", "path": "stable/generated/torch.cummax.html#torch.cummax", "desc": "Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim "}, {"name": "torch.cummin", "type": "torch", "path": "stable/generated/torch.cummin.html#torch.cummin", "desc": "Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim "}, {"name": "torch.cumprod", "type": "torch", "path": "stable/generated/torch.cumprod.html#torch.cumprod", "desc": "Returns the cumulative product of elements of input in the dimension dim "}, {"name": "torch.cumsum", "type": "torch", "path": "stable/generated/torch.cumsum.html#torch.cumsum", "desc": "Returns the cumulative sum of elements of input in the dimension dim "}, {"name": "torch.diag", "type": "torch", "path": "stable/generated/torch.diag.html#torch.diag", "desc": "If input is a vector (1-D tensor), then returns a 2-D square tensor"}, {"name": "torch.diag_embed", "type": "torch", "path": "stable/generated/torch.diag_embed.html#torch.diag_embed", "desc": "Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2 ) are filled by input "}, {"name": "torch.diagflat", "type": "torch", "path": "stable/generated/torch.diagflat.html#torch.diagflat", "desc": "If input is a vector (1-D tensor), then returns a 2-D square tensor"}, {"name": "torch.diagonal", "type": "torch", "path": "stable/generated/torch.diagonal.html#torch.diagonal", "desc": "Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape."}, {"name": "torch.diff", "type": "torch", "path": "stable/generated/torch.diff.html#torch.diff", "desc": "Computes the n-th forward difference along the given dimension."}, {"name": "torch.einsum", "type": "torch", "path": "stable/generated/torch.einsum.html#torch.einsum", "desc": "Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention."}, {"name": "torch.flatten", "type": "torch", "path": "stable/generated/torch.flatten.html#torch.flatten", "desc": "Flattens input by reshaping it into a one-dimensional tensor."}, {"name": "torch.flip", "type": "torch", "path": "stable/generated/torch.flip.html#torch.flip", "desc": "Reverse the order of a n-D tensor along given axis in dims."}, {"name": "torch.fliplr", "type": "torch", "path": "stable/generated/torch.fliplr.html#torch.fliplr", "desc": "Flip tensor in the left/right direction, returning a new tensor."}, {"name": "torch.flipud", "type": "torch", "path": "stable/generated/torch.flipud.html#torch.flipud", "desc": "Flip tensor in the up/down direction, returning a new tensor."}, {"name": "torch.kron", "type": "torch", "path": "stable/generated/torch.kron.html#torch.kron", "desc": "Computes the Kronecker product, denoted by \u2297 \\otimes \u2297 of input and other "}, {"name": "torch.rot90", "type": "torch", "path": "stable/generated/torch.rot90.html#torch.rot90", "desc": "Rotate a n-D tensor by 90 degrees in the plane specified by dims axis."}, {"name": "torch.gcd", "type": "torch", "path": "stable/generated/torch.gcd.html#torch.gcd", "desc": "Computes the element-wise greatest common divisor (GCD) of input and other "}, {"name": "torch.histc", "type": "torch", "path": "stable/generated/torch.histc.html#torch.histc", "desc": "Computes the histogram of a tensor."}, {"name": "torch.histogram", "type": "torch", "path": "stable/generated/torch.histogram.html#torch.histogram", "desc": "Computes a histogram of the values in a tensor."}, {"name": "torch.histogramdd", "type": "torch", "path": "stable/generated/torch.histogramdd.html#torch.histogramdd", "desc": "Computes a multi-dimensional histogram of the values in a tensor."}, {"name": "torch.meshgrid", "type": "torch", "path": "stable/generated/torch.meshgrid.html#torch.meshgrid", "desc": "Creates grids of coordinates specified by the 1D inputs in attr :tensors."}, {"name": "torch.lcm", "type": "torch", "path": "stable/generated/torch.lcm.html#torch.lcm", "desc": "Computes the element-wise least common multiple (LCM) of input and other "}, {"name": "torch.logcumsumexp", "type": "torch", "path": "stable/generated/torch.logcumsumexp.html#torch.logcumsumexp", "desc": "Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim "}, {"name": "torch.ravel", "type": "torch", "path": "stable/generated/torch.ravel.html#torch.ravel", "desc": "Return a contiguous flattened tensor."}, {"name": "torch.renorm", "type": "torch", "path": "stable/generated/torch.renorm.html#torch.renorm", "desc": "Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p -norm of the sub-tensor is lower than the value maxnorm"}, {"name": "torch.repeat_interleave", "type": "torch", "path": "stable/generated/torch.repeat_interleave.html#torch.repeat_interleave", "desc": "Repeat elements of a tensor."}, {"name": "torch.roll", "type": "torch", "path": "stable/generated/torch.roll.html#torch.roll", "desc": "Roll the tensor along the given dimension(s)."}, {"name": "torch.searchsorted", "type": "torch", "path": "stable/generated/torch.searchsorted.html#torch.searchsorted", "desc": "Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, when sorted, the order of the corresponding innermost dimension within sorted_sequence would be preserved."}, {"name": "torch.tensordot", "type": "torch", "path": "stable/generated/torch.tensordot.html#torch.tensordot", "desc": "Returns a contraction of a and b over multiple dimensions."}, {"name": "torch.trace", "type": "torch", "path": "stable/generated/torch.trace.html#torch.trace", "desc": "Returns the sum of the elements of the diagonal of the input 2-D matrix."}, {"name": "torch.tril", "type": "torch", "path": "stable/generated/torch.tril.html#torch.tril", "desc": "Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input the other elements of the result tensor out are set to 0."}, {"name": "torch.tril_indices", "type": "torch", "path": "stable/generated/torch.tril_indices.html#torch.tril_indices", "desc": "Returns the indices of the lower triangular part of a row -by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."}, {"name": "torch.triu", "type": "torch", "path": "stable/generated/torch.triu.html#torch.triu", "desc": "Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input the other elements of the result tensor out are set to 0."}, {"name": "torch.triu_indices", "type": "torch", "path": "stable/generated/torch.triu_indices.html#torch.triu_indices", "desc": "Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."}, {"name": "torch.vander", "type": "torch", "path": "stable/generated/torch.vander.html#torch.vander", "desc": "Generates a Vandermonde matrix."}, {"name": "torch.view_as_real", "type": "torch", "path": "stable/generated/torch.view_as_real.html#torch.view_as_real", "desc": "Returns a view of input as a real tensor."}, {"name": "torch.view_as_complex", "type": "torch", "path": "stable/generated/torch.view_as_complex.html#torch.view_as_complex", "desc": "Returns a view of input as a complex tensor."}, {"name": "torch.resolve_conj", "type": "torch", "path": "stable/generated/torch.resolve_conj.html#torch.resolve_conj", "desc": "Returns a new tensor with materialized conjugation if input \u2019s conjugate bit is set to True else returns input "}, {"name": "torch.resolve_neg", "type": "torch", "path": "stable/generated/torch.resolve_neg.html#torch.resolve_neg", "desc": "Returns a new tensor with materialized negation if input \u2019s negative bit is set to True else returns input "}, {"name": "torch.addbmm", "type": "torch", "path": "stable/generated/torch.addbmm.html#torch.addbmm", "desc": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2 with a reduced add step (all matrix multiplications get accumulated along the first dimension)."}, {"name": "torch.addmm", "type": "torch", "path": "stable/generated/torch.addmm.html#torch.addmm", "desc": "Performs a matrix multiplication of the matrices mat1 and mat2 "}, {"name": "torch.addmv", "type": "torch", "path": "stable/generated/torch.addmv.html#torch.addmv", "desc": "Performs a matrix-vector product of the matrix mat and the vector vec "}, {"name": "torch.addr", "type": "torch", "path": "stable/generated/torch.addr.html#torch.addr", "desc": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input "}, {"name": "torch.baddbmm", "type": "torch", "path": "stable/generated/torch.baddbmm.html#torch.baddbmm", "desc": "Performs a batch matrix-matrix product of matrices in batch1 and batch2 "}, {"name": "torch.bmm", "type": "torch", "path": "stable/generated/torch.bmm.html#torch.bmm", "desc": "Performs a batch matrix-matrix product of matrices stored in input and mat2 "}, {"name": "torch.chain_matmul", "type": "torch", "path": "stable/generated/torch.chain_matmul.html#torch.chain_matmul", "desc": "Returns the matrix product of the N N N 2-D tensors."}, {"name": "torch.cholesky", "type": "torch", "path": "stable/generated/torch.cholesky.html#torch.cholesky", "desc": "Computes the Cholesky decomposition of a symmetric positive-definite matrix A A A or for batches of symmetric positive-definite matrices."}, {"name": "torch.cholesky_inverse", "type": "torch", "path": "stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse", "desc": "Computes the inverse of a symmetric positive-definite matrix A A A using its Cholesky factor u u u : returns matrix inv "}, {"name": "torch.cholesky_solve", "type": "torch", "path": "stable/generated/torch.cholesky_solve.html#torch.cholesky_solve", "desc": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix u u u "}, {"name": "torch.dot", "type": "torch", "path": "stable/generated/torch.dot.html#torch.dot", "desc": "Computes the dot product of two 1D tensors."}, {"name": "torch.eig", "type": "torch", "path": "stable/generated/torch.eig.html#torch.eig", "desc": "Computes the eigenvalues and eigenvectors of a real square matrix."}, {"name": "torch.geqrf", "type": "torch", "path": "stable/generated/torch.geqrf.html#torch.geqrf", "desc": "This is a low-level function for calling LAPACK\u2019s geqrf directly."}, {"name": "torch.ger", "type": "torch", "path": "stable/generated/torch.ger.html#torch.ger", "desc": "Alias of torch.outer() "}, {"name": "torch.inner", "type": "torch", "path": "stable/generated/torch.inner.html#torch.inner", "desc": "Computes the dot product for 1D tensors."}, {"name": "torch.inverse", "type": "torch", "path": "stable/generated/torch.inverse.html#torch.inverse", "desc": "Alias for torch.linalg.inv()"}, {"name": "torch.det", "type": "torch", "path": "stable/generated/torch.det.html#torch.det", "desc": "Alias for torch.linalg.det()"}, {"name": "torch.logdet", "type": "torch", "path": "stable/generated/torch.logdet.html#torch.logdet", "desc": "Calculates log determinant of a square matrix or batches of square matrices."}, {"name": "torch.slogdet", "type": "torch", "path": "stable/generated/torch.slogdet.html#torch.slogdet", "desc": "Alias for torch.linalg.slogdet()"}, {"name": "torch.lstsq", "type": "torch", "path": "stable/generated/torch.lstsq.html#torch.lstsq", "desc": "Computes the solution to the least squares and least norm problems for a full rank matrix A A A of size ( m \u00d7 n ) (m \\times n) ( m \u00d7 n ) and a matrix B B B of size ( m \u00d7 k ) (m \\times k) ( m \u00d7 k ) "}, {"name": "torch.lu", "type": "torch", "path": "stable/generated/torch.lu.html#torch.lu", "desc": "Computes the LU factorization of a matrix or batches of matrices A "}, {"name": "torch.lu_solve", "type": "torch", "path": "stable/generated/torch.lu_solve.html#torch.lu_solve", "desc": "Returns the LU solve of the linear system A x = b Ax = b A x = b using the partially pivoted LU factorization of A from torch.lu() "}, {"name": "torch.lu_unpack", "type": "torch", "path": "stable/generated/torch.lu_unpack.html#torch.lu_unpack", "desc": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu() "}, {"name": "torch.matmul", "type": "torch", "path": "stable/generated/torch.matmul.html#torch.matmul", "desc": "Matrix product of two tensors."}, {"name": "torch.matrix_power", "type": "torch", "path": "stable/generated/torch.matrix_power.html#torch.matrix_power", "desc": "Alias for torch.linalg.matrix_power()"}, {"name": "torch.matrix_rank", "type": "torch", "path": "stable/generated/torch.matrix_rank.html#torch.matrix_rank", "desc": "Returns the numerical rank of a 2-D tensor."}, {"name": "torch.matrix_exp", "type": "torch", "path": "stable/generated/torch.matrix_exp.html#torch.matrix_exp", "desc": "Alias for torch.linalg.matrix_exp() "}, {"name": "torch.mm", "type": "torch", "path": "stable/generated/torch.mm.html#torch.mm", "desc": "Performs a matrix multiplication of the matrices input and mat2 "}, {"name": "torch.mv", "type": "torch", "path": "stable/generated/torch.mv.html#torch.mv", "desc": "Performs a matrix-vector product of the matrix input and the vector vec "}, {"name": "torch.orgqr", "type": "torch", "path": "stable/generated/torch.orgqr.html#torch.orgqr", "desc": "Alias for torch.linalg.householder_product() "}, {"name": "torch.ormqr", "type": "torch", "path": "stable/generated/torch.ormqr.html#torch.ormqr", "desc": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix."}, {"name": "torch.outer", "type": "torch", "path": "stable/generated/torch.outer.html#torch.outer", "desc": "Outer product of input and vec2 "}, {"name": "torch.pinverse", "type": "torch", "path": "stable/generated/torch.pinverse.html#torch.pinverse", "desc": "Alias for torch.linalg.pinv()"}, {"name": "torch.qr", "type": "torch", "path": "stable/generated/torch.qr.html#torch.qr", "desc": "Computes the QR decomposition of a matrix or a batch of matrices input and returns a namedtuple (Q, R) of tensors such that input = Q R \\text{input} = Q R input = QR with Q Q Q being an orthogonal matrix or batch of orthogonal matrices and R R R being an upper triangular matrix or batch of upper triangular matrices."}, {"name": "torch.solve", "type": "torch", "path": "stable/generated/torch.solve.html#torch.solve", "desc": "This function returns the solution to the system of linear equations represented by A X = B AX = B A X = B and the LU factorization of A, in order as a namedtuple solution, LU "}, {"name": "torch.svd", "type": "torch", "path": "stable/generated/torch.svd.html#torch.svd", "desc": "Computes the singular value decomposition of either a matrix or batch of matrices input "}, {"name": "torch.svd_lowrank", "type": "torch", "path": "stable/generated/torch.svd_lowrank.html#torch.svd_lowrank", "desc": "Return the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix A A A such that A \u2248 U d i a g ( S ) V T A \\approx U diag(S) V^T A \u2248 U d ia g ( S ) V T "}, {"name": "torch.pca_lowrank", "type": "torch", "path": "stable/generated/torch.pca_lowrank.html#torch.pca_lowrank", "desc": "Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix."}, {"name": "torch.symeig", "type": "torch", "path": "stable/generated/torch.symeig.html#torch.symeig", "desc": "This function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix input or a batch thereof, represented by a namedtuple (eigenvalues, eigenvectors)."}, {"name": "torch.lobpcg", "type": "torch", "path": "stable/generated/torch.lobpcg.html#torch.lobpcg", "desc": "Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods."}, {"name": "torch.trapz", "type": "torch", "path": "stable/generated/torch.trapz.html#torch.trapz", "desc": "Alias for torch.trapezoid() "}, {"name": "torch.trapezoid", "type": "torch", "path": "stable/generated/torch.trapezoid.html#torch.trapezoid", "desc": "Computes the trapezoidal rule along dim "}, {"name": "torch.cumulative_trapezoid", "type": "torch", "path": "stable/generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid", "desc": "Cumulatively computes the trapezoidal rule along dim "}, {"name": "torch.triangular_solve", "type": "torch", "path": "stable/generated/torch.triangular_solve.html#torch.triangular_solve", "desc": "Solves a system of equations with a square upper or lower triangular invertible matrix A A A and multiple right-hand sides b b b "}, {"name": "torch.vdot", "type": "torch", "path": "stable/generated/torch.vdot.html#torch.vdot", "desc": "Computes the dot product of two 1D tensors."}, {"name": "torch.compiled_with_cxx11_abi", "type": "torch", "path": "stable/generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi", "desc": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"}, {"name": "torch.result_type", "type": "torch", "path": "stable/generated/torch.result_type.html#torch.result_type", "desc": "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors."}, {"name": "torch.can_cast", "type": "torch", "path": "stable/generated/torch.can_cast.html#torch.can_cast", "desc": "Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation "}, {"name": "torch.promote_types", "type": "torch", "path": "stable/generated/torch.promote_types.html#torch.promote_types", "desc": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2 "}, {"name": "torch.use_deterministic_algorithms", "type": "torch", "path": "stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms", "desc": "Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms."}, {"name": "torch.are_deterministic_algorithms_enabled", "type": "torch", "path": "stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled", "desc": "Returns True if the global deterministic flag is turned on."}, {"name": "torch.is_deterministic_algorithms_warn_only_enabled", "type": "torch", "path": "stable/generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled", "desc": "Returns True if the global deterministic flag is set to warn only."}, {"name": "torch.set_deterministic_debug_mode", "type": "torch", "path": "stable/generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode", "desc": "Sets the debug mode for deterministic operations."}, {"name": "torch.get_deterministic_debug_mode", "type": "torch", "path": "stable/generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode", "desc": "Returns the current value of the debug mode for deterministic operations."}, {"name": "torch.set_warn_always", "type": "torch", "path": "stable/generated/torch.set_warn_always.html#torch.set_warn_always", "desc": "When this flag is False (default) then some PyTorch warnings may only appear once per process."}, {"name": "torch.is_warn_always_enabled", "type": "torch", "path": "stable/generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled", "desc": "Returns True if the global warn_always flag is turned on."}, {"name": "torch._assert", "type": "torch", "path": "stable/generated/torch._assert.html#torch._assert", "desc": "A wrapper around Python\u2019s assert which is symbolically traceable."}, {"name": "torch.nn.parameter.Parameter", "type": "torch.nn", "path": "stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter", "desc": "A kind of Tensor that is to be considered a module parameter."}, {"name": "torch.nn.parameter.UninitializedParameter", "type": "torch.nn", "path": "stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter", "desc": "A parameter that is not initialized."}, {"name": "torch.nn.parameter.UninitializedBuffer", "type": "torch.nn", "path": "stable/generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer", "desc": "A buffer that is not initialized."}, {"name": "torch.nn.Module", "type": "torch.nn", "path": "stable/generated/torch.nn.Module.html#torch.nn.Module", "desc": "Base class for all neural network modules."}, {"name": "torch.nn.Sequential", "type": "torch.nn", "path": "stable/generated/torch.nn.Sequential.html#torch.nn.Sequential", "desc": "A sequential container."}, {"name": "torch.nn.ModuleList", "type": "torch.nn", "path": "stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList", "desc": "Holds submodules in a list."}, {"name": "torch.nn.ModuleDict", "type": "torch.nn", "path": "stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict", "desc": "Holds submodules in a dictionary."}, {"name": "torch.nn.ParameterList", "type": "torch.nn", "path": "stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList", "desc": "Holds parameters in a list."}, {"name": "torch.nn.ParameterDict", "type": "torch.nn", "path": "stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict", "desc": "Holds parameters in a dictionary."}, {"name": "torch.nn.modules.module.register_module_forward_pre_hook", "type": "torch.nn", "path": "stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook", "desc": "Registers a forward pre-hook common to all modules."}, {"name": "torch.nn.modules.module.register_module_forward_hook", "type": "torch.nn", "path": "stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook", "desc": "Registers a global forward hook for all the modules"}, {"name": "torch.nn.modules.module.register_module_backward_hook", "type": "torch.nn", "path": "stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook", "desc": "Registers a backward hook common to all the modules."}, {"name": "torch.nn.modules.module.register_module_full_backward_hook", "type": "torch.nn", "path": "stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook", "desc": "Registers a backward hook common to all the modules."}, {"name": "torch.nn.Conv1d", "type": "torch.nn", "path": "stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d", "desc": "Applies a 1D convolution over an input signal composed of several input planes."}, {"name": "torch.nn.Conv2d", "type": "torch.nn", "path": "stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d", "desc": "Applies a 2D convolution over an input signal composed of several input planes."}, {"name": "torch.nn.Conv3d", "type": "torch.nn", "path": "stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d", "desc": "Applies a 3D convolution over an input signal composed of several input planes."}, {"name": "torch.nn.ConvTranspose1d", "type": "torch.nn", "path": "stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d", "desc": "Applies a 1D transposed convolution operator over an input image composed of several input planes."}, {"name": "torch.nn.ConvTranspose2d", "type": "torch.nn", "path": "stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d", "desc": "Applies a 2D transposed convolution operator over an input image composed of several input planes."}, {"name": "torch.nn.ConvTranspose3d", "type": "torch.nn", "path": "stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d", "desc": "Applies a 3D transposed convolution operator over an input image composed of several input planes."}, {"name": "torch.nn.LazyConv1d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d", "desc": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyConv2d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d", "desc": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyConv3d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d", "desc": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyConvTranspose1d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d", "desc": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyConvTranspose2d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d", "desc": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyConvTranspose3d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d", "desc": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1) "}, {"name": "torch.nn.Unfold", "type": "torch.nn", "path": "stable/generated/torch.nn.Unfold.html#torch.nn.Unfold", "desc": "Extracts sliding local blocks from a batched input tensor."}, {"name": "torch.nn.Fold", "type": "torch.nn", "path": "stable/generated/torch.nn.Fold.html#torch.nn.Fold", "desc": "Combines an array of sliding local blocks into a large containing tensor."}, {"name": "torch.nn.MaxPool1d", "type": "torch.nn", "path": "stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d", "desc": "Applies a 1D max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.MaxPool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d", "desc": "Applies a 2D max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.MaxPool3d", "type": "torch.nn", "path": "stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d", "desc": "Applies a 3D max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.MaxUnpool1d", "type": "torch.nn", "path": "stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d", "desc": "Computes a partial inverse of MaxPool1d "}, {"name": "torch.nn.MaxUnpool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d", "desc": "Computes a partial inverse of MaxPool2d "}, {"name": "torch.nn.MaxUnpool3d", "type": "torch.nn", "path": "stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d", "desc": "Computes a partial inverse of MaxPool3d "}, {"name": "torch.nn.AvgPool1d", "type": "torch.nn", "path": "stable/generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d", "desc": "Applies a 1D average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AvgPool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d", "desc": "Applies a 2D average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AvgPool3d", "type": "torch.nn", "path": "stable/generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d", "desc": "Applies a 3D average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.FractionalMaxPool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d", "desc": "Applies a 2D fractional max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.FractionalMaxPool3d", "type": "torch.nn", "path": "stable/generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d", "desc": "Applies a 3D fractional max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.LPPool1d", "type": "torch.nn", "path": "stable/generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d", "desc": "Applies a 1D power-average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.LPPool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d", "desc": "Applies a 2D power-average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AdaptiveMaxPool1d", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d", "desc": "Applies a 1D adaptive max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AdaptiveMaxPool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d", "desc": "Applies a 2D adaptive max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AdaptiveMaxPool3d", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d", "desc": "Applies a 3D adaptive max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AdaptiveAvgPool1d", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d", "desc": "Applies a 1D adaptive average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AdaptiveAvgPool2d", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d", "desc": "Applies a 2D adaptive average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.AdaptiveAvgPool3d", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d", "desc": "Applies a 3D adaptive average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.ReflectionPad1d", "type": "torch.nn", "path": "stable/generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d", "desc": "Pads the input tensor using the reflection of the input boundary."}, {"name": "torch.nn.ReflectionPad2d", "type": "torch.nn", "path": "stable/generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d", "desc": "Pads the input tensor using the reflection of the input boundary."}, {"name": "torch.nn.ReflectionPad3d", "type": "torch.nn", "path": "stable/generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d", "desc": "Pads the input tensor using the reflection of the input boundary."}, {"name": "torch.nn.ReplicationPad1d", "type": "torch.nn", "path": "stable/generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d", "desc": "Pads the input tensor using replication of the input boundary."}, {"name": "torch.nn.ReplicationPad2d", "type": "torch.nn", "path": "stable/generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d", "desc": "Pads the input tensor using replication of the input boundary."}, {"name": "torch.nn.ReplicationPad3d", "type": "torch.nn", "path": "stable/generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d", "desc": "Pads the input tensor using replication of the input boundary."}, {"name": "torch.nn.ZeroPad2d", "type": "torch.nn", "path": "stable/generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d", "desc": "Pads the input tensor boundaries with zero."}, {"name": "torch.nn.ConstantPad1d", "type": "torch.nn", "path": "stable/generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d", "desc": "Pads the input tensor boundaries with a constant value."}, {"name": "torch.nn.ConstantPad2d", "type": "torch.nn", "path": "stable/generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d", "desc": "Pads the input tensor boundaries with a constant value."}, {"name": "torch.nn.ConstantPad3d", "type": "torch.nn", "path": "stable/generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d", "desc": "Pads the input tensor boundaries with a constant value."}, {"name": "torch.nn.ELU", "type": "torch.nn", "path": "stable/generated/torch.nn.ELU.html#torch.nn.ELU", "desc": "Applies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) "}, {"name": "torch.nn.Hardshrink", "type": "torch.nn", "path": "stable/generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink", "desc": "Applies the Hard Shrinkage (Hardshrink) function element-wise."}, {"name": "torch.nn.Hardsigmoid", "type": "torch.nn", "path": "stable/generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid", "desc": "Applies the Hardsigmoid function element-wise."}, {"name": "torch.nn.Hardtanh", "type": "torch.nn", "path": "stable/generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh", "desc": "Applies the HardTanh function element-wise."}, {"name": "torch.nn.Hardswish", "type": "torch.nn", "path": "stable/generated/torch.nn.Hardswish.html#torch.nn.Hardswish", "desc": "Applies the hardswish function, element-wise, as described in the paper:"}, {"name": "torch.nn.LeakyReLU", "type": "torch.nn", "path": "stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.LogSigmoid", "type": "torch.nn", "path": "stable/generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.MultiheadAttention", "type": "torch.nn", "path": "stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention", "desc": "Allows the model to jointly attend to information from different representation subspaces as described in the paper: Attention Is All You Need "}, {"name": "torch.nn.PReLU", "type": "torch.nn", "path": "stable/generated/torch.nn.PReLU.html#torch.nn.PReLU", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.ReLU", "type": "torch.nn", "path": "stable/generated/torch.nn.ReLU.html#torch.nn.ReLU", "desc": "Applies the rectified linear unit function element-wise:"}, {"name": "torch.nn.ReLU6", "type": "torch.nn", "path": "stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.RReLU", "type": "torch.nn", "path": "stable/generated/torch.nn.RReLU.html#torch.nn.RReLU", "desc": "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:"}, {"name": "torch.nn.SELU", "type": "torch.nn", "path": "stable/generated/torch.nn.SELU.html#torch.nn.SELU", "desc": "Applied element-wise, as:"}, {"name": "torch.nn.CELU", "type": "torch.nn", "path": "stable/generated/torch.nn.CELU.html#torch.nn.CELU", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.GELU", "type": "torch.nn", "path": "stable/generated/torch.nn.GELU.html#torch.nn.GELU", "desc": "Applies the Gaussian Error Linear Units function:"}, {"name": "torch.nn.Sigmoid", "type": "torch.nn", "path": "stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.SiLU", "type": "torch.nn", "path": "stable/generated/torch.nn.SiLU.html#torch.nn.SiLU", "desc": "Applies the Sigmoid Linear Unit (SiLU) function, element-wise."}, {"name": "torch.nn.Mish", "type": "torch.nn", "path": "stable/generated/torch.nn.Mish.html#torch.nn.Mish", "desc": "Applies the Mish function, element-wise."}, {"name": "torch.nn.Softplus", "type": "torch.nn", "path": "stable/generated/torch.nn.Softplus.html#torch.nn.Softplus", "desc": "Applies the Softplus function Softplus ( x ) = 1 \u03b2 \u2217 log \u2061 ( 1 + exp \u2061 ( \u03b2 \u2217 x ) ) \\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) Softplus ( x ) = \u03b2 1 \u200b \u2217 lo g ( 1 + exp ( \u03b2 \u2217 x )) element-wise."}, {"name": "torch.nn.Softshrink", "type": "torch.nn", "path": "stable/generated/torch.nn.Softshrink.html#torch.nn.Softshrink", "desc": "Applies the soft shrinkage function elementwise:"}, {"name": "torch.nn.Softsign", "type": "torch.nn", "path": "stable/generated/torch.nn.Softsign.html#torch.nn.Softsign", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.Tanh", "type": "torch.nn", "path": "stable/generated/torch.nn.Tanh.html#torch.nn.Tanh", "desc": "Applies the Hyperbolic Tangent (Tanh) function element-wise."}, {"name": "torch.nn.Tanhshrink", "type": "torch.nn", "path": "stable/generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink", "desc": "Applies the element-wise function:"}, {"name": "torch.nn.Threshold", "type": "torch.nn", "path": "stable/generated/torch.nn.Threshold.html#torch.nn.Threshold", "desc": "Thresholds each element of the input Tensor."}, {"name": "torch.nn.GLU", "type": "torch.nn", "path": "stable/generated/torch.nn.GLU.html#torch.nn.GLU", "desc": "Applies the gated linear unit function G L U ( a b ) = a \u2297 \u03c3 ( b ) {GLU}(a, b)= a \\otimes \\sigma(b) G LU ( a b ) = a \u2297 \u03c3 ( b ) where a a a is the first half of the input matrices and b b b is the second half."}, {"name": "torch.nn.Softmin", "type": "torch.nn", "path": "stable/generated/torch.nn.Softmin.html#torch.nn.Softmin", "desc": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1."}, {"name": "torch.nn.Softmax", "type": "torch.nn", "path": "stable/generated/torch.nn.Softmax.html#torch.nn.Softmax", "desc": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1."}, {"name": "torch.nn.Softmax2d", "type": "torch.nn", "path": "stable/generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d", "desc": "Applies SoftMax over features to each spatial location."}, {"name": "torch.nn.LogSoftmax", "type": "torch.nn", "path": "stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax", "desc": "Applies the log \u2061 ( Softmax ( x ) ) \\log(\\text{Softmax}(x)) lo g ( Softmax ( x )) function to an n-dimensional input Tensor."}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss", "desc": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou "}, {"name": "torch.nn.BatchNorm1d", "type": "torch.nn", "path": "stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d", "desc": "Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift "}, {"name": "torch.nn.BatchNorm2d", "type": "torch.nn", "path": "stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d", "desc": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift "}, {"name": "torch.nn.BatchNorm3d", "type": "torch.nn", "path": "stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d", "desc": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift "}, {"name": "torch.nn.LazyBatchNorm1d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d", "desc": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyBatchNorm2d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d", "desc": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyBatchNorm3d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d", "desc": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1) "}, {"name": "torch.nn.GroupNorm", "type": "torch.nn", "path": "stable/generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm", "desc": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization"}, {"name": "torch.nn.SyncBatchNorm", "type": "torch.nn", "path": "stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm", "desc": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift "}, {"name": "torch.nn.InstanceNorm1d", "type": "torch.nn", "path": "stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d", "desc": "Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization "}, {"name": "torch.nn.InstanceNorm2d", "type": "torch.nn", "path": "stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d", "desc": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization "}, {"name": "torch.nn.InstanceNorm3d", "type": "torch.nn", "path": "stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d", "desc": "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization "}, {"name": "torch.nn.LazyInstanceNorm1d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d", "desc": "A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyInstanceNorm2d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d", "desc": "A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1) "}, {"name": "torch.nn.LazyInstanceNorm3d", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d", "desc": "A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1) "}, {"name": "torch.nn.LayerNorm", "type": "torch.nn", "path": "stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm", "desc": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization"}, {"name": "torch.nn.LocalResponseNorm", "type": "torch.nn", "path": "stable/generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm", "desc": "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."}, {"name": "torch.nn.RNNBase", "type": "torch.nn", "path": "stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase", "desc": ""}, {"name": "torch.nn.RNN", "type": "torch.nn", "path": "stable/generated/torch.nn.RNN.html#torch.nn.RNN", "desc": "Applies a multi-layer Elman RNN with tanh \u2061 \\tanh tanh or ReLU \\text{ReLU} ReLU non-linearity to an input sequence."}, {"name": "torch.nn.LSTM", "type": "torch.nn", "path": "stable/generated/torch.nn.LSTM.html#torch.nn.LSTM", "desc": "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence."}, {"name": "torch.nn.GRU", "type": "torch.nn", "path": "stable/generated/torch.nn.GRU.html#torch.nn.GRU", "desc": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence."}, {"name": "torch.nn.RNNCell", "type": "torch.nn", "path": "stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell", "desc": "An Elman RNN cell with tanh or ReLU non-linearity."}, {"name": "torch.nn.LSTMCell", "type": "torch.nn", "path": "stable/generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell", "desc": "A long short-term memory (LSTM) cell."}, {"name": "torch.nn.GRUCell", "type": "torch.nn", "path": "stable/generated/torch.nn.GRUCell.html#torch.nn.GRUCell", "desc": "A gated recurrent unit (GRU) cell"}, {"name": "torch.nn.Transformer", "type": "torch.nn", "path": "stable/generated/torch.nn.Transformer.html#torch.nn.Transformer", "desc": "A transformer model."}, {"name": "torch.nn.TransformerEncoder", "type": "torch.nn", "path": "stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder", "desc": "TransformerEncoder is a stack of N encoder layers"}, {"name": "torch.nn.TransformerDecoder", "type": "torch.nn", "path": "stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder", "desc": "TransformerDecoder is a stack of N decoder layers"}, {"name": "torch.nn.TransformerEncoderLayer", "type": "torch.nn", "path": "stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer", "desc": "TransformerEncoderLayer is made up of self-attn and feedforward network."}, {"name": "torch.nn.TransformerDecoderLayer", "type": "torch.nn", "path": "stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer", "desc": "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network."}, {"name": "torch.nn.Identity", "type": "torch.nn", "path": "stable/generated/torch.nn.Identity.html#torch.nn.Identity", "desc": "A placeholder identity operator that is argument-insensitive."}, {"name": "torch.nn.Linear", "type": "torch.nn", "path": "stable/generated/torch.nn.Linear.html#torch.nn.Linear", "desc": "Applies a linear transformation to the incoming data: y = x A T + b y = xA^T + b y = x A T + b"}, {"name": "torch.nn.Bilinear", "type": "torch.nn", "path": "stable/generated/torch.nn.Bilinear.html#torch.nn.Bilinear", "desc": "Applies a bilinear transformation to the incoming data: y = x 1 T A x 2 + b y = x_1^T A x_2 + b y = x 1 T \u200b A x 2 \u200b + b"}, {"name": "torch.nn.LazyLinear", "type": "torch.nn", "path": "stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear", "desc": "A torch.nn.Linear module where in_features is inferred."}, {"name": "torch.nn.Dropout", "type": "torch.nn", "path": "stable/generated/torch.nn.Dropout.html#torch.nn.Dropout", "desc": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution."}, {"name": "torch.nn.Dropout2d", "type": "torch.nn", "path": "stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d", "desc": "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the j j j -th channel of the i i i -th sample in the batched input is a 2D tensor input [ i j ] \\text{input}[i, j] input [ i j ] )."}, {"name": "torch.nn.Dropout3d", "type": "torch.nn", "path": "stable/generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d", "desc": "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the j j j -th channel of the i i i -th sample in the batched input is a 3D tensor input [ i j ] \\text{input}[i, j] input [ i j ] )."}, {"name": "torch.nn.AlphaDropout", "type": "torch.nn", "path": "stable/generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout", "desc": "Applies Alpha Dropout over the input."}, {"name": "torch.nn.FeatureAlphaDropout", "type": "torch.nn", "path": "stable/generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout", "desc": "Randomly masks out entire channels (a channel is a feature map, e.g."}, {"name": "torch.nn.Embedding", "type": "torch.nn", "path": "stable/generated/torch.nn.Embedding.html#torch.nn.Embedding", "desc": "A simple lookup table that stores embeddings of a fixed dictionary and size."}, {"name": "torch.nn.EmbeddingBag", "type": "torch.nn", "path": "stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag", "desc": "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings."}, {"name": "torch.nn.CosineSimilarity", "type": "torch.nn", "path": "stable/generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity", "desc": "Returns cosine similarity between x 1 x_1 x 1 \u200b and x 2 x_2 x 2 \u200b computed along dim "}, {"name": "torch.nn.PairwiseDistance", "type": "torch.nn", "path": "stable/generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance", "desc": "Computes the pairwise distance between vectors v 1 v_1 v 1 \u200b v 2 v_2 v 2 \u200b using the p-norm:"}, {"name": "torch.nn.L1Loss", "type": "torch.nn", "path": "stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss", "desc": "Creates a criterion that measures the mean absolute error (MAE) between each element in the input x x x and target y y y "}, {"name": "torch.nn.MSELoss", "type": "torch.nn", "path": "stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss", "desc": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x x x and target y y y "}, {"name": "torch.nn.CrossEntropyLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss", "desc": "This criterion computes the cross entropy loss between input and target."}, {"name": "torch.nn.CTCLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss", "desc": "The Connectionist Temporal Classification loss."}, {"name": "torch.nn.NLLLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss", "desc": "The negative log likelihood loss."}, {"name": "torch.nn.PoissonNLLLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss", "desc": "Negative log likelihood loss with Poisson distribution of target."}, {"name": "torch.nn.GaussianNLLLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss", "desc": "Gaussian negative log likelihood loss."}, {"name": "torch.nn.KLDivLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss", "desc": "The Kullback-Leibler divergence loss."}, {"name": "torch.nn.BCELoss", "type": "torch.nn", "path": "stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss", "desc": "Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:"}, {"name": "torch.nn.BCEWithLogitsLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss", "desc": "This loss combines a Sigmoid layer and the BCELoss in one single class."}, {"name": "torch.nn.MarginRankingLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss", "desc": "Creates a criterion that measures the loss given inputs x 1 x1 x 1 x 2 x2 x 2 two 1D mini-batch or 0D Tensors and a label 1D mini-batch or 0D Tensor y y y (containing 1 or -1)."}, {"name": "torch.nn.HingeEmbeddingLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss", "desc": "Measures the loss given an input tensor x x x and a labels tensor y y y (containing 1 or -1)."}, {"name": "torch.nn.MultiLabelMarginLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss", "desc": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x x x (a 2D mini-batch Tensor ) and output y y y (which is a 2D Tensor of target class indices)."}, {"name": "torch.nn.HuberLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss", "desc": "Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."}, {"name": "torch.nn.SmoothL1Loss", "type": "torch.nn", "path": "stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss", "desc": "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."}, {"name": "torch.nn.SoftMarginLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss", "desc": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor x x x and target tensor y y y (containing 1 or -1)."}, {"name": "torch.nn.MultiLabelSoftMarginLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss", "desc": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x x x and target y y y of size ( N C ) (N, C) ( N C ) "}, {"name": "torch.nn.CosineEmbeddingLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss", "desc": "Creates a criterion that measures the loss given input tensors x 1 x_1 x 1 \u200b x 2 x_2 x 2 \u200b and a Tensor label y y y with values 1 or -1."}, {"name": "torch.nn.MultiMarginLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss", "desc": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x x x (a 2D mini-batch Tensor ) and output y y y (which is a 1D tensor of target class indices, 0 \u2264 y \u2264 x.size ( 1 ) \u2212 1 0 \\leq y \\leq \\text{x.size}(1)-1 0 \u2264 y \u2264 x.size ( 1 ) \u2212 1 ):"}, {"name": "torch.nn.TripletMarginLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss", "desc": "Creates a criterion that measures the triplet loss given an input tensors x 1 x1 x 1 x 2 x2 x 2 x 3 x3 x 3 and a margin with a value greater than 0 0 0 "}, {"name": "torch.nn.TripletMarginWithDistanceLoss", "type": "torch.nn", "path": "stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss", "desc": "Creates a criterion that measures the triplet loss given input tensors a a a p p p and n n n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d)."}, {"name": "torch.nn.PixelShuffle", "type": "torch.nn", "path": "stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle", "desc": "Rearranges elements in a tensor of shape ( \u2217 C \u00d7 r 2 H W ) (*, C \\times r^2, H, W) ( \u2217 C \u00d7 r 2 H W ) to a tensor of shape ( \u2217 C H \u00d7 r W \u00d7 r ) (*, C, H \\times r, W \\times r) ( \u2217 C H \u00d7 r W \u00d7 r ) where r is an upscale factor."}, {"name": "torch.nn.PixelUnshuffle", "type": "torch.nn", "path": "stable/generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle", "desc": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( \u2217 C H \u00d7 r W \u00d7 r ) (*, C, H \\times r, W \\times r) ( \u2217 C H \u00d7 r W \u00d7 r ) to a tensor of shape ( \u2217 C \u00d7 r 2 H W ) (*, C \\times r^2, H, W) ( \u2217 C \u00d7 r 2 H W ) where r is a downscale factor."}, {"name": "torch.nn.Upsample", "type": "torch.nn", "path": "stable/generated/torch.nn.Upsample.html#torch.nn.Upsample", "desc": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data."}, {"name": "torch.nn.UpsamplingNearest2d", "type": "torch.nn", "path": "stable/generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d", "desc": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels."}, {"name": "torch.nn.UpsamplingBilinear2d", "type": "torch.nn", "path": "stable/generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d", "desc": "Applies a 2D bilinear upsampling to an input signal composed of several input channels."}, {"name": "torch.nn.ChannelShuffle", "type": "torch.nn", "path": "stable/generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle", "desc": "Divide the channels in a tensor of shape ( \u2217 C H W ) (*, C H, W) ( \u2217 C H W ) into g groups and rearrange them as ( \u2217 C g g H W ) (*, C \\frac g, g, H, W) ( \u2217 C g \u200b g H W ) while keeping the original tensor shape."}, {"name": "torch.nn.DataParallel", "type": "torch.nn", "path": "stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel", "desc": "Implements data parallelism at the module level."}, {"name": "torch.nn.parallel.DistributedDataParallel", "type": "torch.nn", "path": "stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel", "desc": "Implements distributed data parallelism that is based on torch.distributed package at the module level."}, {"name": "torch.nn.utils.clip_grad_norm_", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_", "desc": "Clips gradient norm of an iterable of parameters."}, {"name": "torch.nn.utils.clip_grad_value_", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_", "desc": "Clips gradient of an iterable of parameters at specified value."}, {"name": "torch.nn.utils.parameters_to_vector", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector", "desc": "Convert parameters to one vector"}, {"name": "torch.nn.utils.vector_to_parameters", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters", "desc": "Convert one vector to the parameters"}, {"name": "torch.nn.utils.prune.BasePruningMethod", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod", "desc": "Abstract base class for creation of new pruning techniques."}, {"name": "torch.nn.utils.prune.PruningContainer", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer", "desc": "Container holding a sequence of pruning methods for iterative pruning."}, {"name": "torch.nn.utils.prune.Identity", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity", "desc": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones."}, {"name": "torch.nn.utils.prune.RandomUnstructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured", "desc": "Prune (currently unpruned) units in a tensor at random."}, {"name": "torch.nn.utils.prune.L1Unstructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured", "desc": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm."}, {"name": "torch.nn.utils.prune.RandomStructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured", "desc": "Prune entire (currently unpruned) channels in a tensor at random."}, {"name": "torch.nn.utils.prune.LnStructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured", "desc": "Prune entire (currently unpruned) channels in a tensor based on their L n -norm."}, {"name": "torch.nn.utils.prune.CustomFromMask", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask", "desc": ""}, {"name": "torch.nn.utils.prune.identity", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity", "desc": "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units."}, {"name": "torch.nn.utils.prune.random_unstructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured", "desc": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random."}, {"name": "torch.nn.utils.prune.l1_unstructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured", "desc": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm."}, {"name": "torch.nn.utils.prune.random_structured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured", "desc": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random."}, {"name": "torch.nn.utils.prune.ln_structured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured", "desc": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L n -norm."}, {"name": "torch.nn.utils.prune.global_unstructured", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured", "desc": "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method "}, {"name": "torch.nn.utils.prune.custom_from_mask", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask", "desc": "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask "}, {"name": "torch.nn.utils.prune.remove", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove", "desc": "Removes the pruning reparameterization from a module and the pruning method from the forward hook."}, {"name": "torch.nn.utils.prune.is_pruned", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned", "desc": "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod "}, {"name": "torch.nn.utils.weight_norm", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.weight_norm.html#torch.nn.utils.weight_norm", "desc": "Applies weight normalization to a parameter in the given module."}, {"name": "torch.nn.utils.remove_weight_norm", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm", "desc": "Removes the weight normalization reparameterization from a module."}, {"name": "torch.nn.utils.spectral_norm", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm", "desc": "Applies spectral normalization to a parameter in the given module."}, {"name": "torch.nn.utils.remove_spectral_norm", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm", "desc": "Removes the spectral normalization reparameterization from a module."}, {"name": "torch.nn.utils.skip_init", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init", "desc": "Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers."}, {"name": "torch.nn.utils.parametrizations.orthogonal", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal", "desc": "Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices."}, {"name": "torch.nn.utils.parametrizations.spectral_norm", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm", "desc": "Applies spectral normalization to a parameter in the given module."}, {"name": "torch.nn.utils.parametrize.register_parametrization", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization", "desc": "Adds a parametrization to a tensor in a module."}, {"name": "torch.nn.utils.parametrize.remove_parametrizations", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations", "desc": "Removes the parametrizations on a tensor in a module."}, {"name": "torch.nn.utils.parametrize.cached", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached", "desc": "Context manager that enables the caching system within parametrizations registered with register_parametrization() "}, {"name": "torch.nn.utils.parametrize.is_parametrized", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized", "desc": "Returns True if module has an active parametrization."}, {"name": "torch.nn.utils.parametrize.ParametrizationList", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList", "desc": "A sequential container that holds and manages the original or original0 original1 "}, {"name": "torch.nn.utils.rnn.PackedSequence", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence", "desc": "Holds the data and list of batch_sizes of a packed sequence."}, {"name": "torch.nn.utils.rnn.pack_padded_sequence", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence", "desc": "Packs a Tensor containing padded sequences of variable length."}, {"name": "torch.nn.utils.rnn.pad_packed_sequence", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence", "desc": "Pads a packed batch of variable length sequences."}, {"name": "torch.nn.utils.rnn.pad_sequence", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence", "desc": "Pad a list of variable length Tensors with padding_value"}, {"name": "torch.nn.utils.rnn.pack_sequence", "type": "torch.nn", "path": "stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence", "desc": "Packs a list of variable length Tensors"}, {"name": "torch.nn.Flatten", "type": "torch.nn", "path": "stable/generated/torch.nn.Flatten.html#torch.nn.Flatten", "desc": "Flattens a contiguous range of dims into a tensor."}, {"name": "torch.nn.Unflatten", "type": "torch.nn", "path": "stable/generated/torch.nn.Unflatten.html#torch.nn.Unflatten", "desc": "Unflattens a tensor dim expanding it to a desired shape."}, {"name": "torch.nn.modules.lazy.LazyModuleMixin", "type": "torch.nn", "path": "stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin", "desc": "A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d"}, {"name": "torch.nn.functional.conv1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d", "desc": "Applies a 1D convolution over an input signal composed of several input planes."}, {"name": "torch.nn.functional.conv2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d", "desc": "Applies a 2D convolution over an input image composed of several input planes."}, {"name": "torch.nn.functional.conv3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d", "desc": "Applies a 3D convolution over an input image composed of several input planes."}, {"name": "torch.nn.functional.conv_transpose1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d", "desc": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d."}, {"name": "torch.nn.functional.conv_transpose2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d", "desc": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d."}, {"name": "torch.nn.functional.conv_transpose3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d", "desc": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d"}, {"name": "torch.nn.functional.unfold", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold", "desc": "Extracts sliding local blocks from a batched input tensor."}, {"name": "torch.nn.functional.fold", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.fold.html#torch.nn.functional.fold", "desc": "Combines an array of sliding local blocks into a large containing tensor."}, {"name": "torch.nn.functional.avg_pool1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d", "desc": "Applies a 1D average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.avg_pool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d", "desc": "Applies 2D average-pooling operation in k H \u00d7 k W kH \\times kW k H \u00d7 kW regions by step size s H \u00d7 s W sH \\times sW sH \u00d7 s W steps."}, {"name": "torch.nn.functional.avg_pool3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d", "desc": "Applies 3D average-pooling operation in k T \u00d7 k H \u00d7 k W kT \\times kH \\times kW k T \u00d7 k H \u00d7 kW regions by step size s T \u00d7 s H \u00d7 s W sT \\times sH \\times sW s T \u00d7 sH \u00d7 s W steps."}, {"name": "torch.nn.functional.max_pool1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d", "desc": "Applies a 1D max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.max_pool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d", "desc": "Applies a 2D max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.max_pool3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d", "desc": "Applies a 3D max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.max_unpool1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d", "desc": "Computes a partial inverse of MaxPool1d "}, {"name": "torch.nn.functional.max_unpool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d", "desc": "Computes a partial inverse of MaxPool2d "}, {"name": "torch.nn.functional.max_unpool3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d", "desc": "Computes a partial inverse of MaxPool3d "}, {"name": "torch.nn.functional.lp_pool1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d", "desc": "Applies a 1D power-average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.lp_pool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d", "desc": "Applies a 2D power-average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.adaptive_max_pool1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d", "desc": "Applies a 1D adaptive max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.adaptive_max_pool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d", "desc": "Applies a 2D adaptive max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.adaptive_max_pool3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d", "desc": "Applies a 3D adaptive max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.adaptive_avg_pool1d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d", "desc": "Applies a 1D adaptive average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.adaptive_avg_pool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d", "desc": "Applies a 2D adaptive average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.adaptive_avg_pool3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d", "desc": "Applies a 3D adaptive average pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.fractional_max_pool2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d", "desc": "Applies 2D fractional max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.fractional_max_pool3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d", "desc": "Applies 3D fractional max pooling over an input signal composed of several input planes."}, {"name": "torch.nn.functional.threshold", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold", "desc": "Thresholds each element of the input Tensor."}, {"name": "torch.nn.functional.threshold_", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_", "desc": "In-place version of threshold() "}, {"name": "torch.nn.functional.relu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu", "desc": "Applies the rectified linear unit function element-wise."}, {"name": "torch.nn.functional.relu_", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_", "desc": "In-place version of relu() "}, {"name": "torch.nn.functional.hardtanh", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh", "desc": "Applies the HardTanh function element-wise."}, {"name": "torch.nn.functional.hardtanh_", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_", "desc": "In-place version of hardtanh() "}, {"name": "torch.nn.functional.hardswish", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish", "desc": "Applies the hardswish function, element-wise, as described in the paper:"}, {"name": "torch.nn.functional.relu6", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6", "desc": "Applies the element-wise function ReLU6 ( x ) = min \u2061 ( max \u2061 ( 0 x ) 6 ) \\text{ReLU6}(x) = \\min(\\max(0,x), 6) ReLU6 ( x ) = min ( max ( 0 x ) 6 ) "}, {"name": "torch.nn.functional.elu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.elu.html#torch.nn.functional.elu", "desc": "Applies element-wise, ELU ( x ) = max \u2061 ( 0 x ) + min \u2061 ( 0 \u03b1 \u2217 ( exp \u2061 ( x ) \u2212 1 ) ) \\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) ELU ( x ) = max ( 0 x ) + min ( 0 \u03b1 \u2217 ( exp ( x ) \u2212 1 )) "}, {"name": "torch.nn.functional.elu_", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_", "desc": "In-place version of elu() "}, {"name": "torch.nn.functional.selu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.selu.html#torch.nn.functional.selu", "desc": "Applies element-wise, SELU ( x ) = s c a l e \u2217 ( max \u2061 ( 0 x ) + min \u2061 ( 0 \u03b1 \u2217 ( exp \u2061 ( x ) \u2212 1 ) ) ) \\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) SELU ( x ) = sc a l e \u2217 ( max ( 0 x ) + min ( 0 \u03b1 \u2217 ( exp ( x ) \u2212 1 ))) with \u03b1 = 1.6732632423543772848170429916717 \\alpha=1.6732632423543772848170429916717 \u03b1 = 1.6732632423543772848170429916717 and s c a l e = 1.0507009873554804934193349852946 scale=1.0507009873554804934193349852946 sc a l e = 1.0507009873554804934193349852946 "}, {"name": "torch.nn.functional.celu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.celu.html#torch.nn.functional.celu", "desc": "Applies element-wise, CELU ( x ) = max \u2061 ( 0 x ) + min \u2061 ( 0 \u03b1 \u2217 ( exp \u2061 ( x / \u03b1 ) \u2212 1 ) ) \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) CELU ( x ) = max ( 0 x ) + min ( 0 \u03b1 \u2217 ( exp ( x / \u03b1 ) \u2212 1 )) "}, {"name": "torch.nn.functional.leaky_relu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu", "desc": "Applies element-wise, LeakyReLU ( x ) = max \u2061 ( 0 x ) + negative_slope \u2217 min \u2061 ( 0 x ) \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x) LeakyReLU ( x ) = max ( 0 x ) + negative_slope \u2217 min ( 0 x )"}, {"name": "torch.nn.functional.leaky_relu_", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_", "desc": "In-place version of leaky_relu() "}, {"name": "torch.nn.functional.prelu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu", "desc": "Applies element-wise the function PReLU ( x ) = max \u2061 ( 0 x ) + weight \u2217 min \u2061 ( 0 x ) \\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x) PReLU ( x ) = max ( 0 x ) + weight \u2217 min ( 0 x ) where weight is a learnable parameter."}, {"name": "torch.nn.functional.rrelu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu", "desc": "Randomized leaky ReLU."}, {"name": "torch.nn.functional.rrelu_", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_", "desc": "In-place version of rrelu() "}, {"name": "torch.nn.functional.glu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.glu.html#torch.nn.functional.glu", "desc": "The gated linear unit."}, {"name": "torch.nn.functional.gelu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu", "desc": "Applies element-wise the function GELU ( x ) = x \u2217 \u03a6 ( x ) \\text{GELU}(x) = x * \\Phi(x) GELU ( x ) = x \u2217 \u03a6 ( x )"}, {"name": "torch.nn.functional.logsigmoid", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid", "desc": "Applies element-wise LogSigmoid ( x i ) = log \u2061 ( 1 1 + exp \u2061 ( \u2212 x i ) ) \\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right) LogSigmoid ( x i \u200b ) = lo g ( 1 + e x p ( \u2212 x i \u200b ) 1 \u200b )"}, {"name": "torch.nn.functional.hardshrink", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink", "desc": "Applies the hard shrinkage function element-wise"}, {"name": "torch.nn.functional.tanhshrink", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink", "desc": "Applies element-wise, Tanhshrink ( x ) = x \u2212 Tanh ( x ) \\text{Tanhshrink}(x) = x - \\text{Tanh}(x) Tanhshrink ( x ) = x \u2212 Tanh ( x )"}, {"name": "torch.nn.functional.softsign", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign", "desc": "Applies element-wise, the function SoftSign ( x ) = x 1 + \u2223 x \u2223 \\text{SoftSign}(x) = \\frac{x}{1 + |x|} SoftSign ( x ) = 1 + \u2223 x \u2223 x \u200b"}, {"name": "torch.nn.functional.softplus", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus", "desc": "Applies element-wise, the function Softplus ( x ) = 1 \u03b2 \u2217 log \u2061 ( 1 + exp \u2061 ( \u03b2 \u2217 x ) ) \\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) Softplus ( x ) = \u03b2 1 \u200b \u2217 lo g ( 1 + exp ( \u03b2 \u2217 x )) "}, {"name": "torch.nn.functional.softmin", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin", "desc": "Applies a softmin function."}, {"name": "torch.nn.functional.softmax", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax", "desc": "Applies a softmax function."}, {"name": "torch.nn.functional.softshrink", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink", "desc": "Applies the soft shrinkage function elementwise"}, {"name": "torch.nn.functional.gumbel_softmax", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax", "desc": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes."}, {"name": "torch.nn.functional.log_softmax", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax", "desc": "Applies a softmax followed by a logarithm."}, {"name": "torch.nn.functional.tanh", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh", "desc": "Applies element-wise, Tanh ( x ) = tanh \u2061 ( x ) = exp \u2061 ( x ) \u2212 exp \u2061 ( \u2212 x ) exp \u2061 ( x ) + exp \u2061 ( \u2212 x ) \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)} Tanh ( x ) = tanh ( x ) = e x p ( x ) + e x p ( \u2212 x ) e x p ( x ) \u2212 e x p ( \u2212 x ) \u200b"}, {"name": "torch.nn.functional.sigmoid", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid", "desc": "Applies the element-wise function Sigmoid ( x ) = 1 1 + exp \u2061 ( \u2212 x ) \\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)} Sigmoid ( x ) = 1 + e x p ( \u2212 x ) 1 \u200b"}, {"name": "torch.nn.functional.hardsigmoid", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid", "desc": "Applies the element-wise function"}, {"name": "torch.nn.functional.silu", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.silu.html#torch.nn.functional.silu", "desc": "Applies the Sigmoid Linear Unit (SiLU) function, element-wise."}, {"name": "torch.nn.functional.mish", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.mish.html#torch.nn.functional.mish", "desc": "Applies the Mish function, element-wise."}, {"name": "torch.nn.functional.batch_norm", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm", "desc": "Applies Batch Normalization for each channel across a batch of data."}, {"name": "torch.nn.functional.group_norm", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm", "desc": "Applies Group Normalization for last certain number of dimensions."}, {"name": "torch.nn.functional.instance_norm", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm", "desc": "Applies Instance Normalization for each channel in each data sample in a batch."}, {"name": "torch.nn.functional.layer_norm", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm", "desc": "Applies Layer Normalization for last certain number of dimensions."}, {"name": "torch.nn.functional.local_response_norm", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm", "desc": "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."}, {"name": "torch.nn.functional.normalize", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize", "desc": "Performs L p L_p L p \u200b normalization of inputs over specified dimension."}, {"name": "torch.nn.functional.linear", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear", "desc": "Applies a linear transformation to the incoming data: y = x A T + b y = xA^T + b y = x A T + b "}, {"name": "torch.nn.functional.bilinear", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear", "desc": "Applies a bilinear transformation to the incoming data: y = x 1 T A x 2 + b y = x_1^T A x_2 + b y = x 1 T \u200b A x 2 \u200b + b"}, {"name": "torch.nn.functional.dropout", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout", "desc": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution."}, {"name": "torch.nn.functional.alpha_dropout", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout", "desc": "Applies alpha dropout to the input."}, {"name": "torch.nn.functional.feature_alpha_dropout", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout", "desc": "Randomly masks out entire channels (a channel is a feature map, e.g."}, {"name": "torch.nn.functional.dropout2d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d", "desc": "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the j j j -th channel of the i i i -th sample in the batched input is a 2D tensor input [ i j ] \\text{input}[i, j] input [ i j ] ) of the input tensor)."}, {"name": "torch.nn.functional.dropout3d", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d", "desc": "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the j j j -th channel of the i i i -th sample in the batched input is a 3D tensor input [ i j ] \\text{input}[i, j] input [ i j ] ) of the input tensor)."}, {"name": "torch.nn.functional.embedding", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.embedding.html#torch.nn.functional.embedding", "desc": "A simple lookup table that looks up embeddings in a fixed dictionary and size."}, {"name": "torch.nn.functional.embedding_bag", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag", "desc": "Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings."}, {"name": "torch.nn.functional.one_hot", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot", "desc": "Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1."}, {"name": "torch.nn.functional.pairwise_distance", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance", "desc": "See torch.nn.PairwiseDistance for details"}, {"name": "torch.nn.functional.cosine_similarity", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity", "desc": "Returns cosine similarity between x1 and x2 computed along dim."}, {"name": "torch.nn.functional.pdist", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist", "desc": "Computes the p-norm distance between every pair of row vectors in the input."}, {"name": "torch.nn.functional.binary_cross_entropy", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy", "desc": "Function that measures the Binary Cross Entropy between the target and input probabilities."}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits", "desc": "Function that measures Binary Cross Entropy between target and input logits."}, {"name": "torch.nn.functional.poisson_nll_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss", "desc": "Poisson negative log likelihood loss."}, {"name": "torch.nn.functional.cosine_embedding_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss", "desc": "See CosineEmbeddingLoss for details."}, {"name": "torch.nn.functional.cross_entropy", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy", "desc": "This criterion computes the cross entropy loss between input and target."}, {"name": "torch.nn.functional.ctc_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss", "desc": "The Connectionist Temporal Classification loss."}, {"name": "torch.nn.functional.gaussian_nll_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss", "desc": "Gaussian negative log likelihood loss."}, {"name": "torch.nn.functional.hinge_embedding_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss", "desc": "See HingeEmbeddingLoss for details."}, {"name": "torch.nn.functional.kl_div", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div", "desc": "The Kullback-Leibler divergence Loss"}, {"name": "torch.nn.functional.l1_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss", "desc": "Function that takes the mean element-wise absolute value difference."}, {"name": "torch.nn.functional.mse_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss", "desc": "Measures the element-wise mean squared error."}, {"name": "torch.nn.functional.margin_ranking_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss", "desc": "See MarginRankingLoss for details."}, {"name": "torch.nn.functional.multilabel_margin_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss", "desc": "See MultiLabelMarginLoss for details."}, {"name": "torch.nn.functional.multilabel_soft_margin_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss", "desc": "See MultiLabelSoftMarginLoss for details."}, {"name": "torch.nn.functional.multi_margin_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss", "desc": "See MultiMarginLoss for details."}, {"name": "torch.nn.functional.nll_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss", "desc": "The negative log likelihood loss."}, {"name": "torch.nn.functional.huber_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss", "desc": "Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."}, {"name": "torch.nn.functional.smooth_l1_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss", "desc": "Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."}, {"name": "torch.nn.functional.soft_margin_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss", "desc": "See SoftMarginLoss for details."}, {"name": "torch.nn.functional.triplet_margin_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss", "desc": "See TripletMarginLoss for details"}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss", "desc": "See TripletMarginWithDistanceLoss for details."}, {"name": "torch.nn.functional.pixel_shuffle", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle", "desc": "Rearranges elements in a tensor of shape ( \u2217 C \u00d7 r 2 H W ) (*, C \\times r^2, H, W) ( \u2217 C \u00d7 r 2 H W ) to a tensor of shape ( \u2217 C H \u00d7 r W \u00d7 r ) (*, C, H \\times r, W \\times r) ( \u2217 C H \u00d7 r W \u00d7 r ) where r is the upscale_factor "}, {"name": "torch.nn.functional.pixel_unshuffle", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle", "desc": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( \u2217 C H \u00d7 r W \u00d7 r ) (*, C, H \\times r, W \\times r) ( \u2217 C H \u00d7 r W \u00d7 r ) to a tensor of shape ( \u2217 C \u00d7 r 2 H W ) (*, C \\times r^2, H, W) ( \u2217 C \u00d7 r 2 H W ) where r is the downscale_factor "}, {"name": "torch.nn.functional.pad", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad", "desc": "Pads tensor."}, {"name": "torch.nn.functional.interpolate", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate", "desc": "Down/up samples the input to either the given size or the given scale_factor"}, {"name": "torch.nn.functional.upsample", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample", "desc": "Upsamples the input to either the given size or the given scale_factor"}, {"name": "torch.nn.functional.upsample_nearest", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest", "desc": "Upsamples the input, using nearest neighbours\u2019 pixel values."}, {"name": "torch.nn.functional.upsample_bilinear", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear", "desc": "Upsamples the input, using bilinear upsampling."}, {"name": "torch.nn.functional.grid_sample", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample", "desc": "Given an input and a flow-field grid computes the output using input values and pixel locations from grid "}, {"name": "torch.nn.functional.affine_grid", "type": "torch.nn.functional", "path": "stable/generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid", "desc": "Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta "}, {"name": "torch.Tensor.new_tensor", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor", "desc": "Returns a new Tensor with data as the tensor data."}, {"name": "torch.Tensor.new_full", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.new_full.html#torch.Tensor.new_full", "desc": "Returns a Tensor of size size filled with fill_value "}, {"name": "torch.Tensor.new_empty", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty", "desc": "Returns a Tensor of size size filled with uninitialized data."}, {"name": "torch.Tensor.new_ones", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones", "desc": "Returns a Tensor of size size filled with 1 "}, {"name": "torch.Tensor.new_zeros", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros", "desc": "Returns a Tensor of size size filled with 0 "}, {"name": "torch.Tensor.is_cuda", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda", "desc": "Is True if the Tensor is stored on the GPU, False otherwise."}, {"name": "torch.Tensor.is_quantized", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized", "desc": "Is True if the Tensor is quantized, False otherwise."}, {"name": "torch.Tensor.is_meta", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta", "desc": "Is True if the Tensor is a meta tensor, False otherwise."}, {"name": "torch.Tensor.device", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.device.html#torch.Tensor.device", "desc": "Is the torch.device where this Tensor is."}, {"name": "torch.Tensor.grad", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.grad.html#torch.Tensor.grad", "desc": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self "}, {"name": "torch.Tensor.ndim", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ndim.html#torch.Tensor.ndim", "desc": "Alias for dim()"}, {"name": "torch.Tensor.real", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.real.html#torch.Tensor.real", "desc": "Returns a new tensor containing real values of the self tensor for a complex-valued input tensor."}, {"name": "torch.Tensor.imag", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.imag.html#torch.Tensor.imag", "desc": "Returns a new tensor containing imaginary values of the self tensor."}, {"name": "torch.Tensor.abs", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.abs.html#torch.Tensor.abs", "desc": "See torch.abs()"}, {"name": "torch.Tensor.abs_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.abs_.html#torch.Tensor.abs_", "desc": "In-place version of abs()"}, {"name": "torch.Tensor.absolute", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute", "desc": "Alias for abs()"}, {"name": "torch.Tensor.absolute_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_", "desc": "In-place version of absolute() Alias for abs_()"}, {"name": "torch.Tensor.acos", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.acos.html#torch.Tensor.acos", "desc": "See torch.acos()"}, {"name": "torch.Tensor.acos_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.acos_.html#torch.Tensor.acos_", "desc": "In-place version of acos()"}, {"name": "torch.Tensor.arccos", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos", "desc": "See torch.arccos()"}, {"name": "torch.Tensor.arccos_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_", "desc": "In-place version of arccos()"}, {"name": "torch.Tensor.add", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.add.html#torch.Tensor.add", "desc": "Add a scalar or tensor to self tensor."}, {"name": "torch.Tensor.add_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.add_.html#torch.Tensor.add_", "desc": "In-place version of add()"}, {"name": "torch.Tensor.addbmm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm", "desc": "See torch.addbmm()"}, {"name": "torch.Tensor.addbmm_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_", "desc": "In-place version of addbmm()"}, {"name": "torch.Tensor.addcdiv", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv", "desc": "See torch.addcdiv()"}, {"name": "torch.Tensor.addcdiv_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_", "desc": "In-place version of addcdiv()"}, {"name": "torch.Tensor.addcmul", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul", "desc": "See torch.addcmul()"}, {"name": "torch.Tensor.addcmul_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_", "desc": "In-place version of addcmul()"}, {"name": "torch.Tensor.addmm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm", "desc": "See torch.addmm()"}, {"name": "torch.Tensor.addmm_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_", "desc": "In-place version of addmm()"}, {"name": "torch.Tensor.sspaddmm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm", "desc": "See torch.sspaddmm()"}, {"name": "torch.Tensor.addmv", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv", "desc": "See torch.addmv()"}, {"name": "torch.Tensor.addmv_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_", "desc": "In-place version of addmv()"}, {"name": "torch.Tensor.addr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addr.html#torch.Tensor.addr", "desc": "See torch.addr()"}, {"name": "torch.Tensor.addr_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.addr_.html#torch.Tensor.addr_", "desc": "In-place version of addr()"}, {"name": "torch.Tensor.adjoint", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint", "desc": "Alias for adjoint()"}, {"name": "torch.Tensor.allclose", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.allclose.html#torch.Tensor.allclose", "desc": "See torch.allclose()"}, {"name": "torch.Tensor.amax", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.amax.html#torch.Tensor.amax", "desc": "See torch.amax()"}, {"name": "torch.Tensor.amin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.amin.html#torch.Tensor.amin", "desc": "See torch.amin()"}, {"name": "torch.Tensor.aminmax", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax", "desc": "See torch.aminmax()"}, {"name": "torch.Tensor.angle", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.angle.html#torch.Tensor.angle", "desc": "See torch.angle()"}, {"name": "torch.Tensor.apply_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.apply_.html#torch.Tensor.apply_", "desc": "Applies the function callable to each element in the tensor, replacing each element with the value returned by callable "}, {"name": "torch.Tensor.argmax", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.argmax.html#torch.Tensor.argmax", "desc": "See torch.argmax()"}, {"name": "torch.Tensor.argmin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.argmin.html#torch.Tensor.argmin", "desc": "See torch.argmin()"}, {"name": "torch.Tensor.argsort", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.argsort.html#torch.Tensor.argsort", "desc": "See torch.argsort()"}, {"name": "torch.Tensor.argwhere", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere", "desc": "See torch.argwhere()"}, {"name": "torch.Tensor.asin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.asin.html#torch.Tensor.asin", "desc": "See torch.asin()"}, {"name": "torch.Tensor.asin_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.asin_.html#torch.Tensor.asin_", "desc": "In-place version of asin()"}, {"name": "torch.Tensor.arcsin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin", "desc": "See torch.arcsin()"}, {"name": "torch.Tensor.arcsin_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_", "desc": "In-place version of arcsin()"}, {"name": "torch.Tensor.as_strided", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided", "desc": "See torch.as_strided()"}, {"name": "torch.Tensor.atan", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.atan.html#torch.Tensor.atan", "desc": "See torch.atan()"}, {"name": "torch.Tensor.atan_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.atan_.html#torch.Tensor.atan_", "desc": "In-place version of atan()"}, {"name": "torch.Tensor.arctan", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan", "desc": "See torch.arctan()"}, {"name": "torch.Tensor.arctan_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_", "desc": "In-place version of arctan()"}, {"name": "torch.Tensor.atan2", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2", "desc": "See torch.atan2()"}, {"name": "torch.Tensor.atan2_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_", "desc": "In-place version of atan2()"}, {"name": "torch.Tensor.arctan2", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2", "desc": "See torch.arctan2()"}, {"name": "torch.Tensor.arctan2_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_", "desc": "atan2_(other) -> Tensor"}, {"name": "torch.Tensor.all", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.all.html#torch.Tensor.all", "desc": "See torch.all()"}, {"name": "torch.Tensor.any", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.any.html#torch.Tensor.any", "desc": "See torch.any()"}, {"name": "torch.Tensor.backward", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.backward.html#torch.Tensor.backward", "desc": "Computes the gradient of current tensor w.r.t."}, {"name": "torch.Tensor.baddbmm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm", "desc": "See torch.baddbmm()"}, {"name": "torch.Tensor.baddbmm_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_", "desc": "In-place version of baddbmm()"}, {"name": "torch.Tensor.bernoulli", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli", "desc": "Returns a result tensor where each result[i] \\texttt{result[i]} result[i] is independently sampled from Bernoulli ( self[i] ) \\text{Bernoulli}(\\texttt{self[i]}) Bernoulli ( self[i] ) "}, {"name": "torch.Tensor.bernoulli_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_", "desc": "Fills each location of self with an independent sample from Bernoulli ( p ) \\text{Bernoulli}(\\texttt{p}) Bernoulli ( p ) "}, {"name": "torch.Tensor.bfloat16", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16", "desc": "self.bfloat16() is equivalent to self.to(torch.bfloat16) "}, {"name": "torch.Tensor.bincount", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bincount.html#torch.Tensor.bincount", "desc": "See torch.bincount()"}, {"name": "torch.Tensor.bitwise_not", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not", "desc": "See torch.bitwise_not()"}, {"name": "torch.Tensor.bitwise_not_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_", "desc": "In-place version of bitwise_not()"}, {"name": "torch.Tensor.bitwise_and", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and", "desc": "See torch.bitwise_and()"}, {"name": "torch.Tensor.bitwise_and_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_", "desc": "In-place version of bitwise_and()"}, {"name": "torch.Tensor.bitwise_or", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or", "desc": "See torch.bitwise_or()"}, {"name": "torch.Tensor.bitwise_or_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_", "desc": "In-place version of bitwise_or()"}, {"name": "torch.Tensor.bitwise_xor", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor", "desc": "See torch.bitwise_xor()"}, {"name": "torch.Tensor.bitwise_xor_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_", "desc": "In-place version of bitwise_xor()"}, {"name": "torch.Tensor.bitwise_left_shift", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift", "desc": "See torch.bitwise_left_shift()"}, {"name": "torch.Tensor.bitwise_left_shift_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_", "desc": "In-place version of bitwise_left_shift()"}, {"name": "torch.Tensor.bitwise_right_shift", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift", "desc": "See torch.bitwise_right_shift()"}, {"name": "torch.Tensor.bitwise_right_shift_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_", "desc": "In-place version of bitwise_right_shift()"}, {"name": "torch.Tensor.bmm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bmm.html#torch.Tensor.bmm", "desc": "See torch.bmm()"}, {"name": "torch.Tensor.bool", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.bool.html#torch.Tensor.bool", "desc": "self.bool() is equivalent to self.to(torch.bool) "}, {"name": "torch.Tensor.byte", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.byte.html#torch.Tensor.byte", "desc": "self.byte() is equivalent to self.to(torch.uint8) "}, {"name": "torch.Tensor.broadcast_to", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to", "desc": "See torch.broadcast_to() "}, {"name": "torch.Tensor.cauchy_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_", "desc": "Fills the tensor with numbers drawn from the Cauchy distribution:"}, {"name": "torch.Tensor.ceil", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil", "desc": "See torch.ceil()"}, {"name": "torch.Tensor.ceil_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_", "desc": "In-place version of ceil()"}, {"name": "torch.Tensor.char", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.char.html#torch.Tensor.char", "desc": "self.char() is equivalent to self.to(torch.int8) "}, {"name": "torch.Tensor.cholesky", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky", "desc": "See torch.cholesky()"}, {"name": "torch.Tensor.cholesky_inverse", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse", "desc": "See torch.cholesky_inverse()"}, {"name": "torch.Tensor.cholesky_solve", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve", "desc": "See torch.cholesky_solve()"}, {"name": "torch.Tensor.chunk", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.chunk.html#torch.Tensor.chunk", "desc": "See torch.chunk()"}, {"name": "torch.Tensor.clamp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp", "desc": "See torch.clamp()"}, {"name": "torch.Tensor.clamp_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_", "desc": "In-place version of clamp()"}, {"name": "torch.Tensor.clip", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.clip.html#torch.Tensor.clip", "desc": "Alias for clamp() "}, {"name": "torch.Tensor.clip_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.clip_.html#torch.Tensor.clip_", "desc": "Alias for clamp_() "}, {"name": "torch.Tensor.clone", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.clone.html#torch.Tensor.clone", "desc": "See torch.clone()"}, {"name": "torch.Tensor.contiguous", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous", "desc": "Returns a contiguous in memory tensor containing the same data as self tensor."}, {"name": "torch.Tensor.copy_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.copy_.html#torch.Tensor.copy_", "desc": "Copies the elements from src into self tensor and returns self "}, {"name": "torch.Tensor.conj", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.conj.html#torch.Tensor.conj", "desc": "See torch.conj()"}, {"name": "torch.Tensor.conj_physical", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical", "desc": "See torch.conj_physical()"}, {"name": "torch.Tensor.conj_physical_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_", "desc": "In-place version of conj_physical()"}, {"name": "torch.Tensor.resolve_conj", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj", "desc": "See torch.resolve_conj()"}, {"name": "torch.Tensor.resolve_neg", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg", "desc": "See torch.resolve_neg()"}, {"name": "torch.Tensor.copysign", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign", "desc": "See torch.copysign()"}, {"name": "torch.Tensor.copysign_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_", "desc": "In-place version of copysign()"}, {"name": "torch.Tensor.cos", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cos.html#torch.Tensor.cos", "desc": "See torch.cos()"}, {"name": "torch.Tensor.cos_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cos_.html#torch.Tensor.cos_", "desc": "In-place version of cos()"}, {"name": "torch.Tensor.cosh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh", "desc": "See torch.cosh()"}, {"name": "torch.Tensor.cosh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_", "desc": "In-place version of cosh()"}, {"name": "torch.Tensor.corrcoef", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef", "desc": "See torch.corrcoef()"}, {"name": "torch.Tensor.count_nonzero", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero", "desc": "See torch.count_nonzero()"}, {"name": "torch.Tensor.cov", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cov.html#torch.Tensor.cov", "desc": "See torch.cov()"}, {"name": "torch.Tensor.acosh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh", "desc": "See torch.acosh()"}, {"name": "torch.Tensor.acosh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_", "desc": "In-place version of acosh()"}, {"name": "torch.Tensor.arccosh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh", "desc": "acosh() -> Tensor"}, {"name": "torch.Tensor.arccosh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_", "desc": "acosh_() -> Tensor"}, {"name": "torch.Tensor.cpu", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cpu.html#torch.Tensor.cpu", "desc": "Returns a copy of this object in CPU memory."}, {"name": "torch.Tensor.cross", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cross.html#torch.Tensor.cross", "desc": "See torch.cross()"}, {"name": "torch.Tensor.cuda", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda", "desc": "Returns a copy of this object in CUDA memory."}, {"name": "torch.Tensor.logcumsumexp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp", "desc": "See torch.logcumsumexp()"}, {"name": "torch.Tensor.cummax", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cummax.html#torch.Tensor.cummax", "desc": "See torch.cummax()"}, {"name": "torch.Tensor.cummin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cummin.html#torch.Tensor.cummin", "desc": "See torch.cummin()"}, {"name": "torch.Tensor.cumprod", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod", "desc": "See torch.cumprod()"}, {"name": "torch.Tensor.cumprod_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_", "desc": "In-place version of cumprod()"}, {"name": "torch.Tensor.cumsum", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum", "desc": "See torch.cumsum()"}, {"name": "torch.Tensor.cumsum_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_", "desc": "In-place version of cumsum()"}, {"name": "torch.Tensor.data_ptr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr", "desc": "Returns the address of the first element of self tensor."}, {"name": "torch.Tensor.deg2rad", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad", "desc": "See torch.deg2rad()"}, {"name": "torch.Tensor.dequantize", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize", "desc": "Given a quantized Tensor, dequantize it and return the dequantized float Tensor."}, {"name": "torch.Tensor.det", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.det.html#torch.Tensor.det", "desc": "See torch.det()"}, {"name": "torch.Tensor.dense_dim", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim", "desc": "Return the number of dense dimensions in a sparse tensor self "}, {"name": "torch.Tensor.detach", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.detach.html#torch.Tensor.detach", "desc": "Returns a new Tensor, detached from the current graph."}, {"name": "torch.Tensor.detach_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.detach_.html#torch.Tensor.detach_", "desc": "Detaches the Tensor from the graph that created it, making it a leaf."}, {"name": "torch.Tensor.diag", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.diag.html#torch.Tensor.diag", "desc": "See torch.diag()"}, {"name": "torch.Tensor.diag_embed", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed", "desc": "See torch.diag_embed()"}, {"name": "torch.Tensor.diagflat", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat", "desc": "See torch.diagflat()"}, {"name": "torch.Tensor.diagonal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal", "desc": "See torch.diagonal()"}, {"name": "torch.Tensor.diagonal_scatter", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter", "desc": "diagonal(src, offset=0, dim1=0, dim2=1) -> Tensor"}, {"name": "torch.Tensor.fill_diagonal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_", "desc": "Fill the main diagonal of a tensor that has at least 2-dimensions."}, {"name": "torch.Tensor.fmax", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fmax.html#torch.Tensor.fmax", "desc": "See torch.fmax()"}, {"name": "torch.Tensor.fmin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fmin.html#torch.Tensor.fmin", "desc": "See torch.fmin()"}, {"name": "torch.Tensor.diff", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.diff.html#torch.Tensor.diff", "desc": "See torch.diff()"}, {"name": "torch.Tensor.digamma", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma", "desc": "See torch.digamma()"}, {"name": "torch.Tensor.digamma_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_", "desc": "In-place version of digamma()"}, {"name": "torch.Tensor.dim", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.dim.html#torch.Tensor.dim", "desc": "Returns the number of dimensions of self tensor."}, {"name": "torch.Tensor.dist", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.dist.html#torch.Tensor.dist", "desc": "See torch.dist()"}, {"name": "torch.Tensor.div", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.div.html#torch.Tensor.div", "desc": "See torch.div()"}, {"name": "torch.Tensor.div_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.div_.html#torch.Tensor.div_", "desc": "In-place version of div()"}, {"name": "torch.Tensor.divide", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.divide.html#torch.Tensor.divide", "desc": "See torch.divide()"}, {"name": "torch.Tensor.divide_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.divide_.html#torch.Tensor.divide_", "desc": "In-place version of divide()"}, {"name": "torch.Tensor.dot", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.dot.html#torch.Tensor.dot", "desc": "See torch.dot()"}, {"name": "torch.Tensor.double", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.double.html#torch.Tensor.double", "desc": "self.double() is equivalent to self.to(torch.float64) "}, {"name": "torch.Tensor.dsplit", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit", "desc": "See torch.dsplit()"}, {"name": "torch.Tensor.eig", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.eig.html#torch.Tensor.eig", "desc": "See torch.eig()"}, {"name": "torch.Tensor.element_size", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.element_size.html#torch.Tensor.element_size", "desc": "Returns the size in bytes of an individual element."}, {"name": "torch.Tensor.eq", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.eq.html#torch.Tensor.eq", "desc": "See torch.eq()"}, {"name": "torch.Tensor.eq_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.eq_.html#torch.Tensor.eq_", "desc": "In-place version of eq()"}, {"name": "torch.Tensor.equal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.equal.html#torch.Tensor.equal", "desc": "See torch.equal()"}, {"name": "torch.Tensor.erf", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.erf.html#torch.Tensor.erf", "desc": "See torch.erf()"}, {"name": "torch.Tensor.erf_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.erf_.html#torch.Tensor.erf_", "desc": "In-place version of erf()"}, {"name": "torch.Tensor.erfc", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc", "desc": "See torch.erfc()"}, {"name": "torch.Tensor.erfc_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_", "desc": "In-place version of erfc()"}, {"name": "torch.Tensor.erfinv", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv", "desc": "See torch.erfinv()"}, {"name": "torch.Tensor.erfinv_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_", "desc": "In-place version of erfinv()"}, {"name": "torch.Tensor.exp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.exp.html#torch.Tensor.exp", "desc": "See torch.exp()"}, {"name": "torch.Tensor.exp_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.exp_.html#torch.Tensor.exp_", "desc": "In-place version of exp()"}, {"name": "torch.Tensor.expm1", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1", "desc": "See torch.expm1()"}, {"name": "torch.Tensor.expm1_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_", "desc": "In-place version of expm1()"}, {"name": "torch.Tensor.expand", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.expand.html#torch.Tensor.expand", "desc": "Returns a new view of the self tensor with singleton dimensions expanded to a larger size."}, {"name": "torch.Tensor.expand_as", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as", "desc": "Expand this tensor to the same size as other "}, {"name": "torch.Tensor.exponential_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_", "desc": "Fills self tensor with elements drawn from the exponential distribution:"}, {"name": "torch.Tensor.fix", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fix.html#torch.Tensor.fix", "desc": "See torch.fix() "}, {"name": "torch.Tensor.fix_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fix_.html#torch.Tensor.fix_", "desc": "In-place version of fix()"}, {"name": "torch.Tensor.fill_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fill_.html#torch.Tensor.fill_", "desc": "Fills self tensor with the specified value."}, {"name": "torch.Tensor.flatten", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.flatten.html#torch.Tensor.flatten", "desc": "See torch.flatten()"}, {"name": "torch.Tensor.flip", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.flip.html#torch.Tensor.flip", "desc": "See torch.flip()"}, {"name": "torch.Tensor.fliplr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr", "desc": "See torch.fliplr()"}, {"name": "torch.Tensor.flipud", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.flipud.html#torch.Tensor.flipud", "desc": "See torch.flipud()"}, {"name": "torch.Tensor.float", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.float.html#torch.Tensor.float", "desc": "self.float() is equivalent to self.to(torch.float32) "}, {"name": "torch.Tensor.float_power", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power", "desc": "See torch.float_power()"}, {"name": "torch.Tensor.float_power_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_", "desc": "In-place version of float_power()"}, {"name": "torch.Tensor.floor", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.floor.html#torch.Tensor.floor", "desc": "See torch.floor()"}, {"name": "torch.Tensor.floor_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.floor_.html#torch.Tensor.floor_", "desc": "In-place version of floor()"}, {"name": "torch.Tensor.floor_divide", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide", "desc": "See torch.floor_divide()"}, {"name": "torch.Tensor.floor_divide_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_", "desc": "In-place version of floor_divide()"}, {"name": "torch.Tensor.fmod", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod", "desc": "See torch.fmod()"}, {"name": "torch.Tensor.fmod_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_", "desc": "In-place version of fmod()"}, {"name": "torch.Tensor.frac", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.frac.html#torch.Tensor.frac", "desc": "See torch.frac()"}, {"name": "torch.Tensor.frac_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.frac_.html#torch.Tensor.frac_", "desc": "In-place version of frac()"}, {"name": "torch.Tensor.frexp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.frexp.html#torch.Tensor.frexp", "desc": "See torch.frexp()"}, {"name": "torch.Tensor.gather", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.gather.html#torch.Tensor.gather", "desc": "See torch.gather()"}, {"name": "torch.Tensor.gcd", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd", "desc": "See torch.gcd()"}, {"name": "torch.Tensor.gcd_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_", "desc": "In-place version of gcd()"}, {"name": "torch.Tensor.ge", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ge.html#torch.Tensor.ge", "desc": "See torch.ge() "}, {"name": "torch.Tensor.ge_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ge_.html#torch.Tensor.ge_", "desc": "In-place version of ge() "}, {"name": "torch.Tensor.greater_equal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal", "desc": "See torch.greater_equal() "}, {"name": "torch.Tensor.greater_equal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_", "desc": "In-place version of greater_equal() "}, {"name": "torch.Tensor.geometric_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_", "desc": "Fills self tensor with elements drawn from the geometric distribution:"}, {"name": "torch.Tensor.geqrf", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf", "desc": "See torch.geqrf()"}, {"name": "torch.Tensor.ger", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ger.html#torch.Tensor.ger", "desc": "See torch.ger()"}, {"name": "torch.Tensor.get_device", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.get_device.html#torch.Tensor.get_device", "desc": "For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides."}, {"name": "torch.Tensor.gt", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.gt.html#torch.Tensor.gt", "desc": "See torch.gt() "}, {"name": "torch.Tensor.gt_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.gt_.html#torch.Tensor.gt_", "desc": "In-place version of gt() "}, {"name": "torch.Tensor.greater", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.greater.html#torch.Tensor.greater", "desc": "See torch.greater() "}, {"name": "torch.Tensor.greater_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.greater_.html#torch.Tensor.greater_", "desc": "In-place version of greater() "}, {"name": "torch.Tensor.half", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.half.html#torch.Tensor.half", "desc": "self.half() is equivalent to self.to(torch.float16) "}, {"name": "torch.Tensor.hardshrink", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink", "desc": "See torch.nn.functional.hardshrink()"}, {"name": "torch.Tensor.heaviside", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside", "desc": "See torch.heaviside()"}, {"name": "torch.Tensor.histc", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.histc.html#torch.Tensor.histc", "desc": "See torch.histc()"}, {"name": "torch.Tensor.histogram", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.histogram.html#torch.Tensor.histogram", "desc": "See torch.histogram()"}, {"name": "torch.Tensor.hsplit", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit", "desc": "See torch.hsplit()"}, {"name": "torch.Tensor.hypot", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot", "desc": "See torch.hypot()"}, {"name": "torch.Tensor.hypot_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_", "desc": "In-place version of hypot()"}, {"name": "torch.Tensor.i0", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.i0.html#torch.Tensor.i0", "desc": "See torch.i0()"}, {"name": "torch.Tensor.i0_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.i0_.html#torch.Tensor.i0_", "desc": "In-place version of i0()"}, {"name": "torch.Tensor.igamma", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma", "desc": "See torch.igamma()"}, {"name": "torch.Tensor.igamma_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_", "desc": "In-place version of igamma()"}, {"name": "torch.Tensor.igammac", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac", "desc": "See torch.igammac()"}, {"name": "torch.Tensor.igammac_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_", "desc": "In-place version of igammac()"}, {"name": "torch.Tensor.index_add_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_", "desc": "Accumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index "}, {"name": "torch.Tensor.index_add", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_add.html#torch.Tensor.index_add", "desc": "Out-of-place version of torch.Tensor.index_add_() "}, {"name": "torch.Tensor.index_copy_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_", "desc": "Copies the elements of tensor into the self tensor by selecting the indices in the order given in index "}, {"name": "torch.Tensor.index_copy", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy", "desc": "Out-of-place version of torch.Tensor.index_copy_() "}, {"name": "torch.Tensor.index_fill_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_", "desc": "Fills the elements of the self tensor with value value by selecting the indices in the order given in index "}, {"name": "torch.Tensor.index_fill", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill", "desc": "Out-of-place version of torch.Tensor.index_fill_() "}, {"name": "torch.Tensor.index_put_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_", "desc": "Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors)."}, {"name": "torch.Tensor.index_put", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_put.html#torch.Tensor.index_put", "desc": "Out-place version of index_put_() "}, {"name": "torch.Tensor.index_select", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.index_select.html#torch.Tensor.index_select", "desc": "See torch.index_select()"}, {"name": "torch.Tensor.indices", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.indices.html#torch.Tensor.indices", "desc": "Return the indices tensor of a sparse COO tensor "}, {"name": "torch.Tensor.inner", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.inner.html#torch.Tensor.inner", "desc": "See torch.inner() "}, {"name": "torch.Tensor.int", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.int.html#torch.Tensor.int", "desc": "self.int() is equivalent to self.to(torch.int32) "}, {"name": "torch.Tensor.int_repr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr", "desc": "Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."}, {"name": "torch.Tensor.inverse", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.inverse.html#torch.Tensor.inverse", "desc": "See torch.inverse()"}, {"name": "torch.Tensor.isclose", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isclose.html#torch.Tensor.isclose", "desc": "See torch.isclose()"}, {"name": "torch.Tensor.isfinite", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite", "desc": "See torch.isfinite()"}, {"name": "torch.Tensor.isinf", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isinf.html#torch.Tensor.isinf", "desc": "See torch.isinf()"}, {"name": "torch.Tensor.isposinf", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf", "desc": "See torch.isposinf()"}, {"name": "torch.Tensor.isneginf", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf", "desc": "See torch.isneginf()"}, {"name": "torch.Tensor.isnan", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isnan.html#torch.Tensor.isnan", "desc": "See torch.isnan()"}, {"name": "torch.Tensor.is_contiguous", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous", "desc": "Returns True if self tensor is contiguous in memory in the order specified by memory format."}, {"name": "torch.Tensor.is_complex", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex", "desc": "Returns True if the data type of self is a complex data type."}, {"name": "torch.Tensor.is_conj", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj", "desc": "Returns True if the conjugate bit of self is set to true."}, {"name": "torch.Tensor.is_floating_point", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point", "desc": "Returns True if the data type of self is a floating point data type."}, {"name": "torch.Tensor.is_inference", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference", "desc": "See torch.is_inference()"}, {"name": "torch.Tensor.is_leaf", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf", "desc": "All Tensors that have requires_grad which is False will be leaf Tensors by convention."}, {"name": "torch.Tensor.is_pinned", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned", "desc": "Returns true if this tensor resides in pinned memory."}, {"name": "torch.Tensor.is_set_to", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to", "desc": "Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride)."}, {"name": "torch.Tensor.is_shared", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared", "desc": "Checks if tensor is in shared memory."}, {"name": "torch.Tensor.is_signed", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed", "desc": "Returns True if the data type of self is a signed data type."}, {"name": "torch.Tensor.is_sparse", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse", "desc": "Is True if the Tensor uses sparse storage layout, False otherwise."}, {"name": "torch.Tensor.istft", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.istft.html#torch.Tensor.istft", "desc": "See torch.istft()"}, {"name": "torch.Tensor.isreal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.isreal.html#torch.Tensor.isreal", "desc": "See torch.isreal()"}, {"name": "torch.Tensor.item", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.item.html#torch.Tensor.item", "desc": "Returns the value of this tensor as a standard Python number."}, {"name": "torch.Tensor.kthvalue", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue", "desc": "See torch.kthvalue()"}, {"name": "torch.Tensor.lcm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm", "desc": "See torch.lcm()"}, {"name": "torch.Tensor.lcm_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_", "desc": "In-place version of lcm()"}, {"name": "torch.Tensor.ldexp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp", "desc": "See torch.ldexp()"}, {"name": "torch.Tensor.ldexp_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_", "desc": "In-place version of ldexp()"}, {"name": "torch.Tensor.le", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.le.html#torch.Tensor.le", "desc": "See torch.le() "}, {"name": "torch.Tensor.le_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.le_.html#torch.Tensor.le_", "desc": "In-place version of le() "}, {"name": "torch.Tensor.less_equal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal", "desc": "See torch.less_equal() "}, {"name": "torch.Tensor.less_equal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_", "desc": "In-place version of less_equal() "}, {"name": "torch.Tensor.lerp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp", "desc": "See torch.lerp()"}, {"name": "torch.Tensor.lerp_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_", "desc": "In-place version of lerp()"}, {"name": "torch.Tensor.lgamma", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma", "desc": "See torch.lgamma()"}, {"name": "torch.Tensor.lgamma_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_", "desc": "In-place version of lgamma()"}, {"name": "torch.Tensor.log", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log.html#torch.Tensor.log", "desc": "See torch.log()"}, {"name": "torch.Tensor.log_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log_.html#torch.Tensor.log_", "desc": "In-place version of log()"}, {"name": "torch.Tensor.logdet", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logdet.html#torch.Tensor.logdet", "desc": "See torch.logdet()"}, {"name": "torch.Tensor.log10", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log10.html#torch.Tensor.log10", "desc": "See torch.log10()"}, {"name": "torch.Tensor.log10_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log10_.html#torch.Tensor.log10_", "desc": "In-place version of log10()"}, {"name": "torch.Tensor.log1p", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p", "desc": "See torch.log1p()"}, {"name": "torch.Tensor.log1p_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_", "desc": "In-place version of log1p()"}, {"name": "torch.Tensor.log2", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log2.html#torch.Tensor.log2", "desc": "See torch.log2()"}, {"name": "torch.Tensor.log2_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log2_.html#torch.Tensor.log2_", "desc": "In-place version of log2()"}, {"name": "torch.Tensor.log_normal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_", "desc": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc \\mu \u03bc and standard deviation \u03c3 \\sigma \u03c3 "}, {"name": "torch.Tensor.logaddexp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp", "desc": "See torch.logaddexp()"}, {"name": "torch.Tensor.logaddexp2", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2", "desc": "See torch.logaddexp2()"}, {"name": "torch.Tensor.logsumexp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp", "desc": "See torch.logsumexp()"}, {"name": "torch.Tensor.logical_and", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and", "desc": "See torch.logical_and()"}, {"name": "torch.Tensor.logical_and_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_", "desc": "In-place version of logical_and()"}, {"name": "torch.Tensor.logical_not", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not", "desc": "See torch.logical_not()"}, {"name": "torch.Tensor.logical_not_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_", "desc": "In-place version of logical_not()"}, {"name": "torch.Tensor.logical_or", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or", "desc": "See torch.logical_or()"}, {"name": "torch.Tensor.logical_or_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_", "desc": "In-place version of logical_or()"}, {"name": "torch.Tensor.logical_xor", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor", "desc": "See torch.logical_xor()"}, {"name": "torch.Tensor.logical_xor_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_", "desc": "In-place version of logical_xor()"}, {"name": "torch.Tensor.logit", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logit.html#torch.Tensor.logit", "desc": "See torch.logit()"}, {"name": "torch.Tensor.logit_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.logit_.html#torch.Tensor.logit_", "desc": "In-place version of logit()"}, {"name": "torch.Tensor.long", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.long.html#torch.Tensor.long", "desc": "self.long() is equivalent to self.to(torch.int64) "}, {"name": "torch.Tensor.lstsq", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lstsq.html#torch.Tensor.lstsq", "desc": "See torch.lstsq()"}, {"name": "torch.Tensor.lt", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lt.html#torch.Tensor.lt", "desc": "See torch.lt() "}, {"name": "torch.Tensor.lt_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lt_.html#torch.Tensor.lt_", "desc": "In-place version of lt() "}, {"name": "torch.Tensor.less", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.less.html#torch.Tensor.less", "desc": "lt(other) -> Tensor"}, {"name": "torch.Tensor.less_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.less_.html#torch.Tensor.less_", "desc": "In-place version of less() "}, {"name": "torch.Tensor.lu", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lu.html#torch.Tensor.lu", "desc": "See torch.lu()"}, {"name": "torch.Tensor.lu_solve", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve", "desc": "See torch.lu_solve()"}, {"name": "torch.Tensor.as_subclass", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass", "desc": "Makes a cls instance with the same data pointer as self "}, {"name": "torch.Tensor.map_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.map_.html#torch.Tensor.map_", "desc": "Applies callable for each element in self tensor and the given tensor and stores the results in self tensor."}, {"name": "torch.Tensor.masked_scatter_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_", "desc": "Copies elements from source into self tensor at positions where the mask is True."}, {"name": "torch.Tensor.masked_scatter", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter", "desc": "Out-of-place version of torch.Tensor.masked_scatter_()"}, {"name": "torch.Tensor.masked_fill_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_", "desc": "Fills elements of self tensor with value where mask is True."}, {"name": "torch.Tensor.masked_fill", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill", "desc": "Out-of-place version of torch.Tensor.masked_fill_()"}, {"name": "torch.Tensor.masked_select", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select", "desc": "See torch.masked_select()"}, {"name": "torch.Tensor.matmul", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.matmul.html#torch.Tensor.matmul", "desc": "See torch.matmul()"}, {"name": "torch.Tensor.matrix_power", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power", "desc": "Note matrix_power() is deprecated, use torch.linalg.matrix_power() instead."}, {"name": "torch.Tensor.matrix_exp", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp", "desc": "See torch.matrix_exp()"}, {"name": "torch.Tensor.max", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.max.html#torch.Tensor.max", "desc": "See torch.max()"}, {"name": "torch.Tensor.maximum", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.maximum.html#torch.Tensor.maximum", "desc": "See torch.maximum()"}, {"name": "torch.Tensor.mean", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mean.html#torch.Tensor.mean", "desc": "See torch.mean()"}, {"name": "torch.Tensor.nanmean", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean", "desc": "See torch.nanmean()"}, {"name": "torch.Tensor.median", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.median.html#torch.Tensor.median", "desc": "See torch.median()"}, {"name": "torch.Tensor.nanmedian", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian", "desc": "See torch.nanmedian()"}, {"name": "torch.Tensor.min", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.min.html#torch.Tensor.min", "desc": "See torch.min()"}, {"name": "torch.Tensor.minimum", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.minimum.html#torch.Tensor.minimum", "desc": "See torch.minimum()"}, {"name": "torch.Tensor.mm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mm.html#torch.Tensor.mm", "desc": "See torch.mm()"}, {"name": "torch.Tensor.smm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.smm.html#torch.Tensor.smm", "desc": "See torch.smm()"}, {"name": "torch.Tensor.mode", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mode.html#torch.Tensor.mode", "desc": "See torch.mode()"}, {"name": "torch.Tensor.movedim", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.movedim.html#torch.Tensor.movedim", "desc": "See torch.movedim()"}, {"name": "torch.Tensor.moveaxis", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis", "desc": "See torch.moveaxis()"}, {"name": "torch.Tensor.msort", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.msort.html#torch.Tensor.msort", "desc": "See torch.msort()"}, {"name": "torch.Tensor.mul", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mul.html#torch.Tensor.mul", "desc": "See torch.mul() "}, {"name": "torch.Tensor.mul_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mul_.html#torch.Tensor.mul_", "desc": "In-place version of mul() "}, {"name": "torch.Tensor.multiply", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply", "desc": "See torch.multiply() "}, {"name": "torch.Tensor.multiply_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_", "desc": "In-place version of multiply() "}, {"name": "torch.Tensor.multinomial", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial", "desc": "See torch.multinomial()"}, {"name": "torch.Tensor.mv", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mv.html#torch.Tensor.mv", "desc": "See torch.mv()"}, {"name": "torch.Tensor.mvlgamma", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma", "desc": "See torch.mvlgamma()"}, {"name": "torch.Tensor.mvlgamma_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_", "desc": "In-place version of mvlgamma()"}, {"name": "torch.Tensor.nansum", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nansum.html#torch.Tensor.nansum", "desc": "See torch.nansum()"}, {"name": "torch.Tensor.narrow", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.narrow.html#torch.Tensor.narrow", "desc": "See torch.narrow()"}, {"name": "torch.Tensor.narrow_copy", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy", "desc": "Same as Tensor.narrow() except returning a copy rather than shared storage."}, {"name": "torch.Tensor.ndimension", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension", "desc": "Alias for dim()"}, {"name": "torch.Tensor.nan_to_num", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num", "desc": "See torch.nan_to_num() "}, {"name": "torch.Tensor.nan_to_num_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_", "desc": "In-place version of nan_to_num() "}, {"name": "torch.Tensor.ne", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ne.html#torch.Tensor.ne", "desc": "See torch.ne() "}, {"name": "torch.Tensor.ne_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ne_.html#torch.Tensor.ne_", "desc": "In-place version of ne() "}, {"name": "torch.Tensor.not_equal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal", "desc": "See torch.not_equal() "}, {"name": "torch.Tensor.not_equal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_", "desc": "In-place version of not_equal() "}, {"name": "torch.Tensor.neg", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.neg.html#torch.Tensor.neg", "desc": "See torch.neg()"}, {"name": "torch.Tensor.neg_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.neg_.html#torch.Tensor.neg_", "desc": "In-place version of neg()"}, {"name": "torch.Tensor.negative", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.negative.html#torch.Tensor.negative", "desc": "See torch.negative()"}, {"name": "torch.Tensor.negative_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.negative_.html#torch.Tensor.negative_", "desc": "In-place version of negative()"}, {"name": "torch.Tensor.nelement", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nelement.html#torch.Tensor.nelement", "desc": "Alias for numel()"}, {"name": "torch.Tensor.nextafter", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter", "desc": "See torch.nextafter()"}, {"name": "torch.Tensor.nextafter_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_", "desc": "In-place version of nextafter()"}, {"name": "torch.Tensor.nonzero", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero", "desc": "See torch.nonzero()"}, {"name": "torch.Tensor.norm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.norm.html#torch.Tensor.norm", "desc": "See torch.norm()"}, {"name": "torch.Tensor.normal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_", "desc": "Fills self tensor with elements samples from the normal distribution parameterized by mean and std "}, {"name": "torch.Tensor.numel", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.numel.html#torch.Tensor.numel", "desc": "See torch.numel()"}, {"name": "torch.Tensor.numpy", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy", "desc": "Returns self tensor as a NumPy ndarray "}, {"name": "torch.Tensor.orgqr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr", "desc": "See torch.orgqr()"}, {"name": "torch.Tensor.ormqr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr", "desc": "See torch.ormqr()"}, {"name": "torch.Tensor.outer", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.outer.html#torch.Tensor.outer", "desc": "See torch.outer() "}, {"name": "torch.Tensor.permute", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.permute.html#torch.Tensor.permute", "desc": "See torch.permute()"}, {"name": "torch.Tensor.pin_memory", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory", "desc": "Copies the tensor to pinned memory, if it\u2019s not already pinned."}, {"name": "torch.Tensor.pinverse", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse", "desc": "See torch.pinverse()"}, {"name": "torch.Tensor.polygamma", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma", "desc": "See torch.polygamma()"}, {"name": "torch.Tensor.polygamma_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_", "desc": "In-place version of polygamma()"}, {"name": "torch.Tensor.positive", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.positive.html#torch.Tensor.positive", "desc": "See torch.positive()"}, {"name": "torch.Tensor.pow", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.pow.html#torch.Tensor.pow", "desc": "See torch.pow()"}, {"name": "torch.Tensor.pow_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_", "desc": "In-place version of pow()"}, {"name": "torch.Tensor.prod", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.prod.html#torch.Tensor.prod", "desc": "See torch.prod()"}, {"name": "torch.Tensor.put_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.put_.html#torch.Tensor.put_", "desc": "Copies the elements from source into the positions specified by index "}, {"name": "torch.Tensor.qr", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.qr.html#torch.Tensor.qr", "desc": "See torch.qr()"}, {"name": "torch.Tensor.qscheme", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme", "desc": "Returns the quantization scheme of a given QTensor."}, {"name": "torch.Tensor.quantile", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.quantile.html#torch.Tensor.quantile", "desc": "See torch.quantile()"}, {"name": "torch.Tensor.nanquantile", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile", "desc": "See torch.nanquantile()"}, {"name": "torch.Tensor.q_scale", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale", "desc": "Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer()."}, {"name": "torch.Tensor.q_zero_point", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point", "desc": "Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."}, {"name": "torch.Tensor.q_per_channel_scales", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales", "desc": "Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer."}, {"name": "torch.Tensor.q_per_channel_zero_points", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points", "desc": "Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer."}, {"name": "torch.Tensor.q_per_channel_axis", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis", "desc": "Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."}, {"name": "torch.Tensor.rad2deg", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg", "desc": "See torch.rad2deg()"}, {"name": "torch.Tensor.random_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.random_.html#torch.Tensor.random_", "desc": "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1] "}, {"name": "torch.Tensor.ravel", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.ravel.html#torch.Tensor.ravel", "desc": "see torch.ravel()"}, {"name": "torch.Tensor.reciprocal", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal", "desc": "See torch.reciprocal()"}, {"name": "torch.Tensor.reciprocal_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_", "desc": "In-place version of reciprocal()"}, {"name": "torch.Tensor.record_stream", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream", "desc": "Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete."}, {"name": "torch.Tensor.register_hook", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook", "desc": "Registers a backward hook."}, {"name": "torch.Tensor.remainder", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder", "desc": "See torch.remainder()"}, {"name": "torch.Tensor.remainder_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_", "desc": "In-place version of remainder()"}, {"name": "torch.Tensor.renorm", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm", "desc": "See torch.renorm()"}, {"name": "torch.Tensor.renorm_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_", "desc": "In-place version of renorm()"}, {"name": "torch.Tensor.repeat", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat", "desc": "Repeats this tensor along the specified dimensions."}, {"name": "torch.Tensor.repeat_interleave", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave", "desc": "See torch.repeat_interleave() "}, {"name": "torch.Tensor.requires_grad", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad", "desc": "Is True if gradients need to be computed for this Tensor, False otherwise."}, {"name": "torch.Tensor.requires_grad_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_", "desc": "Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place."}, {"name": "torch.Tensor.reshape", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape", "desc": "Returns a tensor with the same data and number of elements as self but with the specified shape."}, {"name": "torch.Tensor.reshape_as", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as", "desc": "Returns this tensor as the same shape as other "}, {"name": "torch.Tensor.resize_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_", "desc": "Resizes self tensor to the specified size."}, {"name": "torch.Tensor.resize_as_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_", "desc": "Resizes the self tensor to be the same size as the specified tensor "}, {"name": "torch.Tensor.retain_grad", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad", "desc": "Enables this Tensor to have their grad populated during backward() "}, {"name": "torch.Tensor.retains_grad", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad", "desc": "Is True if this Tensor is non-leaf and its grad is enabled to be populated during backward() False otherwise."}, {"name": "torch.Tensor.roll", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.roll.html#torch.Tensor.roll", "desc": "See torch.roll()"}, {"name": "torch.Tensor.rot90", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.rot90.html#torch.Tensor.rot90", "desc": "See torch.rot90()"}, {"name": "torch.Tensor.round", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.round.html#torch.Tensor.round", "desc": "See torch.round()"}, {"name": "torch.Tensor.round_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.round_.html#torch.Tensor.round_", "desc": "In-place version of round()"}, {"name": "torch.Tensor.rsqrt", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt", "desc": "See torch.rsqrt()"}, {"name": "torch.Tensor.rsqrt_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_", "desc": "In-place version of rsqrt()"}, {"name": "torch.Tensor.scatter", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.scatter.html#torch.Tensor.scatter", "desc": "Out-of-place version of torch.Tensor.scatter_()"}, {"name": "torch.Tensor.scatter_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_", "desc": "Writes all values from the tensor src into self at the indices specified in the index tensor."}, {"name": "torch.Tensor.scatter_add_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_", "desc": "Adds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_() "}, {"name": "torch.Tensor.scatter_add", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add", "desc": "Out-of-place version of torch.Tensor.scatter_add_()"}, {"name": "torch.Tensor.scatter_reduce", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce", "desc": "See torch.scatter_reduce()"}, {"name": "torch.Tensor.select", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.select.html#torch.Tensor.select", "desc": "See torch.select()"}, {"name": "torch.Tensor.select_scatter", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter", "desc": "See torch.select_scatter()"}, {"name": "torch.Tensor.set_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.set_.html#torch.Tensor.set_", "desc": "Sets the underlying storage, size, and strides."}, {"name": "torch.Tensor.share_memory_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_", "desc": "Moves the underlying storage to shared memory."}, {"name": "torch.Tensor.short", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.short.html#torch.Tensor.short", "desc": "self.short() is equivalent to self.to(torch.int16) "}, {"name": "torch.Tensor.sigmoid", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid", "desc": "See torch.sigmoid()"}, {"name": "torch.Tensor.sigmoid_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_", "desc": "In-place version of sigmoid()"}, {"name": "torch.Tensor.sign", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sign.html#torch.Tensor.sign", "desc": "See torch.sign()"}, {"name": "torch.Tensor.sign_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sign_.html#torch.Tensor.sign_", "desc": "In-place version of sign()"}, {"name": "torch.Tensor.signbit", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.signbit.html#torch.Tensor.signbit", "desc": "See torch.signbit()"}, {"name": "torch.Tensor.sgn", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn", "desc": "See torch.sgn()"}, {"name": "torch.Tensor.sgn_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_", "desc": "In-place version of sgn()"}, {"name": "torch.Tensor.sin", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sin.html#torch.Tensor.sin", "desc": "See torch.sin()"}, {"name": "torch.Tensor.sin_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sin_.html#torch.Tensor.sin_", "desc": "In-place version of sin()"}, {"name": "torch.Tensor.sinc", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc", "desc": "See torch.sinc()"}, {"name": "torch.Tensor.sinc_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_", "desc": "In-place version of sinc()"}, {"name": "torch.Tensor.sinh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh", "desc": "See torch.sinh()"}, {"name": "torch.Tensor.sinh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_", "desc": "In-place version of sinh()"}, {"name": "torch.Tensor.asinh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh", "desc": "See torch.asinh()"}, {"name": "torch.Tensor.asinh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_", "desc": "In-place version of asinh()"}, {"name": "torch.Tensor.arcsinh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh", "desc": "See torch.arcsinh()"}, {"name": "torch.Tensor.arcsinh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_", "desc": "In-place version of arcsinh()"}, {"name": "torch.Tensor.size", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.size.html#torch.Tensor.size", "desc": "Returns the size of the self tensor."}, {"name": "torch.Tensor.slogdet", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet", "desc": "See torch.slogdet()"}, {"name": "torch.Tensor.slice_scatter", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter", "desc": "See torch.slice_scatter()"}, {"name": "torch.Tensor.solve", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.solve.html#torch.Tensor.solve", "desc": "See torch.solve()"}, {"name": "torch.Tensor.sort", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sort.html#torch.Tensor.sort", "desc": "See torch.sort()"}, {"name": "torch.Tensor.split", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.split.html#torch.Tensor.split", "desc": "See torch.split()"}, {"name": "torch.Tensor.sparse_mask", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask", "desc": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask "}, {"name": "torch.Tensor.sparse_dim", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim", "desc": "Return the number of sparse dimensions in a sparse tensor self "}, {"name": "torch.Tensor.sqrt", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt", "desc": "See torch.sqrt()"}, {"name": "torch.Tensor.sqrt_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_", "desc": "In-place version of sqrt()"}, {"name": "torch.Tensor.square", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.square.html#torch.Tensor.square", "desc": "See torch.square()"}, {"name": "torch.Tensor.square_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.square_.html#torch.Tensor.square_", "desc": "In-place version of square()"}, {"name": "torch.Tensor.squeeze", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze", "desc": "See torch.squeeze()"}, {"name": "torch.Tensor.squeeze_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_", "desc": "In-place version of squeeze()"}, {"name": "torch.Tensor.std", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.std.html#torch.Tensor.std", "desc": "See torch.std()"}, {"name": "torch.Tensor.stft", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.stft.html#torch.Tensor.stft", "desc": "See torch.stft()"}, {"name": "torch.Tensor.storage", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.storage.html#torch.Tensor.storage", "desc": "Returns the underlying storage."}, {"name": "torch.Tensor.storage_offset", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset", "desc": "Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes)."}, {"name": "torch.Tensor.storage_type", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type", "desc": "Returns the type of the underlying storage."}, {"name": "torch.Tensor.stride", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.stride.html#torch.Tensor.stride", "desc": "Returns the stride of self tensor."}, {"name": "torch.Tensor.sub", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sub.html#torch.Tensor.sub", "desc": "See torch.sub() "}, {"name": "torch.Tensor.sub_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sub_.html#torch.Tensor.sub_", "desc": "In-place version of sub()"}, {"name": "torch.Tensor.subtract", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract", "desc": "See torch.subtract() "}, {"name": "torch.Tensor.subtract_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_", "desc": "In-place version of subtract() "}, {"name": "torch.Tensor.sum", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sum.html#torch.Tensor.sum", "desc": "See torch.sum()"}, {"name": "torch.Tensor.sum_to_size", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size", "desc": "Sum this tensor to size "}, {"name": "torch.Tensor.svd", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.svd.html#torch.Tensor.svd", "desc": "See torch.svd()"}, {"name": "torch.Tensor.swapaxes", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes", "desc": "See torch.swapaxes()"}, {"name": "torch.Tensor.swapdims", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims", "desc": "See torch.swapdims()"}, {"name": "torch.Tensor.symeig", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.symeig.html#torch.Tensor.symeig", "desc": "See torch.symeig()"}, {"name": "torch.Tensor.t", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.t.html#torch.Tensor.t", "desc": "See torch.t()"}, {"name": "torch.Tensor.t_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.t_.html#torch.Tensor.t_", "desc": "In-place version of t()"}, {"name": "torch.Tensor.tensor_split", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split", "desc": "See torch.tensor_split()"}, {"name": "torch.Tensor.tile", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tile.html#torch.Tensor.tile", "desc": "See torch.tile()"}, {"name": "torch.Tensor.to", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.to.html#torch.Tensor.to", "desc": "Performs Tensor dtype and/or device conversion."}, {"name": "torch.Tensor.to_mkldnn", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn", "desc": "Returns a copy of the tensor in torch.mkldnn layout."}, {"name": "torch.Tensor.take", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.take.html#torch.Tensor.take", "desc": "See torch.take()"}, {"name": "torch.Tensor.take_along_dim", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim", "desc": "See torch.take_along_dim()"}, {"name": "torch.Tensor.tan", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tan.html#torch.Tensor.tan", "desc": "See torch.tan()"}, {"name": "torch.Tensor.tan_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tan_.html#torch.Tensor.tan_", "desc": "In-place version of tan()"}, {"name": "torch.Tensor.tanh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh", "desc": "See torch.tanh()"}, {"name": "torch.Tensor.tanh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_", "desc": "In-place version of tanh()"}, {"name": "torch.Tensor.atanh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh", "desc": "See torch.atanh()"}, {"name": "torch.Tensor.atanh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_", "desc": "In-place version of atanh()"}, {"name": "torch.Tensor.arctanh", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh", "desc": "See torch.arctanh()"}, {"name": "torch.Tensor.arctanh_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_", "desc": "In-place version of arctanh()"}, {"name": "torch.Tensor.tolist", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist", "desc": "Returns the tensor as a (nested) list."}, {"name": "torch.Tensor.topk", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.topk.html#torch.Tensor.topk", "desc": "See torch.topk()"}, {"name": "torch.Tensor.to_sparse", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse", "desc": "Returns a sparse copy of the tensor."}, {"name": "torch.Tensor.trace", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.trace.html#torch.Tensor.trace", "desc": "See torch.trace()"}, {"name": "torch.Tensor.transpose", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose", "desc": "See torch.transpose()"}, {"name": "torch.Tensor.transpose_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_", "desc": "In-place version of transpose()"}, {"name": "torch.Tensor.triangular_solve", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve", "desc": "See torch.triangular_solve()"}, {"name": "torch.Tensor.tril", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tril.html#torch.Tensor.tril", "desc": "See torch.tril()"}, {"name": "torch.Tensor.tril_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.tril_.html#torch.Tensor.tril_", "desc": "In-place version of tril()"}, {"name": "torch.Tensor.triu", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.triu.html#torch.Tensor.triu", "desc": "See torch.triu()"}, {"name": "torch.Tensor.triu_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.triu_.html#torch.Tensor.triu_", "desc": "In-place version of triu()"}, {"name": "torch.Tensor.true_divide", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide", "desc": "See torch.true_divide()"}, {"name": "torch.Tensor.true_divide_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_", "desc": "In-place version of true_divide_()"}, {"name": "torch.Tensor.trunc", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc", "desc": "See torch.trunc()"}, {"name": "torch.Tensor.trunc_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_", "desc": "In-place version of trunc()"}, {"name": "torch.Tensor.type", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.type.html#torch.Tensor.type", "desc": "Returns the type if dtype is not provided, else casts this object to the specified type."}, {"name": "torch.Tensor.type_as", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.type_as.html#torch.Tensor.type_as", "desc": "Returns this tensor cast to the type of the given tensor."}, {"name": "torch.Tensor.unbind", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.unbind.html#torch.Tensor.unbind", "desc": "See torch.unbind()"}, {"name": "torch.Tensor.unfold", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.unfold.html#torch.Tensor.unfold", "desc": "Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension "}, {"name": "torch.Tensor.uniform_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_", "desc": "Fills self tensor with numbers sampled from the continuous uniform distribution:"}, {"name": "torch.Tensor.unique", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.unique.html#torch.Tensor.unique", "desc": "Returns the unique elements of the input tensor."}, {"name": "torch.Tensor.unique_consecutive", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive", "desc": "Eliminates all but the first element from every consecutive group of equivalent elements."}, {"name": "torch.Tensor.unsqueeze", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze", "desc": "See torch.unsqueeze()"}, {"name": "torch.Tensor.unsqueeze_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_", "desc": "In-place version of unsqueeze()"}, {"name": "torch.Tensor.values", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.values.html#torch.Tensor.values", "desc": "Return the values tensor of a sparse COO tensor "}, {"name": "torch.Tensor.var", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.var.html#torch.Tensor.var", "desc": "See torch.var()"}, {"name": "torch.Tensor.vdot", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.vdot.html#torch.Tensor.vdot", "desc": "See torch.vdot()"}, {"name": "torch.Tensor.view", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.view.html#torch.Tensor.view", "desc": "Returns a new tensor with the same data as the self tensor but of a different shape "}, {"name": "torch.Tensor.view_as", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.view_as.html#torch.Tensor.view_as", "desc": "View this tensor as the same size as other "}, {"name": "torch.Tensor.vsplit", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit", "desc": "See torch.vsplit()"}, {"name": "torch.Tensor.where", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.where.html#torch.Tensor.where", "desc": "self.where(condition, y) is equivalent to torch.where(condition, self, y) "}, {"name": "torch.Tensor.xlogy", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy", "desc": "See torch.xlogy()"}, {"name": "torch.Tensor.xlogy_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_", "desc": "In-place version of xlogy()"}, {"name": "torch.Tensor.zero_", "type": "torch.tensors", "path": "stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_", "desc": "Fills self tensor with zeros."}, {"name": "torch.Tensor", "type": "torchtorch.tensors", "path": "stable/tensors.html#torch.Tensor", "desc": "There are a few main ways to create a tensor, depending on your use case."}, {"name": "torch.torch.dtype", "type": "torchtorch.tensor_attributes", "path": "stable/tensor_attributes.html#torch.torch.dtype", "desc": "."}, {"name": "torch.torch.device", "type": "torchtorch.tensor_attributes", "path": "stable/tensor_attributes.html#torch.torch.device", "desc": "."}, {"name": "torch.torch.layout", "type": "torchtorch.tensor_attributes", "path": "stable/tensor_attributes.html#torch.torch.layout", "desc": "."}, {"name": "torch.torch.memory_format", "type": "torchtorch.tensor_attributes", "path": "stable/tensor_attributes.html#torch.torch.memory_format", "desc": "."}, {"name": "torch.autograd.backward", "type": "torch.autograd", "path": "stable/generated/torch.autograd.backward.html#torch.autograd.backward", "desc": "Computes the sum of gradients of given tensors with respect to graph leaves."}, {"name": "torch.autograd.grad", "type": "torch.autograd", "path": "stable/generated/torch.autograd.grad.html#torch.autograd.grad", "desc": "Computes and returns the sum of gradients of outputs with respect to the inputs."}, {"name": "torch.autograd.forward_ad.dual_level", "type": "torch.autograd", "path": "stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level", "desc": "Context-manager that enables forward AD."}, {"name": "torch.autograd.forward_ad.make_dual", "type": "torch.autograd", "path": "stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual", "desc": "Associates a tensor value with a forward gradient, the tangent, to create a \u201cdual tensor\u201d, which is used to compute forward AD gradients."}, {"name": "torch.autograd.forward_ad.unpack_dual", "type": "torch.autograd", "path": "stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual", "desc": "Unpacks a \u201cdual tensor\u201d to get both its Tensor value and its forward AD gradient."}, {"name": "torch.autograd.functional.jacobian", "type": "torch.autograd", "path": "stable/generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian", "desc": "Function that computes the Jacobian of a given function."}, {"name": "torch.autograd.functional.hessian", "type": "torch.autograd", "path": "stable/generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian", "desc": "Function that computes the Hessian of a given scalar function."}, {"name": "torch.autograd.functional.vjp", "type": "torch.autograd", "path": "stable/generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp", "desc": "Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs."}, {"name": "torch.autograd.functional.jvp", "type": "torch.autograd", "path": "stable/generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp", "desc": "Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v "}, {"name": "torch.autograd.functional.vhp", "type": "torch.autograd", "path": "stable/generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp", "desc": "Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs."}, {"name": "torch.autograd.functional.hvp", "type": "torch.autograd", "path": "stable/generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp", "desc": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs."}, {"name": "torch.autograd.no_grad", "type": "torch.autograd", "path": "stable/generated/torch.autograd.no_grad.html#torch.autograd.no_grad", "desc": "Context-manager that disabled gradient calculation."}, {"name": "torch.autograd.enable_grad", "type": "torch.autograd", "path": "stable/generated/torch.autograd.enable_grad.html#torch.autograd.enable_grad", "desc": "Context-manager that enables gradient calculation."}, {"name": "torch.autograd.set_grad_enabled", "type": "torch.autograd", "path": "stable/generated/torch.autograd.set_grad_enabled.html#torch.autograd.set_grad_enabled", "desc": "Context-manager that sets gradient calculation to on or off."}, {"name": "torch.autograd.inference_mode", "type": "torch.autograd", "path": "stable/generated/torch.autograd.inference_mode.html#torch.autograd.inference_mode", "desc": "Context-manager that enables or disables inference mode"}, {"name": "torch.autograd.Function.forward", "type": "torch.autograd", "path": "stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward", "desc": "Performs the operation."}, {"name": "torch.autograd.Function.backward", "type": "torch.autograd", "path": "stable/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward", "desc": "Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function)."}, {"name": "torch.autograd.Function.jvp", "type": "torch.autograd", "path": "stable/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp", "desc": "Defines a formula for differentiating the operation with forward mode automatic differentiation."}, {"name": "torch.autograd.function.FunctionCtx.mark_dirty", "type": "torch.autograd", "path": "stable/generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty", "desc": "Marks given tensors as modified in an in-place operation."}, {"name": "torch.autograd.function.FunctionCtx.mark_non_differentiable", "type": "torch.autograd", "path": "stable/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable", "desc": "Marks outputs as non-differentiable."}, {"name": "torch.autograd.function.FunctionCtx.save_for_backward", "type": "torch.autograd", "path": "stable/generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward", "desc": "Saves given tensors for a future call to backward() "}, {"name": "torch.autograd.function.FunctionCtx.set_materialize_grads", "type": "torch.autograd", "path": "stable/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads", "desc": "Sets whether to materialize output grad tensors."}, {"name": "torch.autograd.gradcheck", "type": "torch.autograd", "path": "stable/generated/torch.autograd.gradcheck.html#torch.autograd.gradcheck", "desc": "Check gradients computed via small finite differences against analytical gradients w.r.t."}, {"name": "torch.autograd.gradgradcheck", "type": "torch.autograd", "path": "stable/generated/torch.autograd.gradgradcheck.html#torch.autograd.gradgradcheck", "desc": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t."}, {"name": "torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "path": "stable/generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace", "desc": "Exports an EventList as a Chrome tracing tools file."}, {"name": "torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "path": "stable/generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages", "desc": "Averages all function events over their keys."}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "path": "stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total", "desc": "Returns total time spent on CPU obtained as a sum of all self times across all the events."}, {"name": "torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "path": "stable/generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average", "desc": "Averages all events."}, {"name": "torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "path": "stable/generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof", "desc": "Opens an nvprof trace file and parses autograd annotations."}, {"name": "torch.autograd.Function", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.Function", "desc": "Base class to create custom autograd."}, {"name": "torch.autograd.profiler.profile", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.profiler.profile", "desc": "Context manager that manages autograd profiler state and holds a summary of results."}, {"name": "torch.autograd.profiler.emit_nvtx", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.profiler.emit_nvtx", "desc": "Context manager that makes every autograd operation emit an NVTX range."}, {"name": "torch.autograd.detect_anomaly", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.detect_anomaly", "desc": "Context-manager that enable anomaly detection for the autograd engine."}, {"name": "torch.autograd.set_detect_anomaly", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.set_detect_anomaly", "desc": "Context-manager that sets the anomaly detection for the autograd engine on or off."}, {"name": "torch.autograd.graph.saved_tensors_hooks", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.graph.saved_tensors_hooks", "desc": "Context-manager that sets a pair of pack / unpack hooks for saved tensors."}, {"name": "torch.autograd.graph.save_on_cpu", "type": "torchtorch.autograd", "path": "stable/autograd.html#torch.autograd.graph.save_on_cpu", "desc": "Context-manager under which tensors saved by the forward pass will bestored on cpu, then retrieved for backward."}, {"name": "torch.cuda.StreamContext", "type": "torch.cuda", "path": "stable/generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext", "desc": "Context-manager that selects a given stream."}, {"name": "torch.cuda.can_device_access_peer", "type": "torch.cuda", "path": "stable/generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer", "desc": "Checks if peer access between two devices is possible."}, {"name": "torch.cuda.current_blas_handle", "type": "torch.cuda", "path": "stable/generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle", "desc": "Returns cublasHandle_t pointer to current cuBLAS handle"}, {"name": "torch.cuda.current_device", "type": "torch.cuda", "path": "stable/generated/torch.cuda.current_device.html#torch.cuda.current_device", "desc": "Returns the index of a currently selected device."}, {"name": "torch.cuda.current_stream", "type": "torch.cuda", "path": "stable/generated/torch.cuda.current_stream.html#torch.cuda.current_stream", "desc": "Returns the currently selected Stream for a given device."}, {"name": "torch.cuda.default_stream", "type": "torch.cuda", "path": "stable/generated/torch.cuda.default_stream.html#torch.cuda.default_stream", "desc": "Returns the default Stream for a given device."}, {"name": "torch.cuda.device", "type": "torch.cuda", "path": "stable/generated/torch.cuda.device.html#torch.cuda.device", "desc": "Context-manager that changes the selected device."}, {"name": "torch.cuda.device_count", "type": "torch.cuda", "path": "stable/generated/torch.cuda.device_count.html#torch.cuda.device_count", "desc": "Returns the number of GPUs available."}, {"name": "torch.cuda.device_of", "type": "torch.cuda", "path": "stable/generated/torch.cuda.device_of.html#torch.cuda.device_of", "desc": "Context-manager that changes the current device to that of given object."}, {"name": "torch.cuda.get_arch_list", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list", "desc": "Returns list CUDA architectures this library was compiled for."}, {"name": "torch.cuda.get_device_capability", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability", "desc": "Gets the cuda capability of a device."}, {"name": "torch.cuda.get_device_name", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name", "desc": "Gets the name of a device."}, {"name": "torch.cuda.get_device_properties", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties", "desc": "Gets the properties of a device."}, {"name": "torch.cuda.get_gencode_flags", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags", "desc": "Returns NVCC gencode flags this library was compiled with."}, {"name": "torch.cuda.get_sync_debug_mode", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode", "desc": "Returns current value of debug mode for cuda synchronizing operations."}, {"name": "torch.cuda.init", "type": "torch.cuda", "path": "stable/generated/torch.cuda.init.html#torch.cuda.init", "desc": "Initialize PyTorch\u2019s CUDA state."}, {"name": "torch.cuda.ipc_collect", "type": "torch.cuda", "path": "stable/generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect", "desc": "Force collects GPU memory after it has been released by CUDA IPC."}, {"name": "torch.cuda.is_available", "type": "torch.cuda", "path": "stable/generated/torch.cuda.is_available.html#torch.cuda.is_available", "desc": "Returns a bool indicating if CUDA is currently available."}, {"name": "torch.cuda.is_initialized", "type": "torch.cuda", "path": "stable/generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized", "desc": "Returns whether PyTorch\u2019s CUDA state has been initialized."}, {"name": "torch.cuda.memory_usage", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage", "desc": "Returns the percent of time over the past sample period during which global (device) memory was being read or written."}, {"name": "torch.cuda.set_device", "type": "torch.cuda", "path": "stable/generated/torch.cuda.set_device.html#torch.cuda.set_device", "desc": "Sets the current device."}, {"name": "torch.cuda.set_stream", "type": "torch.cuda", "path": "stable/generated/torch.cuda.set_stream.html#torch.cuda.set_stream", "desc": "Sets the current stream.This is a wrapper API to set the stream."}, {"name": "torch.cuda.set_sync_debug_mode", "type": "torch.cuda", "path": "stable/generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode", "desc": "Sets the debug mode for cuda synchronizing operations."}, {"name": "torch.cuda.stream", "type": "torch.cuda", "path": "stable/generated/torch.cuda.stream.html#torch.cuda.stream", "desc": "Wrapper around the Context-manager StreamContext that selects a given stream."}, {"name": "torch.cuda.synchronize", "type": "torch.cuda", "path": "stable/generated/torch.cuda.synchronize.html#torch.cuda.synchronize", "desc": "Waits for all kernels in all streams on a CUDA device to complete."}, {"name": "torch.cuda.utilization", "type": "torch.cuda", "path": "stable/generated/torch.cuda.utilization.html#torch.cuda.utilization", "desc": "Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi "}, {"name": "torch.cuda.get_rng_state", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state", "desc": "Returns the random number generator state of the specified GPU as a ByteTensor."}, {"name": "torch.cuda.get_rng_state_all", "type": "torch.cuda", "path": "stable/generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all", "desc": "Returns a list of ByteTensor representing the random number states of all devices."}, {"name": "torch.cuda.set_rng_state", "type": "torch.cuda", "path": "stable/generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state", "desc": "Sets the random number generator state of the specified GPU."}, {"name": "torch.cuda.set_rng_state_all", "type": "torch.cuda", "path": "stable/generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all", "desc": "Sets the random number generator state of all devices."}, {"name": "torch.cuda.manual_seed", "type": "torch.cuda", "path": "stable/generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed", "desc": "Sets the seed for generating random numbers for the current GPU."}, {"name": "torch.cuda.manual_seed_all", "type": "torch.cuda", "path": "stable/generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all", "desc": "Sets the seed for generating random numbers on all GPUs."}, {"name": "torch.cuda.seed", "type": "torch.cuda", "path": "stable/generated/torch.cuda.seed.html#torch.cuda.seed", "desc": "Sets the seed for generating random numbers to a random number for the current GPU."}, {"name": "torch.cuda.seed_all", "type": "torch.cuda", "path": "stable/generated/torch.cuda.seed_all.html#torch.cuda.seed_all", "desc": "Sets the seed for generating random numbers to a random number on all GPUs."}, {"name": "torch.cuda.initial_seed", "type": "torch.cuda", "path": "stable/generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed", "desc": "Returns the current random seed of the current GPU."}, {"name": "torch.cuda.comm.broadcast", "type": "torch.cuda", "path": "stable/generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast", "desc": "Broadcasts a tensor to specified GPU devices."}, {"name": "torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "path": "stable/generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced", "desc": "Broadcasts a sequence tensors to the specified GPUs."}, {"name": "torch.cuda.comm.reduce_add", "type": "torch.cuda", "path": "stable/generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add", "desc": "Sums tensors from multiple GPUs."}, {"name": "torch.cuda.comm.scatter", "type": "torch.cuda", "path": "stable/generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter", "desc": "Scatters tensor across multiple GPUs."}, {"name": "torch.cuda.comm.gather", "type": "torch.cuda", "path": "stable/generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather", "desc": "Gathers tensors from multiple GPU devices."}, {"name": "torch.cuda.Stream", "type": "torch.cuda", "path": "stable/generated/torch.cuda.Stream.html#torch.cuda.Stream", "desc": "Wrapper around a CUDA stream."}, {"name": "torch.cuda.ExternalStream", "type": "torch.cuda", "path": "stable/generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream", "desc": "Wrapper around an externally allocated CUDA stream."}, {"name": "torch.cuda.Event", "type": "torch.cuda", "path": "stable/generated/torch.cuda.Event.html#torch.cuda.Event", "desc": "Wrapper around a CUDA event."}, {"name": "torch.cuda.graph_pool_handle", "type": "torch.cuda", "path": "stable/generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle", "desc": "Returns an opaque token representing the id of a graph memory pool."}, {"name": "torch.cuda.CUDAGraph", "type": "torch.cuda", "path": "stable/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph", "desc": "Wrapper around a CUDA graph."}, {"name": "torch.cuda.graph", "type": "torch.cuda", "path": "stable/generated/torch.cuda.graph.html#torch.cuda.graph", "desc": "Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay."}, {"name": "torch.cuda.make_graphed_callables", "type": "torch.cuda", "path": "stable/generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables", "desc": "Accepts callables (functions or nn.Module s) and returns graphed versions."}, {"name": "torch.cuda.empty_cache", "type": "torch.cuda", "path": "stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache", "desc": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi "}, {"name": "torch.cuda.list_gpu_processes", "type": "torch.cuda", "path": "stable/generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes", "desc": "Returns a human-readable printout of the running processes and their GPU memory use for a given device."}, {"name": "torch.cuda.mem_get_info", "type": "torch.cuda", "path": "stable/generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info", "desc": "Returns the global free and total GPU memory occupied for a given device using cudaMemGetInfo."}, {"name": "torch.cuda.memory_stats", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats", "desc": "Returns a dictionary of CUDA memory allocator statistics for a given device."}, {"name": "torch.cuda.memory_summary", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary", "desc": "Returns a human-readable printout of the current memory allocator statistics for a given device."}, {"name": "torch.cuda.memory_snapshot", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot", "desc": "Returns a snapshot of the CUDA memory allocator state across all devices."}, {"name": "torch.cuda.memory_allocated", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated", "desc": "Returns the current GPU memory occupied by tensors in bytes for a given device."}, {"name": "torch.cuda.max_memory_allocated", "type": "torch.cuda", "path": "stable/generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated", "desc": "Returns the maximum GPU memory occupied by tensors in bytes for a given device."}, {"name": "torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "path": "stable/generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated", "desc": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device."}, {"name": "torch.cuda.memory_reserved", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved", "desc": "Returns the current GPU memory managed by the caching allocator in bytes for a given device."}, {"name": "torch.cuda.max_memory_reserved", "type": "torch.cuda", "path": "stable/generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved", "desc": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device."}, {"name": "torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "path": "stable/generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction", "desc": "Set memory fraction for a process."}, {"name": "torch.cuda.memory_cached", "type": "torch.cuda", "path": "stable/generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached", "desc": "Deprecated; see memory_reserved() "}, {"name": "torch.cuda.max_memory_cached", "type": "torch.cuda", "path": "stable/generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached", "desc": "Deprecated; see max_memory_reserved() "}, {"name": "torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "path": "stable/generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached", "desc": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device."}, {"name": "torch.cuda.reset_peak_memory_stats", "type": "torch.cuda", "path": "stable/generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats", "desc": "Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator."}, {"name": "torch.cuda.caching_allocator_alloc", "type": "torch.cuda", "path": "stable/generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc", "desc": "Performs a memory allocation using the CUDA memory allocator."}, {"name": "torch.cuda.caching_allocator_delete", "type": "torch.cuda", "path": "stable/generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete", "desc": "Deletes memory allocated using the CUDA memory allocator."}, {"name": "torch.cuda.nvtx.mark", "type": "torch.cuda", "path": "stable/generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark", "desc": "Describe an instantaneous event that occurred at some point."}, {"name": "torch.cuda.nvtx.range_push", "type": "torch.cuda", "path": "stable/generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push", "desc": "Pushes a range onto a stack of nested range span."}, {"name": "torch.cuda.nvtx.range_pop", "type": "torch.cuda", "path": "stable/generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop", "desc": "Pops a range off of a stack of nested range spans."}, {"name": "torch.autocast", "type": "torchtorch.amp", "path": "stable/amp.html#torch.autocast", "desc": "Instances of autocast serve as context managers or decorators thatallow regions of your script to run in mixed precision."}, {"name": "torch.cuda.amp.autocast", "type": "torchtorch.amp", "path": "stable/amp.html#torch.cuda.amp.autocast", "desc": "See torch."}, {"name": "torch.cuda.amp.custom_fwd", "type": "torchtorch.amp", "path": "stable/amp.html#torch.cuda.amp.custom_fwd", "desc": "Helper decorator for forward methods of custom autograd functions (subclasses of torch."}, {"name": "torch.cuda.amp.custom_bwd", "type": "torchtorch.amp", "path": "stable/amp.html#torch.cuda.amp.custom_bwd", "desc": "Helper decorator for backward methods of custom autograd functions (subclasses of torch."}, {"name": "torch.cuda.amp.GradScaler", "type": "torchtorch.amp", "path": "stable/amp.html#torch.cuda.amp.GradScaler", "desc": "get_backoff_factor ( ) [source] \u00b6 Returns a Python float containing the scale backoff factor."}, {"name": "torch.backends.cuda.is_built", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.cuda.is_built", "desc": "Returns whether PyTorch is built with CUDA support."}, {"name": "torch.backends.cuda.preferred_linalg_library", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.cuda.preferred_linalg_library", "desc": "Warning This flag is experimental and subject to change."}, {"name": "torch.backends.cudnn.version", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.cudnn.version", "desc": "Returns the version of cuDNN."}, {"name": "torch.backends.cudnn.is_available", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.cudnn.is_available", "desc": "Returns a bool indicating if CUDNN is currently available."}, {"name": "torch.backends.mkl.is_available", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.mkl.is_available", "desc": "Returns whether PyTorch is built with MKL support."}, {"name": "torch.backends.mkldnn.is_available", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.mkldnn.is_available", "desc": "Returns whether PyTorch is built with MKL-DNN support."}, {"name": "torch.backends.openmp.is_available", "type": "torchtorch.backends", "path": "stable/backends.html#torch.backends.openmp.is_available", "desc": "Returns whether PyTorch is built with OpenMP support."}, {"name": "torch.distributed.is_available", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.is_available", "desc": "Returns True if the distributed package is available."}, {"name": "torch.distributed.init_process_group", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.init_process_group", "desc": "Initializes the default distributed process group, and this will alsoinitialize the distributed package."}, {"name": "torch.distributed.is_initialized", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.is_initialized", "desc": "Checking if the default process group has been initialized."}, {"name": "torch.distributed.is_mpi_available", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.is_mpi_available", "desc": "Checks if the MPI backend is available."}, {"name": "torch.distributed.is_nccl_available", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.is_nccl_available", "desc": "Checks if the NCCL backend is available."}, {"name": "torch.distributed.is_torchelastic_launched", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.is_torchelastic_launched", "desc": "Checks whether this process was launched with torch."}, {"name": "torch.distributed.Backend", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Backend", "desc": "An enum-like class of available backends: GLOO, NCCL, MPI, and other registeredbackends."}, {"name": "torch.distributed.get_backend", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.get_backend", "desc": "Returns the backend of the given process group."}, {"name": "torch.distributed.get_rank", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.get_rank", "desc": "Returns the rank of the current process in the provided group or thedefault group if none was provided."}, {"name": "torch.distributed.get_world_size", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.get_world_size", "desc": "Returns the number of processes in the current process group Parameters group ( ProcessGroup optional ) \u2013 The process group to work on."}, {"name": "torch.distributed.Store", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store", "desc": "Base class for all store implementations, such as the 3 provided by PyTorchdistributed: ( TCPStore FileStore and HashStore )."}, {"name": "torch.distributed.TCPStore", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.TCPStore", "desc": "A TCP-based distributed key-value store implementation."}, {"name": "torch.distributed.HashStore", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.HashStore", "desc": "A thread-safe store implementation based on an underlying hashmap."}, {"name": "torch.distributed.FileStore", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.FileStore", "desc": "A store implementation that uses a file to store the underlying key-value pairs."}, {"name": "torch.distributed.PrefixStore", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.PrefixStore", "desc": "A wrapper around any of the 3 key-value stores ( TCPStore FileStore and HashStore )that adds a prefix to each key inserted to the store."}, {"name": "torch.distributed.Store.set", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.set", "desc": "Inserts the key-value pair into the store based on the supplied key and value If key already exists in the store, it will overwrite the oldvalue with the new supplied value Parameters key ( str ) \u2013 The key to be added to the store."}, {"name": "torch.distributed.Store.get", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.get", "desc": "Retrieves the value associated with the given key in the store."}, {"name": "torch.distributed.Store.add", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.add", "desc": "The first call to add for a given key creates a counter associatedwith key in the store, initialized to amount Subsequent calls to addwith the same key increment the counter by the specified amount Calling add() with a key that has alreadybeen set in the store by set() will resultin an exception."}, {"name": "torch.distributed.Store.compare_set", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.compare_set", "desc": "Inserts the key-value pair into the store based on the supplied key andperforms comparison between expected_value and desired_value before inserting."}, {"name": "torch.distributed.Store.wait", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.wait", "desc": "Overloaded function."}, {"name": "torch.distributed.Store.num_keys", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.num_keys", "desc": "Returns the number of keys set in the store."}, {"name": "torch.distributed.Store.delete_key", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.delete_key", "desc": "Deletes the key-value pair associated with key from the store."}, {"name": "torch.distributed.Store.set_timeout", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.Store.set_timeout", "desc": "Sets the store\u2019s default timeout."}, {"name": "torch.distributed.new_group", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.new_group", "desc": "Creates a new distributed group."}, {"name": "torch.distributed.send", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.send", "desc": "Sends a tensor synchronously."}, {"name": "torch.distributed.recv", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.recv", "desc": "Receives a tensor synchronously."}, {"name": "torch.distributed.isend", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.isend", "desc": "Sends a tensor asynchronously."}, {"name": "torch.distributed.irecv", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.irecv", "desc": "Receives a tensor asynchronously."}, {"name": "torch.distributed.broadcast", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.broadcast", "desc": "Broadcasts the tensor to the whole group."}, {"name": "torch.distributed.broadcast_object_list", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.broadcast_object_list", "desc": "Broadcasts picklable objects in object_list to the whole group."}, {"name": "torch.distributed.all_reduce", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.all_reduce", "desc": "Reduces the tensor data across all machines in such a way that all getthe final result."}, {"name": "torch.distributed.reduce", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.reduce", "desc": "Reduces the tensor data across all machines."}, {"name": "torch.distributed.all_gather", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.all_gather", "desc": "Gathers tensors from the whole group in a list."}, {"name": "torch.distributed.all_gather_object", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.all_gather_object", "desc": "Gathers picklable objects from the whole group into a list."}, {"name": "torch.distributed.gather", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.gather", "desc": "Gathers a list of tensors in a single process."}, {"name": "torch.distributed.gather_object", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.gather_object", "desc": "Gathers picklable objects from the whole group in a single process."}, {"name": "torch.distributed.scatter", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.scatter", "desc": "Scatters a list of tensors to all processes in a group."}, {"name": "torch.distributed.scatter_object_list", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.scatter_object_list", "desc": "Scatters picklable objects in scatter_object_input_list to the wholegroup."}, {"name": "torch.distributed.reduce_scatter", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.reduce_scatter", "desc": "Reduces, then scatters a list of tensors to all processes in a group."}, {"name": "torch.distributed.all_to_all", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.all_to_all", "desc": "Each process scatters list of input tensors to all processes in a group andreturn gathered list of tensors in output list."}, {"name": "torch.distributed.barrier", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.barrier", "desc": "Synchronizes all processes."}, {"name": "torch.distributed.monitored_barrier", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.monitored_barrier", "desc": "Synchronizes all processes similar to torch."}, {"name": "torch.distributed.ReduceOp", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.ReduceOp", "desc": "An enum-like class for available reduction operations: SUM AVG PRODUCT MIN MAX BAND BOR and BXOR BAND BOR and BXOR reductions are not available whenusing the NCCL backend."}, {"name": "torch.distributed.reduce_op", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.reduce_op", "desc": "Deprecated enum-like class for reduction operations: SUM PRODUCT MIN and MAX ReduceOp is recommended to use instead."}, {"name": "torch.distributed.broadcast_multigpu", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.broadcast_multigpu", "desc": "Broadcasts the tensor to the whole group with multiple GPU tensorsper node."}, {"name": "torch.distributed.all_reduce_multigpu", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.all_reduce_multigpu", "desc": "Reduces the tensor data across all machines in such a way that all getthe final result."}, {"name": "torch.distributed.reduce_multigpu", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.reduce_multigpu", "desc": "Reduces the tensor data on multiple GPUs across all machines."}, {"name": "torch.distributed.all_gather_multigpu", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.all_gather_multigpu", "desc": "Gathers tensors from the whole group in a list."}, {"name": "torch.distributed.reduce_scatter_multigpu", "type": "torchtorch.distributed", "path": "stable/distributed.html#torch.distributed.reduce_scatter_multigpu", "desc": "Reduce and scatter a list of tensors to the whole group."}, {"name": "torch.distributed.algorithms.Join", "type": "torchtorch.distributed.algorithms.join", "path": "stable/distributed.algorithms.join.html#torch.distributed.algorithms.Join", "desc": "This class defines the generic join context manager, which allows customhooks to be called after a process joins."}, {"name": "torch.distributed.algorithms.Joinable", "type": "torchtorch.distributed.algorithms.join", "path": "stable/distributed.algorithms.join.html#torch.distributed.algorithms.Joinable", "desc": "This defines an abstract base class for joinable classes."}, {"name": "torch.distributed.algorithms.JoinHook", "type": "torchtorch.distributed.algorithms.join", "path": "stable/distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook", "desc": "This defines a join hook, which provides two entry points in the joincontext manager: a main hook, which is called repeatedly while there existsa non-joined process, and a post-hook, which is called once all processeshave joined."}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel", "type": "torchtorch.fsdp", "path": "stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel", "desc": "A wrapper for sharding Module parameters across data parallel workers."}, {"name": "torch.distributed.optim.DistributedOptimizer", "type": "torchtorch.distributed.optim", "path": "stable/distributed.optim.html#torch.distributed.optim.DistributedOptimizer", "desc": "DistributedOptimizer takes remote references to parameters scatteredacross workers and applies the given optimizer locally for each parameter."}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer", "type": "torchtorch.distributed.optim", "path": "stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer", "desc": "Wraps an arbitrary torch."}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer", "type": "torchtorch.distributed.optim", "path": "stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer", "desc": "This class wraps an arbitrary optim."}, {"name": "torch.distributions.distribution.Distribution", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.distribution.Distribution", "desc": "Bases: object Distribution is the abstract base class for probability distributions."}, {"name": "torch.distributions.exp_family.ExponentialFamily", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.exp_family.ExponentialFamily", "desc": "Bases: torch."}, {"name": "torch.distributions.bernoulli.Bernoulli", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.bernoulli.Bernoulli", "desc": "Bases: torch."}, {"name": "torch.distributions.beta.Beta", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.beta.Beta", "desc": "Bases: torch."}, {"name": "torch.distributions.binomial.Binomial", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.binomial.Binomial", "desc": "Bases: torch."}, {"name": "torch.distributions.categorical.Categorical", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.categorical.Categorical", "desc": "Bases: torch."}, {"name": "torch.distributions.cauchy.Cauchy", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.cauchy.Cauchy", "desc": "Bases: torch."}, {"name": "torch.distributions.chi2.Chi2", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.chi2.Chi2", "desc": "Bases: torch."}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "desc": "Bases: torch."}, {"name": "torch.distributions.dirichlet.Dirichlet", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.dirichlet.Dirichlet", "desc": "Bases: torch."}, {"name": "torch.distributions.exponential.Exponential", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.exponential.Exponential", "desc": "Bases: torch."}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.fishersnedecor.FisherSnedecor", "desc": "Bases: torch."}, {"name": "torch.distributions.gamma.Gamma", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.gamma.Gamma", "desc": "Bases: torch."}, {"name": "torch.distributions.geometric.Geometric", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.geometric.Geometric", "desc": "Bases: torch."}, {"name": "torch.distributions.gumbel.Gumbel", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.gumbel.Gumbel", "desc": "Bases: torch."}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.half_cauchy.HalfCauchy", "desc": "Bases: torch."}, {"name": "torch.distributions.half_normal.HalfNormal", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.half_normal.HalfNormal", "desc": "Bases: torch."}, {"name": "torch.distributions.independent.Independent", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.independent.Independent", "desc": "Bases: torch."}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.kumaraswamy.Kumaraswamy", "desc": "Bases: torch."}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.lkj_cholesky.LKJCholesky", "desc": "Bases: torch."}, {"name": "torch.distributions.laplace.Laplace", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.laplace.Laplace", "desc": "Bases: torch."}, {"name": "torch.distributions.log_normal.LogNormal", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.log_normal.LogNormal", "desc": "Bases: torch."}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "desc": "Bases: torch."}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily", "desc": "Bases: torch."}, {"name": "torch.distributions.multinomial.Multinomial", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.multinomial.Multinomial", "desc": "Bases: torch."}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal", "desc": "Bases: torch."}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.negative_binomial.NegativeBinomial", "desc": "Bases: torch."}, {"name": "torch.distributions.normal.Normal", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.normal.Normal", "desc": "Bases: torch."}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical", "desc": "Bases: torch."}, {"name": "torch.distributions.pareto.Pareto", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.pareto.Pareto", "desc": "Bases: torch."}, {"name": "torch.distributions.poisson.Poisson", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.poisson.Poisson", "desc": "Bases: torch."}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "desc": "Bases: torch."}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "desc": "Bases: torch."}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "desc": "Bases: torch."}, {"name": "torch.distributions.studentT.StudentT", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.studentT.StudentT", "desc": "Bases: torch."}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution", "desc": "Bases: torch."}, {"name": "torch.distributions.uniform.Uniform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.uniform.Uniform", "desc": "Bases: torch."}, {"name": "torch.distributions.von_mises.VonMises", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.von_mises.VonMises", "desc": "Bases: torch."}, {"name": "torch.distributions.weibull.Weibull", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.weibull.Weibull", "desc": "Bases: torch."}, {"name": "torch.distributions.wishart.Wishart", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.wishart.Wishart", "desc": "Bases: torch."}, {"name": "torch.distributions.kl.kl_divergence", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.kl.kl_divergence", "desc": "Compute Kullback-Leibler divergence K L ( p \u2225 q ) KL(p \\| q) K L ( p \u2225 q ) between two distributions."}, {"name": "torch.distributions.kl.register_kl", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.kl.register_kl", "desc": "Decorator to register a pairwise function with kl_divergence() Usage: @register_kl(Normal, Normal)def kl_normal_normal(p, q): # insert implementation here Lookup returns the most specific (type,type) match ordered by subclass."}, {"name": "torch.distributions.transforms.AbsTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.AbsTransform", "desc": "Transform via the mapping y = \u2223 x \u2223 y = |x| y = \u2223 x \u2223 ."}, {"name": "torch.distributions.transforms.AffineTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.AffineTransform", "desc": "Transform via the pointwise affine mapping y = loc + scale \u00d7 x y = \\text{loc} + \\text{scale} \\times x y = loc + scale \u00d7 x Parameters loc ( Tensor or float ) \u2013 Location parameter."}, {"name": "torch.distributions.transforms.ComposeTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.ComposeTransform", "desc": "Composes multiple transforms in a chain."}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.CorrCholeskyTransform", "desc": "Transforms an uncontrained real vector x x x with length D \u2217 ( D \u2212 1 ) / 2 D*(D-1)/2 D \u2217 ( D \u2212 1 ) /2 into theCholesky factor of a D-dimension correlation matrix."}, {"name": "torch.distributions.transforms.ExpTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.ExpTransform", "desc": "Transform via the mapping y = exp \u2061 ( x ) y = \\exp(x) y = exp ( x ) ."}, {"name": "torch.distributions.transforms.IndependentTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.IndependentTransform", "desc": "Wrapper around another transform to treat reinterpreted_batch_ndims -many extra of the right most dimensions asdependent."}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.LowerCholeskyTransform", "desc": "Transform from unconstrained matrices to lower-triangular matrices withnonnegative diagonal entries."}, {"name": "torch.distributions.transforms.PowerTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.PowerTransform", "desc": "Transform via the mapping y = x exponent y = x^{\\text{exponent}} y = x exponent ."}, {"name": "torch.distributions.transforms.ReshapeTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.ReshapeTransform", "desc": "Unit Jacobian transform to reshape the rightmost part of a tensor."}, {"name": "torch.distributions.transforms.SigmoidTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.SigmoidTransform", "desc": "Transform via the mapping y = 1 1 + exp \u2061 ( \u2212 x ) y = \\frac{1}{1 + \\exp(-x)} y = 1 + e x p ( \u2212 x ) 1 \u200b and x = logit ( y ) x = \\text{logit}(y) x = logit ( y ) ."}, {"name": "torch.distributions.transforms.TanhTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.TanhTransform", "desc": "Transform via the mapping y = tanh \u2061 ( x ) y = \\tanh(x) y = tanh ( x ) It is equivalent to ` ComposeTransform([AffineTransform(0."}, {"name": "torch.distributions.transforms.SoftmaxTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.SoftmaxTransform", "desc": "Transform from unconstrained space to the simplex via y = exp \u2061 ( x ) y = \\exp(x) y = exp ( x ) thennormalizing."}, {"name": "torch.distributions.transforms.StackTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.StackTransform", "desc": "Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch."}, {"name": "torch.distributions.transforms.StickBreakingTransform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.StickBreakingTransform", "desc": "Transform from unconstrained space to the simplex of one additionaldimension via a stick-breaking process."}, {"name": "torch.distributions.transforms.Transform", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.transforms.Transform", "desc": "Abstract class for invertable transformations with computable logdet jacobians."}, {"name": "torch.distributions.constraints.Constraint", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.constraints.Constraint", "desc": "Abstract base class for constraints."}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "type": "torchtorch.distributions", "path": "stable/distributions.html#torch.distributions.constraint_registry.ConstraintRegistry", "desc": "Registry to link constraints to transforms."}, {"name": "torch.fft.fft", "type": "torch.fft", "path": "stable/generated/torch.fft.fft.html#torch.fft.fft", "desc": "Computes the one dimensional discrete Fourier transform of input "}, {"name": "torch.fft.ifft", "type": "torch.fft", "path": "stable/generated/torch.fft.ifft.html#torch.fft.ifft", "desc": "Computes the one dimensional inverse discrete Fourier transform of input "}, {"name": "torch.fft.fft2", "type": "torch.fft", "path": "stable/generated/torch.fft.fft2.html#torch.fft.fft2", "desc": "Computes the 2 dimensional discrete Fourier transform of input "}, {"name": "torch.fft.ifft2", "type": "torch.fft", "path": "stable/generated/torch.fft.ifft2.html#torch.fft.ifft2", "desc": "Computes the 2 dimensional inverse discrete Fourier transform of input "}, {"name": "torch.fft.fftn", "type": "torch.fft", "path": "stable/generated/torch.fft.fftn.html#torch.fft.fftn", "desc": "Computes the N dimensional discrete Fourier transform of input "}, {"name": "torch.fft.ifftn", "type": "torch.fft", "path": "stable/generated/torch.fft.ifftn.html#torch.fft.ifftn", "desc": "Computes the N dimensional inverse discrete Fourier transform of input "}, {"name": "torch.fft.rfft", "type": "torch.fft", "path": "stable/generated/torch.fft.rfft.html#torch.fft.rfft", "desc": "Computes the one dimensional Fourier transform of real-valued input "}, {"name": "torch.fft.irfft", "type": "torch.fft", "path": "stable/generated/torch.fft.irfft.html#torch.fft.irfft", "desc": "Computes the inverse of rfft() "}, {"name": "torch.fft.rfft2", "type": "torch.fft", "path": "stable/generated/torch.fft.rfft2.html#torch.fft.rfft2", "desc": "Computes the 2-dimensional discrete Fourier transform of real input "}, {"name": "torch.fft.irfft2", "type": "torch.fft", "path": "stable/generated/torch.fft.irfft2.html#torch.fft.irfft2", "desc": "Computes the inverse of rfft2() "}, {"name": "torch.fft.rfftn", "type": "torch.fft", "path": "stable/generated/torch.fft.rfftn.html#torch.fft.rfftn", "desc": "Computes the N-dimensional discrete Fourier transform of real input "}, {"name": "torch.fft.irfftn", "type": "torch.fft", "path": "stable/generated/torch.fft.irfftn.html#torch.fft.irfftn", "desc": "Computes the inverse of rfftn() "}, {"name": "torch.fft.hfft", "type": "torch.fft", "path": "stable/generated/torch.fft.hfft.html#torch.fft.hfft", "desc": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal."}, {"name": "torch.fft.ihfft", "type": "torch.fft", "path": "stable/generated/torch.fft.ihfft.html#torch.fft.ihfft", "desc": "Computes the inverse of hfft() "}, {"name": "torch.fft.hfft2", "type": "torch.fft", "path": "stable/generated/torch.fft.hfft2.html#torch.fft.hfft2", "desc": "Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal."}, {"name": "torch.fft.ihfft2", "type": "torch.fft", "path": "stable/generated/torch.fft.ihfft2.html#torch.fft.ihfft2", "desc": "Computes the 2-dimensional inverse discrete Fourier transform of real input "}, {"name": "torch.fft.hfftn", "type": "torch.fft", "path": "stable/generated/torch.fft.hfftn.html#torch.fft.hfftn", "desc": "Computes the n-dimensional discrete Fourier transform of a Herimitian symmetric input signal."}, {"name": "torch.fft.ihfftn", "type": "torch.fft", "path": "stable/generated/torch.fft.ihfftn.html#torch.fft.ihfftn", "desc": "Computes the N-dimensional inverse discrete Fourier transform of real input "}, {"name": "torch.fft.fftfreq", "type": "torch.fft", "path": "stable/generated/torch.fft.fftfreq.html#torch.fft.fftfreq", "desc": "Computes the discrete Fourier Transform sample frequencies for a signal of size n "}, {"name": "torch.fft.rfftfreq", "type": "torch.fft", "path": "stable/generated/torch.fft.rfftfreq.html#torch.fft.rfftfreq", "desc": "Computes the sample frequencies for rfft() with a signal of size n "}, {"name": "torch.fft.fftshift", "type": "torch.fft", "path": "stable/generated/torch.fft.fftshift.html#torch.fft.fftshift", "desc": "Reorders n-dimensional FFT data, as provided by fftn() to have negative frequency terms first."}, {"name": "torch.fft.ifftshift", "type": "torch.fft", "path": "stable/generated/torch.fft.ifftshift.html#torch.fft.ifftshift", "desc": "Inverse of fftshift() "}, {"name": "torch.futures.Future", "type": "torchtorch.futures", "path": "stable/futures.html#torch.futures.Future", "desc": "Wrapper around a torch."}, {"name": "torch.futures.collect_all", "type": "torchtorch.futures", "path": "stable/futures.html#torch.futures.collect_all", "desc": "Collects the provided Future objects into a singlecombined Future that is completed when all of thesub-futures are completed."}, {"name": "torch.futures.wait_all", "type": "torchtorch.futures", "path": "stable/futures.html#torch.futures.wait_all", "desc": "Waits for all provided futures to be complete, and returnsthe list of completed values."}, {"name": "torch.fx.symbolic_trace", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.symbolic_trace", "desc": "Symbolic tracing API Given an nn."}, {"name": "torch.fx.wrap", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.wrap", "desc": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d."}, {"name": "torch.fx.GraphModule", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.GraphModule", "desc": "GraphModule is an nn."}, {"name": "torch.fx.Graph", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.Graph", "desc": "Graph is the main data structure used in the FX Intermediate Representation."}, {"name": "torch.fx.Node", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.Node", "desc": "Node is the data structure that represents individual operations withina Graph For the most part, Nodes represent callsites to various entities,such as operators, methods, and Modules (some exceptions include nodes thatspecify function inputs and outputs)."}, {"name": "torch.fx.Tracer", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.Tracer", "desc": "Tracer is the class that implements the symbolic tracing functionalityof torch."}, {"name": "torch.fx.Proxy", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.Proxy", "desc": "Proxy objects are Node wrappers that flow through theprogram during symbolic tracing and record all the operations( torch function calls, method calls, operators) that they touchinto the growing FX Graph."}, {"name": "torch.fx.Interpreter", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.Interpreter", "desc": "An Interpreter executes an FX graph Node-by-Node."}, {"name": "torch.fx.Transformer", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.Transformer", "desc": "Transformer is a special type of interpreter that produces anew Module It exposes a transform() method that returnsthe transformed Module Transformer does not requirearguments to run, as Interpreter does."}, {"name": "torch.fx.replace_pattern", "type": "torchtorch.fx", "path": "stable/fx.html#torch.fx.replace_pattern", "desc": "Matches all possible non-overlapping sets of operators and theirdata dependencies ( pattern ) in the Graph of a GraphModule( gm ), then replaces each of these matched subgraphs with anothersubgraph ( replacement )."}, {"name": "torch.hub.list", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.list", "desc": "List all callable entrypoints available in the repo specified by github Parameters github ( string ) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optionaltag/branch."}, {"name": "torch.hub.help", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.help", "desc": "Show the docstring of entrypoint model Parameters github ( string ) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optionaltag/branch."}, {"name": "torch.hub.load", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.load", "desc": "Load a model from a github repo or a local directory."}, {"name": "torch.hub.download_url_to_file", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.download_url_to_file", "desc": "Download object at the given URL to a local path."}, {"name": "torch.hub.load_state_dict_from_url", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.load_state_dict_from_url", "desc": "Loads the Torch serialized object at the given URL."}, {"name": "torch.hub.get_dir", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.get_dir", "desc": "Get the Torch Hub cache directory used for storing downloaded models & weights."}, {"name": "torch.hub.set_dir", "type": "torchtorch.hub", "path": "stable/hub.html#torch.hub.set_dir", "desc": "Optionally set the Torch Hub directory used to save downloaded models & weights."}, {"name": "torch.jit.script", "type": "torch.jit", "path": "stable/generated/torch.jit.script.html#torch.jit.script", "desc": "Scripting a function or nn.Module will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a ScriptModule or ScriptFunction "}, {"name": "torch.jit.trace", "type": "torch.jit", "path": "stable/generated/torch.jit.trace.html#torch.jit.trace", "desc": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation."}, {"name": "torch.jit.script_if_tracing", "type": "torch.jit", "path": "stable/generated/torch.jit.script_if_tracing.html#torch.jit.script_if_tracing", "desc": "Compiles fn when it is first called during tracing."}, {"name": "torch.jit.trace_module", "type": "torch.jit", "path": "stable/generated/torch.jit.trace_module.html#torch.jit.trace_module", "desc": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation."}, {"name": "torch.jit.fork", "type": "torch.jit", "path": "stable/generated/torch.jit.fork.html#torch.jit.fork", "desc": "Creates an asynchronous task executing func and a reference to the value of the result of this execution."}, {"name": "torch.jit.wait", "type": "torch.jit", "path": "stable/generated/torch.jit.wait.html#torch.jit.wait", "desc": "Forces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task."}, {"name": "torch.jit.ScriptModule", "type": "torch.jit", "path": "stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule", "desc": "A wrapper around C++ torch::jit::Module "}, {"name": "torch.jit.ScriptFunction", "type": "torch.jit", "path": "stable/generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction", "desc": "Functionally equivalent to a ScriptModule but represents a single function and does not have any attributes or Parameters."}, {"name": "torch.jit.freeze", "type": "torch.jit", "path": "stable/generated/torch.jit.freeze.html#torch.jit.freeze", "desc": "Freezing a ScriptModule will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph."}, {"name": "torch.jit.optimize_for_inference", "type": "torch.jit", "path": "stable/generated/torch.jit.optimize_for_inference.html#torch.jit.optimize_for_inference", "desc": "Performs a set of optimization passes to optimize a model for the purposes of inference."}, {"name": "torch.jit.set_fusion_strategy", "type": "torch.jit", "path": "stable/generated/torch.jit.set_fusion_strategy.html#torch.jit.set_fusion_strategy", "desc": "Sets the type and number of specializations that can occur during fusion."}, {"name": "torch.jit.save", "type": "torch.jit", "path": "stable/generated/torch.jit.save.html#torch.jit.save", "desc": "Save an offline version of this module for use in a separate process."}, {"name": "torch.jit.load", "type": "torch.jit", "path": "stable/generated/torch.jit.load.html#torch.jit.load", "desc": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save"}, {"name": "torch.jit.ignore", "type": "torch.jit", "path": "stable/generated/torch.jit.ignore.html#torch.jit.ignore", "desc": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function."}, {"name": "torch.jit.unused", "type": "torch.jit", "path": "stable/generated/torch.jit.unused.html#torch.jit.unused", "desc": "This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception."}, {"name": "torch.jit.isinstance", "type": "torch.jit", "path": "stable/generated/torch.jit.isinstance.html#torch.jit.isinstance", "desc": "This function provides for conatiner type refinement in TorchScript."}, {"name": "torch.jit.Attribute", "type": "torch.jit", "path": "stable/generated/torch.jit.Attribute.html#torch.jit.Attribute", "desc": "This method is a pass-through function that returns value mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type of type "}, {"name": "torch.jit.annotate", "type": "torch.jit", "path": "stable/generated/torch.jit.annotate.html#torch.jit.annotate", "desc": "This method is a pass-through function that returns the_value used to hint TorchScript compiler the type of the_value "}, {"name": "torch.jit.export", "type": "torchtorch.jit", "path": "stable/jit.html#torch.jit.export", "desc": "This decorator indicates that a method on an nn."}, {"name": "torch.linalg.norm", "type": "torch.linalg", "path": "stable/generated/torch.linalg.norm.html#torch.linalg.norm", "desc": "Computes a vector or matrix norm."}, {"name": "torch.linalg.vector_norm", "type": "torch.linalg", "path": "stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm", "desc": "Computes a vector norm."}, {"name": "torch.linalg.matrix_norm", "type": "torch.linalg", "path": "stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm", "desc": "Computes a matrix norm."}, {"name": "torch.linalg.diagonal", "type": "torch.linalg", "path": "stable/generated/torch.linalg.diagonal.html#torch.linalg.diagonal", "desc": "Alias for torch.diagonal() with defaults dim1 = -2 dim2 = -1 "}, {"name": "torch.linalg.det", "type": "torch.linalg", "path": "stable/generated/torch.linalg.det.html#torch.linalg.det", "desc": "Computes the determinant of a square matrix."}, {"name": "torch.linalg.slogdet", "type": "torch.linalg", "path": "stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet", "desc": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix."}, {"name": "torch.linalg.cond", "type": "torch.linalg", "path": "stable/generated/torch.linalg.cond.html#torch.linalg.cond", "desc": "Computes the condition number of a matrix with respect to a matrix norm."}, {"name": "torch.linalg.matrix_rank", "type": "torch.linalg", "path": "stable/generated/torch.linalg.matrix_rank.html#torch.linalg.matrix_rank", "desc": "Computes the numerical rank of a matrix."}, {"name": "torch.linalg.cholesky", "type": "torch.linalg", "path": "stable/generated/torch.linalg.cholesky.html#torch.linalg.cholesky", "desc": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix."}, {"name": "torch.linalg.qr", "type": "torch.linalg", "path": "stable/generated/torch.linalg.qr.html#torch.linalg.qr", "desc": "Computes the QR decomposition of a matrix."}, {"name": "torch.linalg.lu_factor", "type": "torch.linalg", "path": "stable/generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor", "desc": "Computes a compact representation of the LU factorization with partial pivoting of a matrix."}, {"name": "torch.linalg.eig", "type": "torch.linalg", "path": "stable/generated/torch.linalg.eig.html#torch.linalg.eig", "desc": "Computes the eigenvalue decomposition of a square matrix if it exists."}, {"name": "torch.linalg.eigvals", "type": "torch.linalg", "path": "stable/generated/torch.linalg.eigvals.html#torch.linalg.eigvals", "desc": "Computes the eigenvalues of a square matrix."}, {"name": "torch.linalg.eigh", "type": "torch.linalg", "path": "stable/generated/torch.linalg.eigh.html#torch.linalg.eigh", "desc": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix."}, {"name": "torch.linalg.eigvalsh", "type": "torch.linalg", "path": "stable/generated/torch.linalg.eigvalsh.html#torch.linalg.eigvalsh", "desc": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix."}, {"name": "torch.linalg.svd", "type": "torch.linalg", "path": "stable/generated/torch.linalg.svd.html#torch.linalg.svd", "desc": "Computes the singular value decomposition (SVD) of a matrix."}, {"name": "torch.linalg.svdvals", "type": "torch.linalg", "path": "stable/generated/torch.linalg.svdvals.html#torch.linalg.svdvals", "desc": "Computes the singular values of a matrix."}, {"name": "torch.linalg.solve", "type": "torch.linalg", "path": "stable/generated/torch.linalg.solve.html#torch.linalg.solve", "desc": "Computes the solution of a square system of linear equations with a unique solution."}, {"name": "torch.linalg.solve_triangular", "type": "torch.linalg", "path": "stable/generated/torch.linalg.solve_triangular.html#torch.linalg.solve_triangular", "desc": "Computes the solution of a triangular system of linear equations with a unique solution."}, {"name": "torch.linalg.lstsq", "type": "torch.linalg", "path": "stable/generated/torch.linalg.lstsq.html#torch.linalg.lstsq", "desc": "Computes a solution to the least squares problem of a system of linear equations."}, {"name": "torch.linalg.inv", "type": "torch.linalg", "path": "stable/generated/torch.linalg.inv.html#torch.linalg.inv", "desc": "Computes the inverse of a square matrix if it exists."}, {"name": "torch.linalg.pinv", "type": "torch.linalg", "path": "stable/generated/torch.linalg.pinv.html#torch.linalg.pinv", "desc": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix."}, {"name": "torch.linalg.matrix_exp", "type": "torch.linalg", "path": "stable/generated/torch.linalg.matrix_exp.html#torch.linalg.matrix_exp", "desc": "Computes the matrix exponential of a square matrix."}, {"name": "torch.linalg.matrix_power", "type": "torch.linalg", "path": "stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power", "desc": "Computes the n -th power of a square matrix for an integer n "}, {"name": "torch.linalg.cross", "type": "torch.linalg", "path": "stable/generated/torch.linalg.cross.html#torch.linalg.cross", "desc": "Computes the cross product of two 3-dimensional vectors."}, {"name": "torch.linalg.matmul", "type": "torch.linalg", "path": "stable/generated/torch.linalg.matmul.html#torch.linalg.matmul", "desc": "Alias for torch.matmul()"}, {"name": "torch.linalg.multi_dot", "type": "torch.linalg", "path": "stable/generated/torch.linalg.multi_dot.html#torch.linalg.multi_dot", "desc": "Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed."}, {"name": "torch.linalg.householder_product", "type": "torch.linalg", "path": "stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product", "desc": "Computes the first n columns of a product of Householder matrices."}, {"name": "torch.linalg.tensorinv", "type": "torch.linalg", "path": "stable/generated/torch.linalg.tensorinv.html#torch.linalg.tensorinv", "desc": "Computes the multiplicative inverse of torch.tensordot() "}, {"name": "torch.linalg.tensorsolve", "type": "torch.linalg", "path": "stable/generated/torch.linalg.tensorsolve.html#torch.linalg.tensorsolve", "desc": "Computes the solution X to the system torch.tensordot(A, X) = B "}, {"name": "torch.linalg.cholesky_ex", "type": "torch.linalg", "path": "stable/generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex", "desc": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix."}, {"name": "torch.linalg.inv_ex", "type": "torch.linalg", "path": "stable/generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex", "desc": "Computes the inverse of a square matrix if it is invertible."}, {"name": "torch.linalg.lu_factor_ex", "type": "torch.linalg", "path": "stable/generated/torch.linalg.lu_factor_ex.html#torch.linalg.lu_factor_ex", "desc": "This is a version of lu_factor() that does not perform error checks unless check_errors = True "}, {"name": "torch.monitor.Aggregation", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.Aggregation", "desc": "These are types of aggregations that can be used to accumulate stats."}, {"name": "torch.monitor.Stat", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.Stat", "desc": "Stat is used to compute summary statistics in a performant way overfixed intervals."}, {"name": "torch.monitor.data_value_t", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.data_value_t", "desc": "data_value_t is one of of str float int bool ."}, {"name": "torch.monitor.Event", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.Event", "desc": "Event represents a specific typed event to be logged."}, {"name": "torch.monitor.EventHandlerHandle", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.EventHandlerHandle", "desc": "EventHandlerHandle is a wrapper type returned by register_event_handler used to unregister the handler via unregister_event_handler This cannot be directly initialized."}, {"name": "torch.monitor.log_event", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.log_event", "desc": "log_event logs the specified event to all of the registered eventhandlers."}, {"name": "torch.monitor.register_event_handler", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.register_event_handler", "desc": "register_event_handler registers a callback to be called whenever anevent is logged via log_event These handlers should avoid blockingthe main thread since that may interfere with training as they runduring the log_event call."}, {"name": "torch.monitor.unregister_event_handler", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.unregister_event_handler", "desc": "unregister_event_handler unregisters the EventHandlerHandle returnedafter calling register_event_handler After this returns the eventhandler will no longer receive events."}, {"name": "torch.monitor.TensorboardEventHandler", "type": "torchtorch.monitor", "path": "stable/monitor.html#torch.monitor.TensorboardEventHandler", "desc": "TensorboardEventHandler is an event handler that will write known events tothe provided SummaryWriter."}, {"name": "torch.special.entr", "type": "torchtorch.special", "path": "stable/special.html#torch.special.entr", "desc": "Computes the entropy on input (as defined below), elementwise."}, {"name": "torch.special.erf", "type": "torchtorch.special", "path": "stable/special.html#torch.special.erf", "desc": "Computes the error function of input The error function is defined as follows: e r f ( x ) = 2 \u03c0 \u222b 0 x e \u2212 t 2 d t \\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt erf ( x ) = \u03c0 \u200b 2 \u200b \u222b 0 x \u200b e \u2212 t 2 d t Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.erfc", "type": "torchtorch.special", "path": "stable/special.html#torch.special.erfc", "desc": "Computes the complementary error function of input The complementary error function is defined as follows: e r f c ( x ) = 1 \u2212 2 \u03c0 \u222b 0 x e \u2212 t 2 d t \\mathrm{erfc}(x) = 1 - \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt erfc ( x ) = 1 \u2212 \u03c0 \u200b 2 \u200b \u222b 0 x \u200b e \u2212 t 2 d t Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.erfcx", "type": "torchtorch.special", "path": "stable/special.html#torch.special.erfcx", "desc": "Computes the scaled complementary error function for each element of input The scaled complementary error function is defined as follows: e r f c x ( x ) = e x 2 e r f c ( x ) \\mathrm{erfcx}(x) = e^{x^2} \\mathrm{erfc}(x) erfcx ( x ) = e x 2 erfc ( x ) Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.erfinv", "type": "torchtorch.special", "path": "stable/special.html#torch.special.erfinv", "desc": "Computes the inverse error function of input The inverse error function is defined in the range ( \u2212 1 1 ) (-1, 1) ( \u2212 1 1 ) as: e r f i n v ( e r f ( x ) ) = x \\mathrm{erfinv}(\\mathrm{erf}(x)) = x erfinv ( erf ( x )) = x Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.expit", "type": "torchtorch.special", "path": "stable/special.html#torch.special.expit", "desc": "Computes the expit (also known as the logistic sigmoid function) of the elements of input out i = 1 1 + e \u2212 input i \\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}} out i \u200b = 1 + e \u2212 input i \u200b 1 \u200b Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.expm1", "type": "torchtorch.special", "path": "stable/special.html#torch.special.expm1", "desc": "Computes the exponential of the elements minus 1of input y i = e x i \u2212 1 y_{i} = e^{x_{i}} - 1 y i \u200b = e x i \u200b \u2212 1 Note This function provides greater precision than exp(x) - 1 for small values of x."}, {"name": "torch.special.exp2", "type": "torchtorch.special", "path": "stable/special.html#torch.special.exp2", "desc": "Computes the base two exponential function of input y i = 2 x i y_{i} = 2^{x_{i}} y i \u200b = 2 x i \u200b Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.gammaln", "type": "torchtorch.special", "path": "stable/special.html#torch.special.gammaln", "desc": "Computes the natural logarithm of the absolute value of the gamma function on input out i = ln \u2061 \u0393 ( \u2223 input i \u2223 ) \\text{out}_{i} = \\ln \\Gamma(|\\text{input}_{i}|) out i \u200b = ln \u0393 ( \u2223 input i \u200b \u2223 ) Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.gammainc", "type": "torchtorch.special", "path": "stable/special.html#torch.special.gammainc", "desc": "Computes the regularized lower incomplete gamma function: out i = 1 \u0393 ( input i ) \u222b 0 other i t input i \u2212 1 e \u2212 t d t \\text{out}_{i} = \\frac{1}{\\Gamma(\\text{input}_i)} \\int_0^{\\text{other}_i} t^{\\text{input}_i-1} e^{-t} dt out i \u200b = \u0393 ( input i \u200b ) 1 \u200b \u222b 0 other i \u200b \u200b t input i \u200b \u2212 1 e \u2212 t d t where both input i \\text{input}_i input i \u200b and other i \\text{other}_i other i \u200b are weakly positiveand at least one is strictly positive."}, {"name": "torch.special.gammaincc", "type": "torchtorch.special", "path": "stable/special.html#torch.special.gammaincc", "desc": "Computes the regularized upper incomplete gamma function: out i = 1 \u0393 ( input i ) \u222b other i \u221e t input i \u2212 1 e \u2212 t d t \\text{out}_{i} = \\frac{1}{\\Gamma(\\text{input}_i)} \\int_{\\text{other}_i}^{\\infty} t^{\\text{input}_i-1} e^{-t} dt out i \u200b = \u0393 ( input i \u200b ) 1 \u200b \u222b other i \u200b \u221e \u200b t input i \u200b \u2212 1 e \u2212 t d t where both input i \\text{input}_i input i \u200b and other i \\text{other}_i other i \u200b are weakly positiveand at least one is strictly positive."}, {"name": "torch.special.polygamma", "type": "torchtorch.special", "path": "stable/special.html#torch.special.polygamma", "desc": "Computes the n t h n^{th} n t h derivative of the digamma function on input n \u2265 0 n \\geq 0 n \u2265 0 is called the order of the polygamma function."}, {"name": "torch.special.digamma", "type": "torchtorch.special", "path": "stable/special.html#torch.special.digamma", "desc": "Computes the logarithmic derivative of the gamma function on input \u03dd ( x ) = d d x ln \u2061 ( \u0393 ( x ) ) = \u0393 \u2032 ( x ) \u0393 ( x ) \\digamma(x) = \\frac{d}{dx} \\ln\\left(\\Gamma\\left(x\\right)\\right) = \\frac{\\Gamma'(x)}{\\Gamma(x)} \u03dd ( x ) = d x d \u200b ln ( \u0393 ( x ) ) = \u0393 ( x ) \u0393 \u2032 ( x ) \u200b Parameters input ( Tensor ) \u2013 the tensor to compute the digamma function on Keyword Arguments out ( Tensor optional ) \u2013 the output tensor."}, {"name": "torch.special.psi", "type": "torchtorch.special", "path": "stable/special.html#torch.special.psi", "desc": "Alias for torch."}, {"name": "torch.special.i0", "type": "torchtorch.special", "path": "stable/special.html#torch.special.i0", "desc": "Computes the zeroth order modified Bessel function of the first kind for each element of input out i = I 0 ( input i ) = \u2211 k = 0 \u221e ( input i 2 / 4 ) k ( k ! ) 2 \\text{out}_{i} = I_0(\\text{input}_{i}) = \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2} out i \u200b = I 0 \u200b ( input i \u200b ) = k = 0 \u2211 \u221e \u200b ( k ! ) 2 ( input i 2 \u200b /4 ) k \u200b Parameters input ( Tensor ) \u2013 the input tensor Keyword Arguments out ( Tensor optional ) \u2013 the output tensor."}, {"name": "torch.special.i0e", "type": "torchtorch.special", "path": "stable/special.html#torch.special.i0e", "desc": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)for each element of input out i = exp \u2061 ( \u2212 \u2223 x \u2223 ) \u2217 i 0 ( x ) = exp \u2061 ( \u2212 \u2223 x \u2223 ) \u2217 \u2211 k = 0 \u221e ( input i 2 / 4 ) k ( k ! ) 2 \\text{out}_{i} = \\exp(-|x|) * i0(x) = \\exp(-|x|) * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2} out i \u200b = exp ( \u2212 \u2223 x \u2223 ) \u2217 i 0 ( x ) = exp ( \u2212 \u2223 x \u2223 ) \u2217 k = 0 \u2211 \u221e \u200b ( k ! ) 2 ( input i 2 \u200b /4 ) k \u200b Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.i1", "type": "torchtorch.special", "path": "stable/special.html#torch.special.i1", "desc": "Computes the first order modified Bessel function of the first kind (as defined below)for each element of input out i = ( input i ) 2 \u2217 \u2211 k = 0 \u221e ( input i 2 / 4 ) k ( k ! ) \u2217 ( k + 1 ) ! \\text{out}_{i} = \\frac{(\\text{input}_{i})}{2} * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!) * (k+1)!} out i \u200b = 2 ( input i \u200b ) \u200b \u2217 k = 0 \u2211 \u221e \u200b ( k !) \u2217 ( k + 1 )! ( input i 2 \u200b /4 ) k \u200b Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.i1e", "type": "torchtorch.special", "path": "stable/special.html#torch.special.i1e", "desc": "Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below)for each element of input out i = exp \u2061 ( \u2212 \u2223 x \u2223 ) \u2217 i 1 ( x ) = exp \u2061 ( \u2212 \u2223 x \u2223 ) \u2217 ( input i ) 2 \u2217 \u2211 k = 0 \u221e ( input i 2 / 4 ) k ( k ! ) \u2217 ( k + 1 ) ! \\text{out}_{i} = \\exp(-|x|) * i1(x) = \\exp(-|x|) * \\frac{(\\text{input}_{i})}{2} * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!) * (k+1)!} out i \u200b = exp ( \u2212 \u2223 x \u2223 ) \u2217 i 1 ( x ) = exp ( \u2212 \u2223 x \u2223 ) \u2217 2 ( input i \u200b ) \u200b \u2217 k = 0 \u2211 \u221e \u200b ( k !) \u2217 ( k + 1 )! ( input i 2 \u200b /4 ) k \u200b Parameters input ( Tensor ) \u2013 the input tensor."}, {"name": "torch.special.logit", "type": "torchtorch.special", "path": "stable/special.html#torch.special.logit", "desc": "Returns a new tensor with the logit of the elements of input input is clamped to [eps, 1 - eps] when eps is not None."}, {"name": "torch.special.logsumexp", "type": "torchtorch.special", "path": "stable/special.html#torch.special.logsumexp", "desc": "Alias for torch."}, {"name": "torch.special.log1p", "type": "torchtorch.special", "path": "stable/special.html#torch.special.log1p", "desc": "Alias for torch."}, {"name": "torch.special.log_softmax", "type": "torchtorch.special", "path": "stable/special.html#torch.special.log_softmax", "desc": "Computes softmax followed by a logarithm."}, {"name": "torch.special.multigammaln", "type": "torchtorch.special", "path": "stable/special.html#torch.special.multigammaln", "desc": "Computes the multivariate log-gamma function with dimension p p p element-wise, given by log \u2061 ( \u0393 p ( a ) ) = C + \u2211 i = 1 p log \u2061 ( \u0393 ( a \u2212 i \u2212 1 2 ) ) \\log(\\Gamma_{p}(a)) = C + \\displaystyle \\sum_{i=1}^{p} \\log\\left(\\Gamma\\left(a - \\frac{i - 1}{2}\\right)\\right) lo g ( \u0393 p \u200b ( a )) = C + i = 1 \u2211 p \u200b lo g ( \u0393 ( a \u2212 2 i \u2212 1 \u200b ) ) where C = log \u2061 ( \u03c0 ) \u00d7 p ( p \u2212 1 ) 4 C = \\log(\\pi) \\times \\frac{p (p - 1)}{4} C = lo g ( \u03c0 ) \u00d7 4 p ( p \u2212 1 ) \u200b and \u0393 ( \u22c5 ) \\Gamma(\\cdot) \u0393 ( \u22c5 ) is the Gamma function."}, {"name": "torch.special.ndtr", "type": "torchtorch.special", "path": "stable/special.html#torch.special.ndtr", "desc": "Computes the area under the standard Gaussian probability density function,integrated from minus infinity to input elementwise."}, {"name": "torch.special.ndtri", "type": "torchtorch.special", "path": "stable/special.html#torch.special.ndtri", "desc": "Computes the argument, x, for which the area under the Gaussian probability density function(integrated from minus infinity to x) is equal to input elementwise."}, {"name": "torch.special.round", "type": "torchtorch.special", "path": "stable/special.html#torch.special.round", "desc": "Alias for torch."}, {"name": "torch.special.sinc", "type": "torchtorch.special", "path": "stable/special.html#torch.special.sinc", "desc": "Computes the normalized sinc of input."}, {"name": "torch.special.softmax", "type": "torchtorch.special", "path": "stable/special.html#torch.special.softmax", "desc": "Computes the softmax function."}, {"name": "torch.special.xlog1py", "type": "torchtorch.special", "path": "stable/special.html#torch.special.xlog1py", "desc": "Computes input * log1p(other) with the following cases."}, {"name": "torch.special.xlogy", "type": "torchtorch.special", "path": "stable/special.html#torch.special.xlogy", "desc": "Computes input * log(other) with the following cases."}, {"name": "torch.special.zeta", "type": "torchtorch.special", "path": "stable/special.html#torch.special.zeta", "desc": "Computes the Hurwitz zeta function, elementwise."}, {"name": "torch.overrides.get_ignored_functions", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.get_ignored_functions", "desc": "Return public functions that cannot be overridden by __torch_function__ Returns A tuple of functions that are publicly available in the torch API but cannotbe overridden with __torch_function__ Mostly this is because none of thearguments of these functions are tensors or tensor-likes."}, {"name": "torch.overrides.get_overridable_functions", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.get_overridable_functions", "desc": "List functions that are overridable via __torch_function__ Returns A dictionary that maps namespaces that contain overridable functionsto functions in that namespace that can be overridden."}, {"name": "torch.overrides.get_testing_overrides", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.get_testing_overrides", "desc": "Return a dict containing dummy overrides for all overridable functions Returns A dictionary that maps overridable functions in the PyTorch API tolambda functions that have the same signature as the real functionand unconditionally return -1."}, {"name": "torch.overrides.handle_torch_function", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.handle_torch_function", "desc": "Implement a function with checks for __torch_function__ overrides."}, {"name": "torch.overrides.has_torch_function", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.has_torch_function", "desc": "Check for __torch_function__ implementations in the elements of an iterable."}, {"name": "torch.overrides.is_tensor_like", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.is_tensor_like", "desc": "Returns True if the passed-in input is a Tensor-like."}, {"name": "torch.overrides.is_tensor_method_or_property", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.is_tensor_method_or_property", "desc": "Returns True if the function passed in is a handler for amethod or property belonging to torch."}, {"name": "torch.overrides.wrap_torch_function", "type": "torchtorch.torch.overrides", "path": "stable/torch.overrides.html#torch.overrides.wrap_torch_function", "desc": "Wraps a given function with __torch_function__ -related functionality."}, {"name": "torch.package.PackagingError", "type": "torchtorch.package", "path": "stable/package.html#torch.package.PackagingError", "desc": "This exception is raised when there is an issue with exporting a package."}, {"name": "torch.package.EmptyMatchError", "type": "torchtorch.package", "path": "stable/package.html#torch.package.EmptyMatchError", "desc": "This is an exception that is thrown when a mock or extern is marked as allow_empty=False and is not matched with any module during packaging."}, {"name": "torch.package.PackageExporter", "type": "torchtorch.package", "path": "stable/package.html#torch.package.PackageExporter", "desc": "Exporters allow you to write packages of code, pickled Python data, andarbitrary binary and text resources into a self-contained package."}, {"name": "torch.package.PackageImporter", "type": "torchtorch.package", "path": "stable/package.html#torch.package.PackageImporter", "desc": "Importers allow you to load code written to packages by PackageExporter Code is loaded in a hermetic way, using files from the packagerather than the normal python import system."}, {"name": "torch.package.Directory", "type": "torchtorch.package", "path": "stable/package.html#torch.package.Directory", "desc": "A file structure representation."}, {"name": "torch.profiler._KinetoProfile", "type": "torchtorch.profiler", "path": "stable/profiler.html#torch.profiler._KinetoProfile", "desc": "Low-level profiler wrap the autograd profile Parameters activities ( iterable ) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values: torch."}, {"name": "torch.profiler.profile", "type": "torchtorch.profiler", "path": "stable/profiler.html#torch.profiler.profile", "desc": "Profiler context manager."}, {"name": "torch.profiler.ProfilerAction", "type": "torchtorch.profiler", "path": "stable/profiler.html#torch.profiler.ProfilerAction", "desc": "Profiler actions that can be taken at the specified intervals."}, {"name": "torch.profiler.ProfilerActivity", "type": "torchtorch.profiler", "path": "stable/profiler.html#torch.profiler.ProfilerActivity", "desc": "Members: CPU CUDA property name \u00b6."}, {"name": "torch.profiler.schedule", "type": "torchtorch.profiler", "path": "stable/profiler.html#torch.profiler.schedule", "desc": "Returns a callable that can be used as profiler schedule argument."}, {"name": "torch.profiler.tensorboard_trace_handler", "type": "torchtorch.profiler", "path": "stable/profiler.html#torch.profiler.tensorboard_trace_handler", "desc": "Outputs tracing files to directory of dir_name then that directory can bedirectly delivered to tensorboard as logdir."}, {"name": "torch.nn.init.calculate_gain", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.calculate_gain", "desc": "Return the recommended gain value for the given nonlinearity function."}, {"name": "torch.nn.init.uniform_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.uniform_", "desc": "Fills the input Tensor with values drawn from the uniformdistribution U ( a b ) \\mathcal{U}(a, b) U ( a b ) Parameters tensor \u2013 an n-dimensional torch."}, {"name": "torch.nn.init.normal_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.normal_", "desc": "Fills the input Tensor with values drawn from the normaldistribution N ( mean std 2 ) \\mathcal{N}(\\text{mean}, \\text{std}^2) N ( mean std 2 ) Parameters tensor \u2013 an n-dimensional torch."}, {"name": "torch.nn.init.constant_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.constant_", "desc": "Fills the input Tensor with the value val \\text{val} val Parameters tensor \u2013 an n-dimensional torch."}, {"name": "torch.nn.init.ones_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.ones_", "desc": "Fills the input Tensor with the scalar value 1 Parameters tensor \u2013 an n-dimensional torch."}, {"name": "torch.nn.init.zeros_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.zeros_", "desc": "Fills the input Tensor with the scalar value 0 Parameters tensor \u2013 an n-dimensional torch."}, {"name": "torch.nn.init.eye_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.eye_", "desc": "Fills the 2-dimensional input Tensor with the identitymatrix."}, {"name": "torch.nn.init.dirac_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.dirac_", "desc": "Fills the {3, 4, 5}-dimensional input Tensor with the Diracdelta function."}, {"name": "torch.nn.init.xavier_uniform_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.xavier_uniform_", "desc": "Fills the input Tensor with values according to the methoddescribed in Understanding the difficulty of training deep feedforwardneural networks - Glorot, X."}, {"name": "torch.nn.init.xavier_normal_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.xavier_normal_", "desc": "Fills the input Tensor with values according to the methoddescribed in Understanding the difficulty of training deep feedforwardneural networks - Glorot, X."}, {"name": "torch.nn.init.kaiming_uniform_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.kaiming_uniform_", "desc": "Fills the input Tensor with values according to the methoddescribed in Delving deep into rectifiers: Surpassing human-levelperformance on ImageNet classification - He, K."}, {"name": "torch.nn.init.kaiming_normal_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.kaiming_normal_", "desc": "Fills the input Tensor with values according to the methoddescribed in Delving deep into rectifiers: Surpassing human-levelperformance on ImageNet classification - He, K."}, {"name": "torch.nn.init.orthogonal_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.orthogonal_", "desc": "Fills the input Tensor with a (semi) orthogonal matrix, asdescribed in Exact solutions to the nonlinear dynamics of learning in deeplinear neural networks - Saxe, A."}, {"name": "torch.nn.init.sparse_", "type": "torchtorch.nn.init", "path": "stable/nn.init.html#torch.nn.init.sparse_", "desc": "Fills the 2D input Tensor as a sparse matrix, where thenon-zero elements will be drawn from the normal distribution N ( 0 0."}, {"name": "torch.onnx.export", "type": "torchtorch.onnx", "path": "stable/onnx.html#torch.onnx.export", "desc": "Exports a model into ONNX format."}, {"name": "torch.onnx.export_to_pretty_string", "type": "torchtorch.onnx", "path": "stable/onnx.html#torch.onnx.export_to_pretty_string", "desc": "Similar to export() but returns a text representation of the ONNXmodel."}, {"name": "torch.onnx.register_custom_op_symbolic", "type": "torchtorch.onnx", "path": "stable/onnx.html#torch.onnx.register_custom_op_symbolic", "desc": "Registers symbolic_fn to handle symbolic_name See\u201cCustom Operators\u201d in the module documentation for an example usage."}, {"name": "torch.onnx.select_model_mode_for_export", "type": "torchtorch.onnx", "path": "stable/onnx.html#torch.onnx.select_model_mode_for_export", "desc": "A context manager to temporarily set the training mode of model to mode resetting it when we exit the with-block."}, {"name": "torch.onnx.is_in_onnx_export", "type": "torchtorch.onnx", "path": "stable/onnx.html#torch.onnx.is_in_onnx_export", "desc": "Returns True iff export() is running in the current thread."}, {"name": "torch.optim.Optimizer.add_param_group", "type": "torch.optim", "path": "stable/generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group", "desc": "Add a param group to the Optimizer s param_groups "}, {"name": "torch.optim.Optimizer.load_state_dict", "type": "torch.optim", "path": "stable/generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict", "desc": "Loads the optimizer state."}, {"name": "torch.optim.Optimizer.state_dict", "type": "torch.optim", "path": "stable/generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict", "desc": "Returns the state of the optimizer as a dict "}, {"name": "torch.optim.Optimizer.step", "type": "torch.optim", "path": "stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step", "desc": "Performs a single optimization step (parameter update)."}, {"name": "torch.optim.Optimizer.zero_grad", "type": "torch.optim", "path": "stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad", "desc": "Sets the gradients of all optimized torch.Tensor s to zero."}, {"name": "torch.optim.Adadelta", "type": "torch.optim", "path": "stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta", "desc": "Implements Adadelta algorithm."}, {"name": "torch.optim.Adagrad", "type": "torch.optim", "path": "stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad", "desc": "Implements Adagrad algorithm."}, {"name": "torch.optim.Adam", "type": "torch.optim", "path": "stable/generated/torch.optim.Adam.html#torch.optim.Adam", "desc": "Implements Adam algorithm."}, {"name": "torch.optim.AdamW", "type": "torch.optim", "path": "stable/generated/torch.optim.AdamW.html#torch.optim.AdamW", "desc": "Implements AdamW algorithm."}, {"name": "torch.optim.SparseAdam", "type": "torch.optim", "path": "stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam", "desc": "Implements lazy version of Adam algorithm suitable for sparse tensors."}, {"name": "torch.optim.Adamax", "type": "torch.optim", "path": "stable/generated/torch.optim.Adamax.html#torch.optim.Adamax", "desc": "Implements Adamax algorithm (a variant of Adam based on infinity norm)."}, {"name": "torch.optim.ASGD", "type": "torch.optim", "path": "stable/generated/torch.optim.ASGD.html#torch.optim.ASGD", "desc": "Implements Averaged Stochastic Gradient Descent."}, {"name": "torch.optim.LBFGS", "type": "torch.optim", "path": "stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS", "desc": "Implements L-BFGS algorithm, heavily inspired by minFunc "}, {"name": "torch.optim.NAdam", "type": "torch.optim", "path": "stable/generated/torch.optim.NAdam.html#torch.optim.NAdam", "desc": "Implements NAdam algorithm."}, {"name": "torch.optim.RAdam", "type": "torch.optim", "path": "stable/generated/torch.optim.RAdam.html#torch.optim.RAdam", "desc": "Implements RAdam algorithm."}, {"name": "torch.optim.RMSprop", "type": "torch.optim", "path": "stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop", "desc": "Implements RMSprop algorithm."}, {"name": "torch.optim.Rprop", "type": "torch.optim", "path": "stable/generated/torch.optim.Rprop.html#torch.optim.Rprop", "desc": "Implements the resilient backpropagation algorithm."}, {"name": "torch.optim.SGD", "type": "torch.optim", "path": "stable/generated/torch.optim.SGD.html#torch.optim.SGD", "desc": "Implements stochastic gradient descent (optionally with momentum)."}, {"name": "torch.optim.lr_scheduler.LambdaLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR", "desc": "Sets the learning rate of each parameter group to the initial lr times a given function."}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR", "desc": "Multiply the learning rate of each parameter group by the factor given in the specified function."}, {"name": "torch.optim.lr_scheduler.StepLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR", "desc": "Decays the learning rate of each parameter group by gamma every step_size epochs."}, {"name": "torch.optim.lr_scheduler.MultiStepLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR", "desc": "Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones."}, {"name": "torch.optim.lr_scheduler.ConstantLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR", "desc": "Decays the learning rate of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters."}, {"name": "torch.optim.lr_scheduler.LinearLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR", "desc": "Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters."}, {"name": "torch.optim.lr_scheduler.ExponentialLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR", "desc": "Decays the learning rate of each parameter group by gamma every epoch."}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR", "desc": "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7 m a x \\eta_{max} \u03b7 ma x \u200b is set to the initial lr and T c u r T_{cur} T c u r \u200b is the number of epochs since the last restart in SGDR:"}, {"name": "torch.optim.lr_scheduler.ChainedScheduler", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler", "desc": "Chains list of learning rate schedulers."}, {"name": "torch.optim.lr_scheduler.SequentialLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR", "desc": "Receives the list of schedulers that is expected to be called sequentially during optimization process and milestone points that provides exact intervals to reflect which scheduler is supposed to be called at a given epoch."}, {"name": "torch.optim.lr_scheduler.ReduceLROnPlateau", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau", "desc": "Reduce learning rate when a metric has stopped improving."}, {"name": "torch.optim.lr_scheduler.CyclicLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR", "desc": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR)."}, {"name": "torch.optim.lr_scheduler.OneCycleLR", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR", "desc": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy."}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "type": "torch.optim", "path": "stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "desc": "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7 m a x \\eta_{max} \u03b7 ma x \u200b is set to the initial lr, T c u r T_{cur} T c u r \u200b is the number of epochs since the last restart and T i T_{i} T i \u200b is the number of epochs between two warm restarts in SGDR:"}, {"name": "torch.optim.Optimizer", "type": "torchtorch.optim", "path": "stable/optim.html#torch.optim.Optimizer", "desc": "Base class for all optimizers."}, {"name": "torch.distributed.GradBucket", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket", "desc": "This class mainly passes a flattened gradient tensor(returned by buffer() )to DDP communication hook."}, {"name": "torch.distributed.GradBucket.index", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket.index", "desc": "Warning Since the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training."}, {"name": "torch.distributed.GradBucket.buffer", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket.buffer", "desc": "Returns A flattened 1D torch."}, {"name": "torch.distributed.GradBucket.gradients", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket.gradients", "desc": "Returns A list of torch."}, {"name": "torch.distributed.GradBucket.is_last", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket.is_last", "desc": "Returns Whether this bucket is the last bucket to allreduce in an iteration."}, {"name": "torch.distributed.GradBucket.set_buffer", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket.set_buffer", "desc": "Replaces the tensor in the bucket with the input tensor buffer."}, {"name": "torch.distributed.GradBucket.parameters", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.GradBucket.parameters", "desc": "Returns A list of torch."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "desc": "This DDP communication hook just calls allreduce using GradBucket tensors."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "desc": "This DDP communication hook implements a simple gradient compressionapproach that casts GradBucket tensor to half-precision floating-point format ( torch."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook", "desc": "Warning: This API is experimental, and it requires NCCL version later than 2."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper", "desc": "This wrapper casts the input gradient tensor of a given DDP communication hook to half-precisionfloating point format ( torch."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper", "desc": "Warning: This API is experimental, and it requires NCCL version later than 2."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "desc": "Stores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "desc": "This DDP communication hook implements PowerSGD gradient compressionalgorithm described in the paper Once gradient tensors are aggregated across all workers, this hook appliescompression as follows: Views the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups: 1."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "desc": "This DDP communication hook implements a simplified PowerSGD gradient compressionalgorithm described in the paper This variant does not compress the gradients layer by layer,but instead compresses the flattened input tensor that batches all the gradients."}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook", "type": "torchtorch.ddp_comm_hooks", "path": "stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook", "desc": "This DDP communication hook returns a future that wraps the input,so it is a noop that does not incur any communication overheads."}, {"name": "torch.distributed.pipeline.sync.Pipe", "type": "torchtorch.pipeline", "path": "stable/pipeline.html#torch.distributed.pipeline.sync.Pipe", "desc": "Wraps an arbitrary nn."}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "torchtorch.pipeline", "path": "stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.skippable", "desc": "The decorator to define a nn."}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "type": "torchtorch.pipeline", "path": "stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.stash", "desc": "The command to stash a skip tensor."}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "type": "torchtorch.pipeline", "path": "stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.pop", "desc": "The command to pop a skip tensor."}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "torchtorch.pipeline", "path": "stable/pipeline.html#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "desc": "Verifies if the underlying skippable modules satisfy integrity."}, {"name": "torch.distributed.rpc.init_rpc", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.init_rpc", "desc": "Initializes RPC primitives such as the local RPC agentand distributed autograd, which immediately makes the currentprocess ready to send and receive RPCs."}, {"name": "torch.distributed.rpc.rpc_sync", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.rpc_sync", "desc": "Make a blocking RPC call to run function func on worker to RPCmessages are sent and received in parallel to execution of Python code."}, {"name": "torch.distributed.rpc.rpc_async", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.rpc_async", "desc": "Make a non-blocking RPC call to run function func on worker to RPCmessages are sent and received in parallel to execution of Python code."}, {"name": "torch.distributed.rpc.remote", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.remote", "desc": "Make a remote call to run func on worker to and return an RRef to the result value immediately."}, {"name": "torch.distributed.rpc.get_worker_info", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.get_worker_info", "desc": "Get WorkerInfo of a given worker name."}, {"name": "torch.distributed.rpc.shutdown", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.shutdown", "desc": "Perform a shutdown of the RPC agent, and then destroy the RPC agent."}, {"name": "torch.distributed.rpc.WorkerInfo", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.WorkerInfo", "desc": "A structure that encapsulates information of a worker in the system."}, {"name": "torch.distributed.rpc.functions.async_execution", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.functions.async_execution", "desc": "A decorator for a function indicating that the return value of the functionis guaranteed to be a Future object and thisfunction can run asynchronously on the RPC callee."}, {"name": "torch.distributed.rpc.BackendType", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.BackendType", "desc": "An enum class of available backends."}, {"name": "torch.distributed.rpc.RpcBackendOptions", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.RpcBackendOptions", "desc": "An abstract structure encapsulating the options passed into the RPCbackend."}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions", "desc": "The backend options for TensorPipeAgent derived from RpcBackendOptions Parameters num_worker_threads ( int optional ) \u2013 The number of threads in thethread-pool used by TensorPipeAgent to executerequests (default: 16)."}, {"name": "torch.distributed.rpc.RRef", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.rpc.RRef", "desc": "backward ( self : torch."}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule", "desc": "A RemoteModule instance can only be created after RPC initialization."}, {"name": "torch.distributed.autograd.backward", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.autograd.backward", "desc": "Kicks off the distributed backward pass using the provided roots."}, {"name": "torch.distributed.autograd.context", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.autograd.context", "desc": "Context object to wrap forward and backward passes when usingdistributed autograd."}, {"name": "torch.distributed.autograd.get_gradients", "type": "torchtorch.rpc", "path": "stable/rpc.html#torch.distributed.autograd.get_gradients", "desc": "Retrieves a map from Tensor to the appropriate gradient for that Tensoraccumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass."}, {"name": "torch.random.fork_rng", "type": "torchtorch.random", "path": "stable/random.html#torch.random.fork_rng", "desc": "Forks the RNG, so that when you return, the RNG is resetto the state that it was previously in."}, {"name": "torch.random.get_rng_state", "type": "torchtorch.random", "path": "stable/random.html#torch.random.get_rng_state", "desc": "Returns the random number generator state as a torch."}, {"name": "torch.random.initial_seed", "type": "torchtorch.random", "path": "stable/random.html#torch.random.initial_seed", "desc": "Returns the initial seed for generating random numbers as aPython long ."}, {"name": "torch.random.manual_seed", "type": "torchtorch.random", "path": "stable/random.html#torch.random.manual_seed", "desc": "Sets the seed for generating random numbers."}, {"name": "torch.random.seed", "type": "torchtorch.random", "path": "stable/random.html#torch.random.seed", "desc": "Sets the seed for generating random numbers to a non-deterministicrandom number."}, {"name": "torch.random.set_rng_state", "type": "torchtorch.random", "path": "stable/random.html#torch.random.set_rng_state", "desc": "Sets the random number generator state."}, {"name": "torch.Tensor.is_sparse", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse", "desc": "Is True if the Tensor uses sparse storage layout, False otherwise."}, {"name": "torch.Tensor.dense_dim", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim", "desc": "Return the number of dense dimensions in a sparse tensor self "}, {"name": "torch.Tensor.sparse_dim", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim", "desc": "Return the number of sparse dimensions in a sparse tensor self "}, {"name": "torch.Tensor.sparse_mask", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask", "desc": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask "}, {"name": "torch.Tensor.to_sparse", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse", "desc": "Returns a sparse copy of the tensor."}, {"name": "torch.Tensor.indices", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.indices.html#torch.Tensor.indices", "desc": "Return the indices tensor of a sparse COO tensor "}, {"name": "torch.Tensor.values", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.values.html#torch.Tensor.values", "desc": "Return the values tensor of a sparse COO tensor "}, {"name": "torch.Tensor.coalesce", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce", "desc": "Returns a coalesced copy of self if self is an uncoalesced tensor "}, {"name": "torch.Tensor.sparse_resize_", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.sparse_resize_.html#torch.Tensor.sparse_resize_", "desc": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions."}, {"name": "torch.Tensor.sparse_resize_and_clear_", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.sparse_resize_and_clear_.html#torch.Tensor.sparse_resize_and_clear_", "desc": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions."}, {"name": "torch.Tensor.is_coalesced", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced", "desc": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise."}, {"name": "torch.Tensor.to_dense", "type": "torch.sparse", "path": "stable/generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense", "desc": "Creates a strided copy of self "}, {"name": "torch.sparse_coo_tensor", "type": "torch.sparse", "path": "stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor", "desc": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices "}, {"name": "torch.sparse_csr_tensor", "type": "torch.sparse", "path": "stable/generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor", "desc": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices "}, {"name": "torch.sparse.sum", "type": "torch.sparse", "path": "stable/generated/torch.sparse.sum.html#torch.sparse.sum", "desc": "Returns the sum of each row of the sparse tensor input in the given dimensions dim "}, {"name": "torch.sparse.addmm", "type": "torch.sparse", "path": "stable/generated/torch.sparse.addmm.html#torch.sparse.addmm", "desc": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1 "}, {"name": "torch.sparse.sampled_addmm", "type": "torch.sparse", "path": "stable/generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm", "desc": "Performs a matrix multiplication of the dense matrices mat1 and mat2 at the locations specified by the sparsity pattern of input "}, {"name": "torch.sparse.mm", "type": "torch.sparse", "path": "stable/generated/torch.sparse.mm.html#torch.sparse.mm", "desc": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2 "}, {"name": "torch.sspaddmm", "type": "torch.sparse", "path": "stable/generated/torch.sspaddmm.html#torch.sspaddmm", "desc": "Matrix multiplies a sparse tensor mat1 with a dense tensor mat2 then adds the sparse tensor input to the result."}, {"name": "torch.hspmm", "type": "torch.sparse", "path": "stable/generated/torch.hspmm.html#torch.hspmm", "desc": "Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2 "}, {"name": "torch.smm", "type": "torch.sparse", "path": "stable/generated/torch.smm.html#torch.smm", "desc": "Performs a matrix multiplication of the sparse matrix input with the dense matrix mat "}, {"name": "torch.sparse.softmax", "type": "torch.sparse", "path": "stable/generated/torch.sparse.softmax.html#torch.sparse.softmax", "desc": "Applies a softmax function."}, {"name": "torch.sparse.log_softmax", "type": "torch.sparse", "path": "stable/generated/torch.sparse.log_softmax.html#torch.sparse.log_softmax", "desc": "Applies a softmax function followed by logarithm."}, {"name": "torch.DoubleStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.DoubleStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.FloatStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.FloatStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.HalfStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.HalfStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.LongStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.LongStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.IntStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.IntStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.ShortStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.ShortStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.CharStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.CharStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.ByteStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.ByteStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.BoolStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.BoolStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.BFloat16Storage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.BFloat16Storage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.ComplexDoubleStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.ComplexDoubleStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.ComplexFloatStorage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.ComplexFloatStorage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.QUInt8Storage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.QUInt8Storage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.QInt8Storage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.QInt8Storage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.QInt32Storage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.QInt32Storage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.QUInt4x2Storage", "type": "torchtorch.storage", "path": "stable/storage.html#torch.QUInt4x2Storage", "desc": "bfloat16 ( ) \u00b6 Casts this storage to bfloat16 type bool ( ) \u00b6 Casts this storage to bool type byte ( ) \u00b6 Casts this storage to byte type char ( ) \u00b6 Casts this storage to char type clone ( ) \u00b6 Returns a copy of this storage complex_double ( ) \u00b6 Casts this storage to complex double type complex_float ( ) \u00b6 Casts this storage to complex float type copy_ ( source non_blocking = None ) \u00b6 cpu ( ) \u00b6 Returns a CPU copy of this storage if it\u2019s not already on the CPU cuda ( device = None non_blocking = False ** kwargs ) \u00b6 data_ptr ( ) \u00b6 property device \u00b6 double ( ) \u00b6 Casts this storage to double type dtype = torch."}, {"name": "torch.testing.assert_close", "type": "torchtorch.testing", "path": "stable/testing.html#torch.testing.assert_close", "desc": "Asserts that actual and expected are close."}, {"name": "torch.testing.make_tensor", "type": "torchtorch.testing", "path": "stable/testing.html#torch.testing.make_tensor", "desc": "Creates a tensor with the given shape device and dtype and filled withvalues uniformly drawn from [low, high) If low or high are specified and are outside the range of the dtype \u2019s representablefinite values then they are clamped to the lowest or highest representable finite value, respectively."}, {"name": "torch.utils.benchmark.Timer", "type": "torchtorch.benchmark_utils", "path": "stable/benchmark_utils.html#torch.utils.benchmark.Timer", "desc": "Helper class for measuring execution time of PyTorch statements."}, {"name": "torch.utils.benchmark.Measurement", "type": "torchtorch.benchmark_utils", "path": "stable/benchmark_utils.html#torch.utils.benchmark.Measurement", "desc": "The result of a Timer measurement."}, {"name": "torch.utils.benchmark.CallgrindStats", "type": "torchtorch.benchmark_utils", "path": "stable/benchmark_utils.html#torch.utils.benchmark.CallgrindStats", "desc": "Top level container for Callgrind results collected by Timer."}, {"name": "torch.utils.benchmark.FunctionCounts", "type": "torchtorch.benchmark_utils", "path": "stable/benchmark_utils.html#torch.utils.benchmark.FunctionCounts", "desc": "Container for manipulating Callgrind results."}, {"name": "torch.utils.checkpoint.checkpoint", "type": "torchtorch.checkpoint", "path": "stable/checkpoint.html#torch.utils.checkpoint.checkpoint", "desc": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory."}, {"name": "torch.utils.checkpoint.checkpoint_sequential", "type": "torchtorch.checkpoint", "path": "stable/checkpoint.html#torch.utils.checkpoint.checkpoint_sequential", "desc": "A helper function for checkpointing sequential models."}, {"name": "torch.utils.cpp_extension.CppExtension", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.CppExtension", "desc": "Creates a setuptools."}, {"name": "torch.utils.cpp_extension.CUDAExtension", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension", "desc": "Creates a setuptools."}, {"name": "torch.utils.cpp_extension.BuildExtension", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.BuildExtension", "desc": "A custom setuptools build extension This setuptools."}, {"name": "torch.utils.cpp_extension.load", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.load", "desc": "Loads a PyTorch C++ extension just-in-time (JIT)."}, {"name": "torch.utils.cpp_extension.load_inline", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.load_inline", "desc": "Loads a PyTorch C++ extension just-in-time (JIT) from string sources."}, {"name": "torch.utils.cpp_extension.include_paths", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.include_paths", "desc": "Get the include paths required to build a C++ or CUDA extension."}, {"name": "torch.utils.cpp_extension.check_compiler_abi_compatibility", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.check_compiler_abi_compatibility", "desc": "Verifies that the given compiler is ABI-compatible with PyTorch."}, {"name": "torch.utils.cpp_extension.verify_ninja_availability", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.verify_ninja_availability", "desc": "Raises RuntimeError if ninja build system is notavailable on the system, does nothing otherwise."}, {"name": "torch.utils.cpp_extension.is_ninja_available", "type": "torchtorch.cpp_extension", "path": "stable/cpp_extension.html#torch.utils.cpp_extension.is_ninja_available", "desc": "Returns True if the ninja build system isavailable on the system, False otherwise."}, {"name": "torch.utils.data.DataLoader", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.DataLoader", "desc": "Data loader."}, {"name": "torch.utils.data.Dataset", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.Dataset", "desc": "An abstract class representing a Dataset All datasets that represent a map from keys to data samples should subclassit."}, {"name": "torch.utils.data.IterableDataset", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.IterableDataset", "desc": "An iterable Dataset."}, {"name": "torch.utils.data.TensorDataset", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.TensorDataset", "desc": "Dataset wrapping tensors."}, {"name": "torch.utils.data.ConcatDataset", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.ConcatDataset", "desc": "Dataset as a concatenation of multiple datasets."}, {"name": "torch.utils.data.ChainDataset", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.ChainDataset", "desc": "Dataset for chaining multiple IterableDataset s."}, {"name": "torch.utils.data.Subset", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.Subset", "desc": "Subset of a dataset at specified indices."}, {"name": "torch.utils.data.default_collate", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.default_collate", "desc": "Function that takes in a batch of data and puts the elements within the batchinto a tensor with an additional outer dimension - batch size."}, {"name": "torch.utils.data.default_convert", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.default_convert", "desc": "Function that converts each NumPy array element into a torch."}, {"name": "torch.utils.data.get_worker_info", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.get_worker_info", "desc": "Returns the information about the current DataLoader iterator worker process."}, {"name": "torch.utils.data.random_split", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.random_split", "desc": "Randomly split a dataset into non-overlapping new datasets of given lengths."}, {"name": "torch.utils.data.Sampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.Sampler", "desc": "Base class for all Samplers."}, {"name": "torch.utils.data.SequentialSampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.SequentialSampler", "desc": "Samples elements sequentially, always in the same order."}, {"name": "torch.utils.data.RandomSampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.RandomSampler", "desc": "Samples elements randomly."}, {"name": "torch.utils.data.SubsetRandomSampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.SubsetRandomSampler", "desc": "Samples elements randomly from a given list of indices, without replacement."}, {"name": "torch.utils.data.WeightedRandomSampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.WeightedRandomSampler", "desc": "Samples elements from [0,."}, {"name": "torch.utils.data.BatchSampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.BatchSampler", "desc": "Wraps another sampler to yield a mini-batch of indices."}, {"name": "torch.utils.data.distributed.DistributedSampler", "type": "torchtorch.data", "path": "stable/data.html#torch.utils.data.distributed.DistributedSampler", "desc": "Sampler that restricts data loading to a subset of the dataset."}, {"name": "torch.utils.dlpack.from_dlpack", "type": "torchtorch.dlpack", "path": "stable/dlpack.html#torch.utils.dlpack.from_dlpack", "desc": "Converts a tensor from an external library into a torch."}, {"name": "torch.utils.dlpack.to_dlpack", "type": "torchtorch.dlpack", "path": "stable/dlpack.html#torch.utils.dlpack.to_dlpack", "desc": "Returns an opaque object (a \u201cDLPack capsule\u201d) representing the tensor."}, {"name": "torch.utils.mobile_optimizer.optimize_for_mobile", "type": "torchtorch.mobile_optimizer", "path": "stable/mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile", "desc": "Parameters script_module \u2013 An instance of torch script module with type of ScriptModule."}, {"name": "torch.utils.model_zoo.load_url", "type": "torchtorch.model_zoo", "path": "stable/model_zoo.html#torch.utils.model_zoo.load_url", "desc": "Loads the Torch serialized object at the given URL."}, {"name": "torch.utils.tensorboard.writer.SummaryWriter", "type": "torchtorch.tensorboard", "path": "stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter", "desc": "Writes entries directly to event files in the log_dir to beconsumed by TensorBoard."}, {"name": "torch.torch.finfo", "type": "torchtorch.type_info", "path": "stable/type_info.html#torch.torch.finfo", "desc": "."}, {"name": "torch.torch.iinfo", "type": "torchtorch.type_info", "path": "stable/type_info.html#torch.torch.iinfo", "desc": "."}]