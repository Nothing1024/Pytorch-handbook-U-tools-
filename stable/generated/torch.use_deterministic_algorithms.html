<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.use_deterministic_algorithms — PyTorch 1.11.0 documentation
  </title>
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-use-deterministic-algorithms">
         <h1>
          torch.use_deterministic_algorithms
          <a class="headerlink" href="#torch-use-deterministic-algorithms" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py function">
          <dt id="torch.use_deterministic_algorithms">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             use_deterministic_algorithms
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              mode
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="o">
             <span class="pre">
              *
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              warn_only
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              False
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           <a class="reference internal" href="../_modules/torch.html#use_deterministic_algorithms">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.use_deterministic_algorithms" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Sets whether PyTorch operations must use “deterministic”
algorithms. That is, algorithms which, given the same input, and when
run on the same software and hardware, always produce the same output.
When enabled, operations will use deterministic algorithms when available,
and if only nondeterministic algorithms are available they will throw a
            <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.10)">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               RuntimeError
              </span>
             </code>
            </a>
            when called.
           </p>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             <a class="reference internal" href="torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode" title="torch.set_deterministic_debug_mode">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                torch.set_deterministic_debug_mode()
               </span>
              </code>
             </a>
             offers an alternative
interface for this feature.
            </p>
           </div>
           <p>
            The following normally-nondeterministic operations will act
deterministically when
            <code class="docutils literal notranslate">
             <span class="pre">
              mode=True
             </span>
            </code>
            :
           </p>
           <blockquote>
            <div>
             <ul class="simple">
              <li>
               <p>
                <a class="reference internal" href="torch.nn.Conv1d.html#torch.nn.Conv1d" title="torch.nn.Conv1d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.Conv1d
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.Conv2d
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.Conv3d.html#torch.nn.Conv3d" title="torch.nn.Conv3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.Conv3d
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ConvTranspose1d
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ConvTranspose2d
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ConvTranspose3d
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.bmm.html#torch.bmm" title="torch.bmm">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.bmm()
                  </span>
                 </code>
                </a>
                when called on sparse-dense CUDA tensors
               </p>
              </li>
              <li>
               <p>
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.Tensor.__getitem__()
                 </span>
                </code>
                when attempting to differentiate a CPU tensor
and the index is a list of tensors
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.index_put.html#torch.Tensor.index_put" title="torch.Tensor.index_put">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.index_put()
                  </span>
                 </code>
                </a>
                with
                <code class="docutils literal notranslate">
                 <span class="pre">
                  accumulate=False
                 </span>
                </code>
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.index_put.html#torch.Tensor.index_put" title="torch.Tensor.index_put">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.index_put()
                  </span>
                 </code>
                </a>
                with
                <code class="docutils literal notranslate">
                 <span class="pre">
                  accumulate=True
                 </span>
                </code>
                when called on a CPU
tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.put_.html#torch.Tensor.put_" title="torch.Tensor.put_">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.put_()
                  </span>
                 </code>
                </a>
                with
                <code class="docutils literal notranslate">
                 <span class="pre">
                  accumulate=True
                 </span>
                </code>
                when called on a CPU
tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.scatter_add_()
                  </span>
                 </code>
                </a>
                when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  input
                 </span>
                </code>
                dimension is one and called
on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.gather.html#torch.gather" title="torch.gather">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.gather()
                  </span>
                 </code>
                </a>
                when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  input
                 </span>
                </code>
                dimension is one and called
on a CUDA tensor that requires grad
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.index_add.html#torch.index_add" title="torch.index_add">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.index_add()
                  </span>
                 </code>
                </a>
                when called on CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.index_select.html#torch.index_select" title="torch.index_select">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.index_select()
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.repeat_interleave.html#torch.repeat_interleave" title="torch.repeat_interleave">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.repeat_interleave()
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.index_copy.html#torch.Tensor.index_copy" title="torch.Tensor.index_copy">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.index_copy()
                  </span>
                 </code>
                </a>
                when called on a CPU or CUDA tensor
               </p>
              </li>
             </ul>
            </div>
           </blockquote>
           <p>
            The following normally-nondeterministic operations will throw a
            <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.10)">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               RuntimeError
              </span>
             </code>
            </a>
            when
            <code class="docutils literal notranslate">
             <span class="pre">
              mode=True
             </span>
            </code>
            :
           </p>
           <blockquote>
            <div>
             <ul class="simple">
              <li>
               <p>
                <a class="reference internal" href="torch.nn.AvgPool3d.html#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.AvgPool3d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.AdaptiveAvgPool2d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.AdaptiveAvgPool3d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.MaxPool3d.html#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.MaxPool3d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.AdaptiveMaxPool2d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d" title="torch.nn.FractionalMaxPool2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.FractionalMaxPool2d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d" title="torch.nn.FractionalMaxPool3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.FractionalMaxPool3d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.functional.interpolate.html#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.nn.functional.interpolate()
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
and one of the following modes is used:
               </p>
               <ul>
                <li>
                 <p>
                  <code class="docutils literal notranslate">
                   <span class="pre">
                    linear
                   </span>
                  </code>
                 </p>
                </li>
                <li>
                 <p>
                  <code class="docutils literal notranslate">
                   <span class="pre">
                    bilinear
                   </span>
                  </code>
                 </p>
                </li>
                <li>
                 <p>
                  <code class="docutils literal notranslate">
                   <span class="pre">
                    bicubic
                   </span>
                  </code>
                 </p>
                </li>
                <li>
                 <p>
                  <code class="docutils literal notranslate">
                   <span class="pre">
                    trilinear
                   </span>
                  </code>
                 </p>
                </li>
               </ul>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d" title="torch.nn.ReflectionPad1d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ReflectionPad1d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ReflectionPad2d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d" title="torch.nn.ReflectionPad3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ReflectionPad3d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d" title="torch.nn.ReplicationPad1d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ReplicationPad1d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ReplicationPad2d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d" title="torch.nn.ReplicationPad3d">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.ReplicationPad3d
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.NLLLoss.html#torch.nn.NLLLoss" title="torch.nn.NLLLoss">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.NLLLoss
                  </span>
                 </code>
                </a>
                when called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.CTCLoss.html#torch.nn.CTCLoss" title="torch.nn.CTCLoss">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.CTCLoss
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.nn.EmbeddingBag
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  mode='max'
                 </span>
                </code>
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.scatter_add_()
                  </span>
                 </code>
                </a>
                when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  input
                 </span>
                </code>
                dimension is larger than one
and called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.gather.html#torch.gather" title="torch.gather">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.gather()
                  </span>
                 </code>
                </a>
                when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  input
                 </span>
                </code>
                dimension is larger than one
and called on a CUDA tensor that requires grad
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.put_.html#torch.Tensor.put_" title="torch.Tensor.put_">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.put_()
                  </span>
                 </code>
                </a>
                when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  accumulate=False
                 </span>
                </code>
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.Tensor.put_.html#torch.Tensor.put_" title="torch.Tensor.put_">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.Tensor.put_()
                  </span>
                 </code>
                </a>
                when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  accumulate=True
                 </span>
                </code>
                and called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.histc.html#torch.histc" title="torch.histc">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.histc()
                  </span>
                 </code>
                </a>
                when called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.bincount.html#torch.bincount" title="torch.bincount">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.bincount()
                  </span>
                 </code>
                </a>
                when called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.kthvalue.html#torch.kthvalue" title="torch.kthvalue">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.kthvalue()
                  </span>
                 </code>
                </a>
                with called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.median.html#torch.median" title="torch.median">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.median()
                  </span>
                 </code>
                </a>
                with indices output when called on a CUDA tensor
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.nn.functional.grid_sample()
                  </span>
                 </code>
                </a>
                when attempting to differentiate a CUDA tensor
               </p>
              </li>
             </ul>
            </div>
           </blockquote>
           <p>
            A handful of CUDA operations are nondeterministic if the CUDA version is
10.2 or greater, unless the environment variable
            <code class="docutils literal notranslate">
             <span class="pre">
              CUBLAS_WORKSPACE_CONFIG=:4096:8
             </span>
            </code>
            or
            <code class="docutils literal notranslate">
             <span class="pre">
              CUBLAS_WORKSPACE_CONFIG=:16:8
             </span>
            </code>
            is set. See the CUDA documentation for more
details:
            <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility">
             https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
            </a>
            If one of these environment variable configurations is not set, a
            <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.10)">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               RuntimeError
              </span>
             </code>
            </a>
            will be raised from these operations when called with CUDA tensors:
           </p>
           <blockquote>
            <div>
             <ul class="simple">
              <li>
               <p>
                <a class="reference internal" href="torch.mm.html#torch.mm" title="torch.mm">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.mm()
                  </span>
                 </code>
                </a>
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.mv.html#torch.mv" title="torch.mv">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.mv()
                  </span>
                 </code>
                </a>
               </p>
              </li>
              <li>
               <p>
                <a class="reference internal" href="torch.bmm.html#torch.bmm" title="torch.bmm">
                 <code class="xref py py-func docutils literal notranslate">
                  <span class="pre">
                   torch.bmm()
                  </span>
                 </code>
                </a>
               </p>
              </li>
             </ul>
            </div>
           </blockquote>
           <p>
            Note that deterministic operations tend to have worse performance than
nondeterministic operations.
           </p>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             This flag does not detect or prevent nondeterministic behavior caused
by calling an inplace operation on a tensor with an internal memory
overlap or by giving such a tensor as the
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               out
              </span>
             </code>
             argument for an
operation. In these cases, multiple writes of different data may target
a single memory location, and the order of writes is not guaranteed.
            </p>
           </div>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <p>
              <strong>
               mode
              </strong>
              (
              <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 bool
                </span>
               </code>
              </a>
              ) – If True, makes potentially nondeterministic
operations switch to a deterministic algorithm or throw a runtime
error. If False, allows nondeterministic operations.
             </p>
            </dd>
            <dt class="field-even">
             Keyword Arguments
            </dt>
            <dd class="field-even">
             <p>
              <strong>
               warn_only
              </strong>
              (
              <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 bool
                </span>
               </code>
              </a>
              , optional) – If True, operations that do not
have a deterministic implementation will throw a warning instead of
an error. Default:
              <code class="docutils literal notranslate">
               <span class="pre">
                False
               </span>
              </code>
             </p>
            </dd>
           </dl>
           <p>
            Example:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">use_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="go"># Forward mode nondeterministic error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">index_copy</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">...</span>
<span class="go">RuntimeError: index_copy does not have a deterministic implementation...</span>

<span class="go"># Backward mode nondeterministic error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda'</span><span class="p">)</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda'</span><span class="p">))</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">...</span>
<span class="go">RuntimeError: index_add_cuda_ does not have a deterministic implementation...</span>
</pre>
            </div>
           </div>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>