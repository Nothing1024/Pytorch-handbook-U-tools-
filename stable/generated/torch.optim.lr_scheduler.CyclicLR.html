<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   CyclicLR — PyTorch 1.10 documentation
  </title>
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="cycliclr">
         <h1>
          CyclicLR
          <a class="headerlink" href="#cycliclr" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py class">
          <dt id="torch.optim.lr_scheduler.CyclicLR">
           <em class="property">
            <span class="pre">
             class
            </span>
           </em>
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.optim.lr_scheduler.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             CyclicLR
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              optimizer
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              base_lr
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              max_lr
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              step_size_up
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              2000
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              step_size_down
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              None
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              mode
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              'triangular'
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              gamma
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              1.0
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              scale_fn
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              None
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              scale_mode
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              'cycle'
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              cycle_momentum
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              True
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              base_momentum
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              0.8
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              max_momentum
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              0.9
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              last_epoch
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              -
             </span>
             <span class="pre">
              1
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              verbose
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              False
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           <a class="reference internal" href="../_modules/torch/optim/lr_scheduler.html#CyclicLR">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.optim.lr_scheduler.CyclicLR" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Sets the learning rate of each parameter group according to
cyclical learning rate policy (CLR). The policy cycles the learning
rate between two boundaries with a constant frequency, as detailed in
the paper
            <a class="reference external" href="https://arxiv.org/abs/1506.01186">
             Cyclical Learning Rates for Training Neural Networks
            </a>
            .
The distance between the two boundaries can be scaled on a per-iteration
or per-cycle basis.
           </p>
           <p>
            Cyclical learning rate policy changes the learning rate after every batch.
            <cite>
             step
            </cite>
            should be called after a batch has been used for training.
           </p>
           <p>
            This class has three built-in policies, as put forth in the paper:
           </p>
           <ul class="simple">
            <li>
             <p>
              “triangular”: A basic triangular cycle without amplitude scaling.
             </p>
            </li>
            <li>
             <p>
              “triangular2”: A basic triangular cycle that scales initial amplitude by half each cycle.
             </p>
            </li>
            <li>
             <p>
              “exp_range”: A cycle that scales initial amplitude by
              <span class="math">
               <span class="katex">
                <span class="katex-mathml">
                 <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                   <mrow>
                    <msup>
                     <mtext>
                      gamma
                     </mtext>
                     <mtext>
                      cycle iterations
                     </mtext>
                    </msup>
                   </mrow>
                   <annotation encoding="application/x-tex">
                    \text{gamma}^{\text{cycle iterations}}
                   </annotation>
                  </semantics>
                 </math>
                </span>
                <span aria-hidden="true" class="katex-html">
                 <span class="base">
                  <span class="strut" style="height:1.043548em;vertical-align:-0.19444em;">
                  </span>
                  <span class="mord">
                   <span class="mord text">
                    <span class="mord">
                     gamma
                    </span>
                   </span>
                   <span class="msupsub">
                    <span class="vlist-t">
                     <span class="vlist-r">
                      <span class="vlist" style="height:0.8491079999999999em;">
                       <span style="top:-3.063em;margin-right:0.05em;">
                        <span class="pstrut" style="height:2.7em;">
                        </span>
                        <span class="sizing reset-size6 size3 mtight">
                         <span class="mord mtight">
                          <span class="mord text mtight">
                           <span class="mord mtight">
                            cycle iterations
                           </span>
                          </span>
                         </span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
              </span>
              at each cycle iteration.
             </p>
            </li>
           </ul>
           <p>
            This implementation was adapted from the github repo:
            <a class="reference external" href="https://github.com/bckenstler/CLR">
             bckenstler/CLR
            </a>
           </p>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <ul class="simple">
              <li>
               <p>
                <strong>
                 optimizer
                </strong>
                (
                <a class="reference internal" href="../optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer">
                 <em>
                  Optimizer
                 </em>
                </a>
                ) – Wrapped optimizer.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 base_lr
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">
                 <em>
                  float
                 </em>
                </a>
                <em>
                 or
                </em>
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">
                 <em>
                  list
                 </em>
                </a>
                ) – Initial learning rate which is the
lower boundary in the cycle for each parameter group.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 max_lr
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">
                 <em>
                  float
                 </em>
                </a>
                <em>
                 or
                </em>
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">
                 <em>
                  list
                 </em>
                </a>
                ) – Upper learning rate boundaries in the cycle
for each parameter group. Functionally,
it defines the cycle amplitude (max_lr - base_lr).
The lr at any cycle is the sum of base_lr
and some scaling of the amplitude; therefore
max_lr may not actually be reached depending on
scaling function.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 step_size_up
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">
                 <em>
                  int
                 </em>
                </a>
                ) – Number of training iterations in the
increasing half of a cycle. Default: 2000
               </p>
              </li>
              <li>
               <p>
                <strong>
                 step_size_down
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">
                 <em>
                  int
                 </em>
                </a>
                ) – Number of training iterations in the
decreasing half of a cycle. If step_size_down is None,
it is set to step_size_up. Default: None
               </p>
              </li>
              <li>
               <p>
                <strong>
                 mode
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)">
                 <em>
                  str
                 </em>
                </a>
                ) – One of {triangular, triangular2, exp_range}.
Values correspond to policies detailed above.
If scale_fn is not None, this argument is ignored.
Default: ‘triangular’
               </p>
              </li>
              <li>
               <p>
                <strong>
                 gamma
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">
                 <em>
                  float
                 </em>
                </a>
                ) – Constant in ‘exp_range’ scaling function:
gamma**(cycle iterations)
Default: 1.0
               </p>
              </li>
              <li>
               <p>
                <strong>
                 scale_fn
                </strong>
                (
                <em>
                 function
                </em>
                ) – Custom scaling policy defined by a single
argument lambda function, where
0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0.
If specified, then ‘mode’ is ignored.
Default: None
               </p>
              </li>
              <li>
               <p>
                <strong>
                 scale_mode
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)">
                 <em>
                  str
                 </em>
                </a>
                ) – {‘cycle’, ‘iterations’}.
Defines whether scale_fn is evaluated on
cycle number or cycle iterations (training
iterations since start of cycle).
Default: ‘cycle’
               </p>
              </li>
              <li>
               <p>
                <strong>
                 cycle_momentum
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                ) – If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                , momentum is cycled inversely
to learning rate between ‘base_momentum’ and ‘max_momentum’.
Default: True
               </p>
              </li>
              <li>
               <p>
                <strong>
                 base_momentum
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">
                 <em>
                  float
                 </em>
                </a>
                <em>
                 or
                </em>
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">
                 <em>
                  list
                 </em>
                </a>
                ) – Lower momentum boundaries in the cycle
for each parameter group. Note that momentum is cycled inversely
to learning rate; at the peak of a cycle, momentum is
‘base_momentum’ and learning rate is ‘max_lr’.
Default: 0.8
               </p>
              </li>
              <li>
               <p>
                <strong>
                 max_momentum
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">
                 <em>
                  float
                 </em>
                </a>
                <em>
                 or
                </em>
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">
                 <em>
                  list
                 </em>
                </a>
                ) – Upper momentum boundaries in the cycle
for each parameter group. Functionally,
it defines the cycle amplitude (max_momentum - base_momentum).
The momentum at any cycle is the difference of max_momentum
and some scaling of the amplitude; therefore
base_momentum may not actually be reached depending on
scaling function. Note that momentum is cycled inversely
to learning rate; at the start of a cycle, momentum is ‘max_momentum’
and learning rate is ‘base_lr’
Default: 0.9
               </p>
              </li>
              <li>
               <p>
                <strong>
                 last_epoch
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">
                 <em>
                  int
                 </em>
                </a>
                ) – The index of the last batch. This parameter is used when
resuming a training job. Since
                <cite>
                 step()
                </cite>
                should be invoked after each
batch instead of after each epoch, this number represents the total
number of
                <em>
                 batches
                </em>
                computed, not the total number of epochs computed.
When last_epoch=-1, the schedule is started from the beginning.
Default: -1
               </p>
              </li>
              <li>
               <p>
                <strong>
                 verbose
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                ) – If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                , prints a message to stdout for
each update. Default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                .
               </p>
              </li>
             </ul>
            </dd>
           </dl>
           <p class="rubric">
            Example
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CyclicLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre>
            </div>
           </div>
           <dl class="py method">
            <dt id="torch.optim.lr_scheduler.CyclicLR.get_last_lr">
             <code class="sig-name descname">
              <span class="pre">
               get_last_lr
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <span class="sig-paren">
              )
             </span>
             <a class="headerlink" href="#torch.optim.lr_scheduler.CyclicLR.get_last_lr" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Return last computed learning rate by current scheduler.
             </p>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.optim.lr_scheduler.CyclicLR.get_lr">
             <code class="sig-name descname">
              <span class="pre">
               get_lr
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <span class="sig-paren">
              )
             </span>
             <a class="reference internal" href="../_modules/torch/optim/lr_scheduler.html#CyclicLR.get_lr">
              <span class="viewcode-link">
               <span class="pre">
                [source]
               </span>
              </span>
             </a>
             <a class="headerlink" href="#torch.optim.lr_scheduler.CyclicLR.get_lr" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Calculates the learning rate at batch index. This function treats
              <cite>
               self.last_epoch
              </cite>
              as the last batch index.
             </p>
             <p>
              If
              <cite>
               self.cycle_momentum
              </cite>
              is
              <code class="docutils literal notranslate">
               <span class="pre">
                True
               </span>
              </code>
              , this function has a side effect of
updating the optimizer’s momentum.
             </p>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.optim.lr_scheduler.CyclicLR.load_state_dict">
             <code class="sig-name descname">
              <span class="pre">
               load_state_dict
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                state_dict
               </span>
              </span>
             </em>
             <span class="sig-paren">
              )
             </span>
             <a class="headerlink" href="#torch.optim.lr_scheduler.CyclicLR.load_state_dict" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Loads the schedulers state.
             </p>
             <dl class="field-list simple">
              <dt class="field-odd">
               Parameters
              </dt>
              <dd class="field-odd">
               <p>
                <strong>
                 state_dict
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
                 <em>
                  dict
                 </em>
                </a>
                ) – scheduler state. Should be an object returned
from a call to
                <a class="reference internal" href="#torch.optim.lr_scheduler.CyclicLR.state_dict" title="torch.optim.lr_scheduler.CyclicLR.state_dict">
                 <code class="xref py py-meth docutils literal notranslate">
                  <span class="pre">
                   state_dict()
                  </span>
                 </code>
                </a>
                .
               </p>
              </dd>
             </dl>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.optim.lr_scheduler.CyclicLR.print_lr">
             <code class="sig-name descname">
              <span class="pre">
               print_lr
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                is_verbose
               </span>
              </span>
             </em>
             ,
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                group
               </span>
              </span>
             </em>
             ,
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                lr
               </span>
              </span>
             </em>
             ,
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                epoch
               </span>
              </span>
              <span class="o">
               <span class="pre">
                =
               </span>
              </span>
              <span class="default_value">
               <span class="pre">
                None
               </span>
              </span>
             </em>
             <span class="sig-paren">
              )
             </span>
             <a class="headerlink" href="#torch.optim.lr_scheduler.CyclicLR.print_lr" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Display the current learning rate.
             </p>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.optim.lr_scheduler.CyclicLR.state_dict">
             <code class="sig-name descname">
              <span class="pre">
               state_dict
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <span class="sig-paren">
              )
             </span>
             <a class="headerlink" href="#torch.optim.lr_scheduler.CyclicLR.state_dict" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Returns the state of the scheduler as a
              <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 dict
                </span>
               </code>
              </a>
              .
             </p>
             <p>
              It contains an entry for every variable in self.__dict__ which
is not the optimizer.
             </p>
            </dd>
           </dl>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>