<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.einsum — PyTorch 1.10 documentation
  </title>
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-einsum">
         <h1>
          torch.einsum
          <a class="headerlink" href="#torch-einsum" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py function">
          <dt id="torch.einsum">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             einsum
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              equation
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="o">
             <span class="pre">
              *
             </span>
            </span>
            <span class="n">
             <span class="pre">
              operands
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           →
           <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">
            <span class="pre">
             Tensor
            </span>
           </a>
           <a class="reference internal" href="../_modules/torch/functional.html#einsum">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.einsum" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Sums the product of the elements of the input
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              operands
             </span>
            </code>
            along dimensions specified using a notation
based on the Einstein summation convention.
           </p>
           <p>
            Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them
in a short-hand format based on the Einstein summation convention, given by
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              equation
             </span>
            </code>
            . The details of
this format are described below, but the general idea is to label every dimension of the input
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              operands
             </span>
            </code>
            with some subscript and define which subscripts are part of the output. The output is then computed by summing
the product of the elements of the
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              operands
             </span>
            </code>
            along the dimensions whose subscripts are not part of the
output. For example, matrix multiplication can be computed using einsum as
            <cite>
             torch.einsum(“ij,jk-&gt;ik”, A, B)
            </cite>
            .
Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).
           </p>
           <p>
            Equation:
           </p>
           <blockquote>
            <div>
             <p>
              The
              <code class="xref py py-attr docutils literal notranslate">
               <span class="pre">
                equation
               </span>
              </code>
              string specifies the subscripts (letters in
              <cite>
               [a-zA-Z]
              </cite>
              ) for each dimension of
the input
              <code class="xref py py-attr docutils literal notranslate">
               <span class="pre">
                operands
               </span>
              </code>
              in the same order as the dimensions, separating subcripts for each operand by a
comma (‘,’), e.g.
              <cite>
               ‘ij,jk’
              </cite>
              specify subscripts for two 2D operands. The dimensions labeled with the same subscript
must be broadcastable, that is, their size must either match or be
              <cite>
               1
              </cite>
              . The exception is if a subscript is
repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand
must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that
appear exactly once in the
              <code class="xref py py-attr docutils literal notranslate">
               <span class="pre">
                equation
               </span>
              </code>
              will be part of the output, sorted in increasing alphabetical order.
The output is computed by multiplying the input
              <code class="xref py py-attr docutils literal notranslate">
               <span class="pre">
                operands
               </span>
              </code>
              element-wise, with their dimensions aligned based
on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.
             </p>
             <p>
              Optionally, the output subscripts can be explicitly defined by adding an arrow (‘-&gt;’) at the end of the equation
followed by the subscripts for the output. For instance, the following equation computes the transpose of a
matrix multiplication: ‘ij,jk-&gt;ki’. The output subscripts must appear at least once for some input operand and
at most once for the output.
             </p>
             <p>
              Ellipsis (‘…’) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.
Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,
e.g. for an input operand with 5 dimensions, the ellipsis in the equation
              <cite>
               ‘ab…c’
              </cite>
              cover the third and fourth
dimensions. The ellipsis does not need to cover the same number of dimensions across the
              <code class="xref py py-attr docutils literal notranslate">
               <span class="pre">
                operands
               </span>
              </code>
              but the
‘shape’ of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not
explicitly defined with the arrow (‘-&gt;’) notation, the ellipsis will come first in the output (left-most dimensions),
before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements
batch matrix multiplication
              <cite>
               ‘…ij,…jk’
              </cite>
              .
             </p>
             <p>
              A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,
arrow and comma) but something like
              <cite>
               ‘…’
              </cite>
              is not valid. An empty string
              <cite>
               ‘’
              </cite>
              is valid for scalar operands.
             </p>
            </div>
           </blockquote>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.einsum
              </span>
             </code>
             handles ellipsis (‘…’) differently from NumPy in that it allows dimensions
covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.
            </p>
           </div>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             This function does not optimize the given expression, so a different formula for the same computation may
run faster or consume less memory. Projects like opt_einsum (
             <a class="reference external" href="https://optimized-einsum.readthedocs.io/en/stable/">
              https://optimized-einsum.readthedocs.io/en/stable/
             </a>
             )
can optimize the formula for you.
            </p>
           </div>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             As of PyTorch 1.10
             <a class="reference internal" href="#torch.einsum" title="torch.einsum">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                torch.einsum()
               </span>
              </code>
             </a>
             also supports the sublist format (see examples below). In this format,
subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists
follow their operands, and an extra sublist can appear at the end of the input to specify the output’s
subscripts., e.g.
             <cite>
              torch.einsum(op1, sublist1, op2, sublist2, …, [subslist_out])
             </cite>
             . Python’s
             <cite>
              Ellipsis
             </cite>
             object
may be provided in a sublist to enable broadcasting as described in the Equation section above.
            </p>
           </div>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <ul class="simple">
              <li>
               <p>
                <strong>
                 equation
                </strong>
                (
                <em>
                 string
                </em>
                ) – The subscripts for the Einstein summation.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 operands
                </strong>
                (
                <em>
                 List
                </em>
                <em>
                 [
                </em>
                <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">
                 <em>
                  Tensor
                 </em>
                </a>
                <em>
                 ]
                </em>
                ) – The tensors to compute the Einstein summation of.
               </p>
              </li>
             </ul>
            </dd>
           </dl>
           <p>
            Examples:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="c1"># trace</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'ii'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2104</span><span class="p">)</span>

<span class="c1"># diagonal</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'ii-&gt;i'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1034</span><span class="p">,</span>  <span class="mf">0.7952</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2433</span><span class="p">,</span>  <span class="mf">0.4545</span><span class="p">])</span>

<span class="c1"># outer product</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'i,j-&gt;ij'</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.1156</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2897</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3918</span><span class="p">,</span>  <span class="mf">0.4963</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3744</span><span class="p">,</span>  <span class="mf">0.9381</span><span class="p">,</span>  <span class="mf">1.2685</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6070</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.7208</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8058</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4419</span><span class="p">,</span>  <span class="mf">3.0936</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1713</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4291</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5802</span><span class="p">,</span>  <span class="mf">0.7350</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5704</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4290</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9323</span><span class="p">,</span>  <span class="mf">2.4480</span><span class="p">]])</span>

<span class="c1"># batch matrix multiplication</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">As</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Bs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'bij,bjk-&gt;bik'</span><span class="p">,</span> <span class="n">As</span><span class="p">,</span> <span class="n">Bs</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.0564</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5904</span><span class="p">,</span>  <span class="mf">3.2023</span><span class="p">,</span>  <span class="mf">3.1271</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.6706</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8025</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1183</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">4.2239</span><span class="p">,</span>  <span class="mf">0.3107</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5756</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2354</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4558</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3460</span><span class="p">,</span>  <span class="mf">1.5087</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8530</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">2.8153</span><span class="p">,</span>  <span class="mf">1.8787</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3839</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2112</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3728</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1131</span><span class="p">,</span>  <span class="mf">0.0921</span><span class="p">,</span>  <span class="mf">0.8305</span><span class="p">]]])</span>

<span class="c1"># with sublist format and ellipsis</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="n">As</span><span class="p">,</span> <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">Bs</span><span class="p">,</span> <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.0564</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5904</span><span class="p">,</span>  <span class="mf">3.2023</span><span class="p">,</span>  <span class="mf">3.1271</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.6706</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8025</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1183</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">4.2239</span><span class="p">,</span>  <span class="mf">0.3107</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5756</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2354</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4558</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3460</span><span class="p">,</span>  <span class="mf">1.5087</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8530</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">2.8153</span><span class="p">,</span>  <span class="mf">1.8787</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3839</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2112</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3728</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1131</span><span class="p">,</span>  <span class="mf">0.0921</span><span class="p">,</span>  <span class="mf">0.8305</span><span class="p">]]])</span>

<span class="c1"># batch permute</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'...ij-&gt;...ji'</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># equivalent to torch.nn.functional.bilinear</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'bn,anm,bm-&gt;ba'</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.3430</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.2405</span><span class="p">,</span>  <span class="mf">0.4494</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3311</span><span class="p">,</span>  <span class="mf">5.5201</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0356</span><span class="p">]])</span>
</pre>
            </div>
           </div>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>