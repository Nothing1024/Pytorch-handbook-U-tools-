<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.autograd.functional.jvp — PyTorch 1.10 documentation
  </title>
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-autograd-functional-jvp">
         <h1>
          torch.autograd.functional.jvp
          <a class="headerlink" href="#torch-autograd-functional-jvp" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py function">
          <dt id="torch.autograd.functional.jvp">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.autograd.functional.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             jvp
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              func
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              inputs
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              v
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              None
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              create_graph
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              False
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              strict
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              False
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           <a class="reference internal" href="../_modules/torch/autograd/functional.html#jvp">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.autograd.functional.jvp" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Function that computes the dot product between  the Jacobian of
the given function at the point given by the inputs and a vector
            <code class="docutils literal notranslate">
             <span class="pre">
              v
             </span>
            </code>
            .
           </p>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <ul class="simple">
              <li>
               <p>
                <strong>
                 func
                </strong>
                (
                <em>
                 function
                </em>
                ) – a Python function that takes Tensor inputs and returns
a tuple of Tensors or a Tensor.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 inputs
                </strong>
                (
                <em>
                 tuple of Tensors
                </em>
                <em>
                 or
                </em>
                <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">
                 <em>
                  Tensor
                 </em>
                </a>
                ) – inputs to the function
                <code class="docutils literal notranslate">
                 <span class="pre">
                  func
                 </span>
                </code>
                .
               </p>
              </li>
              <li>
               <p>
                <strong>
                 v
                </strong>
                (
                <em>
                 tuple of Tensors
                </em>
                <em>
                 or
                </em>
                <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">
                 <em>
                  Tensor
                 </em>
                </a>
                ) – The vector for which the Jacobian
vector product is computed. Must be the same size as the input of
                <code class="docutils literal notranslate">
                 <span class="pre">
                  func
                 </span>
                </code>
                . This argument is optional when the input to
                <code class="docutils literal notranslate">
                 <span class="pre">
                  func
                 </span>
                </code>
                contains a single element and (if it is not provided) will be set
as a Tensor containing a single
                <code class="docutils literal notranslate">
                 <span class="pre">
                  1
                 </span>
                </code>
                .
               </p>
              </li>
              <li>
               <p>
                <strong>
                 create_graph
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                <em>
                 ,
                </em>
                <em>
                 optional
                </em>
                ) – If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                , both the output and result
will be computed in a differentiable way. Note that when
                <code class="docutils literal notranslate">
                 <span class="pre">
                  strict
                 </span>
                </code>
                is
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                , the result can not require gradients or be
disconnected from the inputs.  Defaults to
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                .
               </p>
              </li>
              <li>
               <p>
                <strong>
                 strict
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                <em>
                 ,
                </em>
                <em>
                 optional
                </em>
                ) – If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                , an error will be raised when we
detect that there exists an input such that all the outputs are
independent of it. If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                , we return a Tensor of zeros as the
jvp for said inputs, which is the expected mathematical value.
Defaults to
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                .
               </p>
              </li>
             </ul>
            </dd>
            <dt class="field-even">
             Returns
            </dt>
            <dd class="field-even">
             <p>
              <dl>
               <dt>
                tuple with:
               </dt>
               <dd>
                <p>
                 func_output (tuple of Tensors or Tensor): output of
                 <code class="docutils literal notranslate">
                  <span class="pre">
                   func(inputs)
                  </span>
                 </code>
                </p>
                <p>
                 jvp (tuple of Tensors or Tensor): result of the dot product with
the same shape as the output.
                </p>
               </dd>
              </dl>
             </p>
            </dd>
            <dt class="field-odd">
             Return type
            </dt>
            <dd class="field-odd">
             <p>
              output (
              <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)">
               tuple
              </a>
              )
             </p>
            </dd>
           </dl>
           <p class="rubric">
            Example
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">exp_reducer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jvp</span><span class="p">(</span><span class="n">exp_reducer</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="go">(tensor([6.3090, 4.6742, 7.9114, 8.2106]),</span>
<span class="go"> tensor([6.3090, 4.6742, 7.9114, 8.2106]))</span>
</pre>
            </div>
           </div>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">jvp</span><span class="p">(</span><span class="n">exp_reducer</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SumBackward1&gt;),</span>
<span class="go"> tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SqueezeBackward1&gt;))</span>
</pre>
            </div>
           </div>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">adder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jvp</span><span class="p">(</span><span class="n">adder</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="go">(tensor([2.2399, 2.5005]),</span>
<span class="go"> tensor([5., 5.]))</span>
</pre>
            </div>
           </div>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             The jvp is currently computed by using the backward of the backward
(sometimes called the double backwards trick) as we don’t have support
for forward mode AD in PyTorch at the moment.
            </p>
           </div>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>