<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.asarray — PyTorch 1.12 documentation
  </title>
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-asarray">
         <h1>
          torch.asarray
          <a class="headerlink" href="#torch-asarray" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py function">
          <dt id="torch.asarray">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             asarray
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              obj
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="o">
             <span class="pre">
              *
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              dtype
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              None
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              device
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              None
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              copy
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              None
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              requires_grad
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              False
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           →
           <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">
            <span class="pre">
             Tensor
            </span>
           </a>
           <a class="headerlink" href="#torch.asarray" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Converts
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            to a tensor.
           </p>
           <p>
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            can be one of:
           </p>
           <ol class="arabic simple">
            <li>
             <p>
              a tensor
             </p>
            </li>
            <li>
             <p>
              a NumPy array
             </p>
            </li>
            <li>
             <p>
              a DLPack capsule
             </p>
            </li>
            <li>
             <p>
              an object that implements Python’s buffer protocol
             </p>
            </li>
            <li>
             <p>
              a scalar
             </p>
            </li>
            <li>
             <p>
              a sequence of scalars
             </p>
            </li>
           </ol>
           <p>
            When
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            is a tensor, NumPy array, or DLPack capsule the returned tensor will,
by default, not require a gradient, have the same datatype as
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            , be on the
same device, and share memory with it. These properties can be controlled with the
            <a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype">
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               dtype
              </span>
             </code>
            </a>
            ,
            <a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device">
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               device
              </span>
             </code>
            </a>
            ,
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              copy
             </span>
            </code>
            , and
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              requires_grad
             </span>
            </code>
            keyword arguments.
If the returned tensor is of a different datatype, on a different device, or a copy is
requested then it will not share its memory with
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            . If
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              requires_grad
             </span>
            </code>
            is
            <code class="docutils literal notranslate">
             <span class="pre">
              True
             </span>
            </code>
            then the returned tensor will require a gradient, and if
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            is
also a tensor with an autograd history then the returned tensor will have the same history.
           </p>
           <p>
            When
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            is not a tensor, NumPy Array, or DLPack capsule but implements Python’s
buffer protocol then the buffer is interpreted as an array of bytes grouped according to
the size of the datatype passed to the
            <a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype">
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               dtype
              </span>
             </code>
            </a>
            keyword argument. (If no datatype is
passed then the default floating point datatype is used, instead.) The returned tensor
will have the specified datatype (or default floating point datatype if none is specified)
and, by default, be on the CPU device and share memory with the buffer.
           </p>
           <p>
            When
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              obj
             </span>
            </code>
            is none of the above but a scalar or sequence of scalars then the
returned tensor will, by default, infer its datatype from the scalar values, be on the
CPU device, and not share its memory.
           </p>
           <div class="admonition seealso">
            <p class="admonition-title">
             See also
            </p>
            <p>
             <a class="reference internal" href="torch.tensor.html#torch.tensor" title="torch.tensor">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                torch.tensor()
               </span>
              </code>
             </a>
             creates a tensor that always copies the data from the input object.
            </p>
            <p>
             <a class="reference internal" href="torch.from_numpy.html#torch.from_numpy" title="torch.from_numpy">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                torch.from_numpy()
               </span>
              </code>
             </a>
             creates a tensor that always shares memory from NumPy arrays.
            </p>
            <dl class="simple">
             <dt>
              <a class="reference internal" href="torch.frombuffer.html#torch.frombuffer" title="torch.frombuffer">
               <code class="xref py py-func docutils literal notranslate">
                <span class="pre">
                 torch.frombuffer()
                </span>
               </code>
              </a>
              creates a tensor that always shares memory from objects that
             </dt>
             <dd>
              <p>
               implement the buffer protocol.
              </p>
             </dd>
             <dt>
              <a class="reference internal" href="torch.from_dlpack.html#torch.from_dlpack" title="torch.from_dlpack">
               <code class="xref py py-func docutils literal notranslate">
                <span class="pre">
                 torch.from_dlpack()
                </span>
               </code>
              </a>
              creates a tensor that always shares memory from
             </dt>
             <dd>
              <p>
               DLPack capsules.
              </p>
             </dd>
            </dl>
           </div>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <p>
              <strong>
               obj
              </strong>
              (
              <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.10)">
               <em>
                object
               </em>
              </a>
              ) – a tensor, NumPy array, DLPack Capsule, object that implements Python’s
buffer protocol, scalar, or sequence of scalars.
             </p>
            </dd>
            <dt class="field-even">
             Keyword Arguments
            </dt>
            <dd class="field-even">
             <ul class="simple">
              <li>
               <p>
                <strong>
                 dtype
                </strong>
                (
                <a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.dtype
                  </span>
                 </code>
                </a>
                , optional) – the datatype of the returned tensor.
Default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  None
                 </span>
                </code>
                , which causes the datatype of the returned tensor to be
inferred from
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  obj
                 </span>
                </code>
                .
               </p>
              </li>
              <li>
               <p>
                <strong>
                 copy
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                <em>
                 ,
                </em>
                <em>
                 optional
                </em>
                ) – controls whether the returned tensor shares memory with
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  obj
                 </span>
                </code>
                .
Default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  None
                 </span>
                </code>
                , which causes the returned tensor to share memory with
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  obj
                 </span>
                </code>
                whenever possible. If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                then the returned tensor does not share its memory.
If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                then the returned tensor shares its memory with
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  obj
                 </span>
                </code>
                and an
error is thrown if it cannot.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 device
                </strong>
                (
                <a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   torch.device
                  </span>
                 </code>
                </a>
                , optional) – the device of the returned tensor.
Default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  None
                 </span>
                </code>
                , which causes the device of
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  obj
                 </span>
                </code>
                to be used.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 requires_grad
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                <em>
                 ,
                </em>
                <em>
                 optional
                </em>
                ) – whether the returned tensor requires grad.
Default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                , which causes the returned tensor not to require a gradient.
If
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                , then the returned tensor will require a gradient, and if
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  obj
                 </span>
                </code>
                is also a tensor with an autograd history then the returned tensor will have
the same history.
               </p>
              </li>
             </ul>
            </dd>
           </dl>
           <p>
            Example:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Shares memory with tensor 'a'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Forces memory copy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">c</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1., 2., 3.], grad_fn=&lt;AddBackward0&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Shares memory with tensor 'b', with no grad</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor([1., 2., 3.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Shares memory with tensor 'b', retaining autograd history</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([1., 2., 3.], grad_fn=&lt;AddBackward0&gt;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Shares memory with array 'array'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array</span><span class="o">.</span><span class="n">__array_interface__</span><span class="p">[</span><span class="s1">'data'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">t1</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Copies memory due to dtype mismatch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array</span><span class="o">.</span><span class="n">__array_interface__</span><span class="p">[</span><span class="s1">'data'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">t1</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="go">False</span>
</pre>
            </div>
           </div>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>