<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.nn.utils.parametrize.cached — PyTorch 1.11.0 documentation
  </title>
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-nn-utils-parametrize-cached">
         <h1>
          torch.nn.utils.parametrize.cached
          <a class="headerlink" href="#torch-nn-utils-parametrize-cached" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py function">
          <dt id="torch.nn.utils.parametrize.cached">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.nn.utils.parametrize.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             cached
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <span class="sig-paren">
            )
           </span>
           <a class="reference internal" href="../_modules/torch/nn/utils/parametrize.html#cached">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.nn.utils.parametrize.cached" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Context manager that enables the caching system within parametrizations
registered with
            <a class="reference internal" href="torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization" title="torch.nn.utils.parametrize.register_parametrization">
             <code class="xref py py-func docutils literal notranslate">
              <span class="pre">
               register_parametrization()
              </span>
             </code>
            </a>
            .
           </p>
           <p>
            The value of the parametrized objects is computed and cached the first time
they are required when this context manager is active. The cached values are
discarded when leaving the context manager.
           </p>
           <p>
            This is useful when using a parametrized parameter more than once in the forward pass.
An example of this is when parametrizing the recurrent kernel of an RNN or when
sharing weights.
           </p>
           <p>
            The simplest way to activate the cache is by wrapping the forward pass of the neural network
           </p>
           <div class="highlight-python notranslate">
            <div class="highlight">
             <pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.utils.parametrize</span> <span class="k">as</span> <span class="nn">P</span>
<span class="o">...</span>
<span class="k">with</span> <span class="n">P</span><span class="o">.</span><span class="n">cached</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            in training and evaluation. One may also wrap the parts of the modules that use
several times the parametrized tensors. For example, the loop of an RNN with a
parametrized recurrent kernel:
           </p>
           <div class="highlight-python notranslate">
            <div class="highlight">
             <pre><span></span><span class="k">with</span> <span class="n">P</span><span class="o">.</span><span class="n">cached</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
        <span class="n">out_rnn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_rnn</span><span class="p">)</span>
</pre>
            </div>
           </div>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>