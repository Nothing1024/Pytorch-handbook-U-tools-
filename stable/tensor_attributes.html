<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Tensor Attributes — PyTorch 1.12 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="tensor-attributes">
         <span id="tensor-attributes-doc">
         </span>
         <h1>
          Tensor Attributes
          <a class="headerlink" href="#tensor-attributes" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <p>
          Each
          <code class="docutils literal notranslate">
           <span class="pre">
            torch.Tensor
           </span>
          </code>
          has a
          <a class="reference internal" href="#torch.dtype" title="torch.dtype">
           <code class="xref py py-class docutils literal notranslate">
            <span class="pre">
             torch.dtype
            </span>
           </code>
          </a>
          ,
          <a class="reference internal" href="#torch.device" title="torch.device">
           <code class="xref py py-class docutils literal notranslate">
            <span class="pre">
             torch.device
            </span>
           </code>
          </a>
          , and
          <a class="reference internal" href="#torch.layout" title="torch.layout">
           <code class="xref py py-class docutils literal notranslate">
            <span class="pre">
             torch.layout
            </span>
           </code>
          </a>
          .
         </p>
         <div class="section" id="torch-dtype">
          <span id="dtype-doc">
          </span>
          <h2>
           torch.dtype
           <a class="headerlink" href="#torch-dtype" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py class">
           <dt id="torch.dtype">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              dtype
             </span>
            </code>
            <a class="headerlink" href="#torch.dtype" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
           </dd>
          </dl>
          <p>
           A
           <a class="reference internal" href="#torch.dtype" title="torch.dtype">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.dtype
             </span>
            </code>
           </a>
           is an object that represents the data type of a
           <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
           </a>
           . PyTorch has twelve different data types:
          </p>
          <table class="docutils colwidths-auto align-default">
           <thead>
            <tr class="row-odd">
             <th class="head">
              <p>
               Data type
              </p>
             </th>
             <th class="head">
              <p>
               dtype
              </p>
             </th>
             <th class="head">
              <p>
               Legacy Constructors
              </p>
             </th>
            </tr>
           </thead>
           <tbody>
            <tr class="row-even">
             <td>
              <p>
               32-bit floating point
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.float32
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.float
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.FloatTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               64-bit floating point
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.float64
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.double
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.DoubleTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               64-bit complex
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.complex64
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.cfloat
                </span>
               </code>
              </p>
             </td>
             <td>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               128-bit complex
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.complex128
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.cdouble
                </span>
               </code>
              </p>
             </td>
             <td>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               16-bit floating point
               <a class="footnote-reference brackets" href="#id3" id="id1">
                1
               </a>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.float16
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.half
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.HalfTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               16-bit floating point
               <a class="footnote-reference brackets" href="#id4" id="id2">
                2
               </a>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.bfloat16
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.BFloat16Tensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               8-bit integer (unsigned)
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.uint8
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.ByteTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               8-bit integer (signed)
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.int8
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.CharTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               16-bit integer (signed)
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.int16
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.short
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.ShortTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               32-bit integer (signed)
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.int32
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.int
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.IntTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               64-bit integer (signed)
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.int64
                </span>
               </code>
               or
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.long
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.LongTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               Boolean
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.bool
                </span>
               </code>
              </p>
             </td>
             <td>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 torch.*.BoolTensor
                </span>
               </code>
              </p>
             </td>
            </tr>
           </tbody>
          </table>
          <dl class="footnote brackets">
           <dt class="label" id="id3">
            <span class="brackets">
             <a class="fn-backref" href="#id1">
              1
             </a>
            </span>
           </dt>
           <dd>
            <p>
             Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10
significand bits. Useful when precision is important.
            </p>
           </dd>
           <dt class="label" id="id4">
            <span class="brackets">
             <a class="fn-backref" href="#id2">
              2
             </a>
            </span>
           </dt>
           <dd>
            <p>
             Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7
significand bits. Useful when range is important, since it has the same
number of exponent bits as
             <code class="docutils literal notranslate">
              <span class="pre">
               float32
              </span>
             </code>
            </p>
           </dd>
          </dl>
          <p>
           To find out if a
           <a class="reference internal" href="#torch.dtype" title="torch.dtype">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.dtype
             </span>
            </code>
           </a>
           is a floating point data type, the property
           <a class="reference internal" href="generated/torch.is_floating_point.html#torch.is_floating_point" title="torch.is_floating_point">
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              is_floating_point
             </span>
            </code>
           </a>
           can be used, which returns
           <code class="docutils literal notranslate">
            <span class="pre">
             True
            </span>
           </code>
           if the data type is a floating point data type.
          </p>
          <p>
           To find out if a
           <a class="reference internal" href="#torch.dtype" title="torch.dtype">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.dtype
             </span>
            </code>
           </a>
           is a complex data type, the property
           <a class="reference internal" href="generated/torch.is_complex.html#torch.is_complex" title="torch.is_complex">
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              is_complex
             </span>
            </code>
           </a>
           can be used, which returns
           <code class="docutils literal notranslate">
            <span class="pre">
             True
            </span>
           </code>
           if the data type is a complex data type.
          </p>
          <p id="type-promotion-doc">
           When the dtypes of inputs to an arithmetic operation (
           <cite>
            add
           </cite>
           ,
           <cite>
            sub
           </cite>
           ,
           <cite>
            div
           </cite>
           ,
           <cite>
            mul
           </cite>
           ) differ, we promote
by finding the minimum dtype that satisfies the following rules:
          </p>
          <ul class="simple">
           <li>
            <p>
             If the type of a scalar operand is of a higher category than tensor operands
(where complex &gt; floating &gt; integral &gt; boolean), we promote to a type with sufficient size to hold
all scalar operands of that category.
            </p>
           </li>
           <li>
            <p>
             If a zero-dimension tensor operand has a higher category than dimensioned operands,
we promote to a type with sufficient size and category to hold all zero-dim tensor operands of
that category.
            </p>
           </li>
           <li>
            <p>
             If there are no higher-category zero-dim operands, we promote to a type with sufficient size
and category to hold all dimensioned operands.
            </p>
           </li>
          </ul>
          <p>
           A floating point scalar operand has dtype
           <cite>
            torch.get_default_dtype()
           </cite>
           and an integral
non-boolean scalar operand has dtype
           <cite>
            torch.int64
           </cite>
           . Unlike numpy, we do not inspect
values when determining the minimum
           <cite>
            dtypes
           </cite>
           of an operand.  Quantized and complex types
are not yet supported.
          </p>
          <p>
           Promotion Examples:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">float_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">double_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">complex_float_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">complex_double_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">int_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">long_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uint_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">double_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bool_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="go"># zero-dim tensors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">long_zerodim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">int_zerodim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int64</span>
<span class="go"># 5 is an int64, but does not have higher category than int_tensor so is not considered.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">int_tensor</span> <span class="o">+</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">int_tensor</span> <span class="o">+</span> <span class="n">long_zerodim</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">long_tensor</span> <span class="o">+</span> <span class="n">int_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">bool_tensor</span> <span class="o">+</span> <span class="n">long_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">bool_tensor</span> <span class="o">+</span> <span class="n">uint_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.uint8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">float_tensor</span> <span class="o">+</span> <span class="n">double_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">complex_float_tensor</span> <span class="o">+</span> <span class="n">complex_double_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.complex128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">bool_tensor</span> <span class="o">+</span> <span class="n">int_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="go"># Since long is a different kind than float, result dtype only needs to be large enough</span>
<span class="go"># to hold the float.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">long_tensor</span><span class="p">,</span> <span class="n">float_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
</pre>
           </div>
          </div>
          <dl class="simple">
           <dt>
            When the output tensor of an arithmetic operation is specified, we allow casting to its
            <cite>
             dtype
            </cite>
            except that:
           </dt>
           <dd>
            <ul class="simple">
             <li>
              <p>
               An integral output tensor cannot accept a floating point tensor.
              </p>
             </li>
             <li>
              <p>
               A boolean output tensor cannot accept a non-boolean tensor.
              </p>
             </li>
             <li>
              <p>
               A non-complex output tensor cannot accept a complex tensor
              </p>
             </li>
            </ul>
           </dd>
          </dl>
          <p>
           Casting Examples:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="c1"># allowed:</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">float_tensor</span> <span class="o">*=</span> <span class="n">float_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">float_tensor</span> <span class="o">*=</span> <span class="n">int_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">float_tensor</span> <span class="o">*=</span> <span class="n">uint_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">float_tensor</span> <span class="o">*=</span> <span class="n">bool_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">float_tensor</span> <span class="o">*=</span> <span class="n">double_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">int_tensor</span> <span class="o">*=</span> <span class="n">long_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">int_tensor</span> <span class="o">*=</span> <span class="n">uint_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">uint_tensor</span> <span class="o">*=</span> <span class="n">int_tensor</span>

<span class="c1"># disallowed (RuntimeError: result type can't be cast to the desired output type):</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">int_tensor</span> <span class="o">*=</span> <span class="n">float_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bool_tensor</span> <span class="o">*=</span> <span class="n">int_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bool_tensor</span> <span class="o">*=</span> <span class="n">uint_tensor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">float_tensor</span> <span class="o">*=</span> <span class="n">complex_float_tensor</span>
</pre>
           </div>
          </div>
         </div>
         <div class="section" id="torch-device">
          <span id="device-doc">
          </span>
          <h2>
           torch.device
           <a class="headerlink" href="#torch-device" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py class">
           <dt id="torch.device">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              device
             </span>
            </code>
            <a class="headerlink" href="#torch.device" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
           </dd>
          </dl>
          <p>
           A
           <a class="reference internal" href="#torch.device" title="torch.device">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.device
             </span>
            </code>
           </a>
           is an object representing the device on which a
           <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
           </a>
           is
or will be allocated.
          </p>
          <p>
           The
           <a class="reference internal" href="#torch.device" title="torch.device">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.device
             </span>
            </code>
           </a>
           contains a device type (
           <code class="docutils literal notranslate">
            <span class="pre">
             'cpu'
            </span>
           </code>
           or
           <code class="docutils literal notranslate">
            <span class="pre">
             'cuda'
            </span>
           </code>
           ) and optional device
ordinal for the device type. If the device ordinal is not present, this object will always represent
the current device for the device type, even after
           <a class="reference internal" href="generated/torch.cuda.set_device.html#torch.cuda.set_device" title="torch.cuda.set_device">
            <code class="xref py py-func docutils literal notranslate">
             <span class="pre">
              torch.cuda.set_device()
             </span>
            </code>
           </a>
           is called; e.g.,
a
           <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
           </a>
           constructed with device
           <code class="docutils literal notranslate">
            <span class="pre">
             'cuda'
            </span>
           </code>
           is equivalent to
           <code class="docutils literal notranslate">
            <span class="pre">
             'cuda:X'
            </span>
           </code>
           where X is
the result of
           <a class="reference internal" href="generated/torch.cuda.current_device.html#torch.cuda.current_device" title="torch.cuda.current_device">
            <code class="xref py py-func docutils literal notranslate">
             <span class="pre">
              torch.cuda.current_device()
             </span>
            </code>
           </a>
           .
          </p>
          <p>
           A
           <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
           </a>
           ’s device can be accessed via the
           <a class="reference internal" href="generated/torch.Tensor.device.html#torch.Tensor.device" title="torch.Tensor.device">
            <code class="xref py py-attr docutils literal notranslate">
             <span class="pre">
              Tensor.device
             </span>
            </code>
           </a>
           property.
          </p>
          <p>
           A
           <a class="reference internal" href="#torch.device" title="torch.device">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.device
             </span>
            </code>
           </a>
           can be constructed via a string or via a string and device ordinal
          </p>
          <p>
           Via a string:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda:0'</span><span class="p">)</span>
<span class="go">device(type='cuda', index=0)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cpu'</span><span class="p">)</span>
<span class="go">device(type='cpu')</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">)</span>  <span class="c1"># current cuda device</span>
<span class="go">device(type='cuda')</span>
</pre>
           </div>
          </div>
          <p>
           Via a string and device ordinal:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">device(type='cuda', index=0)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">device(type='cpu', index=0)</span>
</pre>
           </div>
          </div>
          <div class="admonition note">
           <p class="admonition-title">
            Note
           </p>
           <p>
            The
            <a class="reference internal" href="#torch.device" title="torch.device">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               torch.device
              </span>
             </code>
            </a>
            argument in functions can generally be substituted with a string.
This allows for fast prototyping of code.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example of a function that takes in a torch.device</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda1</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># You can substitute the torch.device with a string</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda:1'</span><span class="p">)</span>
</pre>
            </div>
           </div>
          </div>
          <div class="admonition note">
           <p class="admonition-title">
            Note
           </p>
           <p>
            For legacy reasons, a device can be constructed via a single device ordinal, which is treated
as a cuda device.  This matches
            <a class="reference internal" href="generated/torch.Tensor.get_device.html#torch.Tensor.get_device" title="torch.Tensor.get_device">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               Tensor.get_device()
              </span>
             </code>
            </a>
            , which returns an ordinal for cuda
tensors and is not supported for cpu tensors.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">device(type='cuda', index=1)</span>
</pre>
            </div>
           </div>
          </div>
          <div class="admonition note">
           <p class="admonition-title">
            Note
           </p>
           <p>
            Methods which take a device will generally accept a (properly formatted) string
or (legacy) integer device ordinal, i.e. the following are all equivalent:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda:1'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># legacy</span>
</pre>
            </div>
           </div>
          </div>
         </div>
         <div class="section" id="torch-layout">
          <span id="layout-doc">
          </span>
          <h2>
           torch.layout
           <a class="headerlink" href="#torch-layout" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py class">
           <dt id="torch.layout">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              layout
             </span>
            </code>
            <a class="headerlink" href="#torch.layout" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
           </dd>
          </dl>
          <div class="admonition warning">
           <p class="admonition-title">
            Warning
           </p>
           <p>
            The
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.layout
             </span>
            </code>
            class is in beta and subject to change.
           </p>
          </div>
          <p>
           A
           <a class="reference internal" href="#torch.layout" title="torch.layout">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.layout
             </span>
            </code>
           </a>
           is an object that represents the memory layout of a
           <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
           </a>
           . Currently, we support
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.strided
            </span>
           </code>
           (dense Tensors)
and have beta support for
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.sparse_coo
            </span>
           </code>
           (sparse COO Tensors).
          </p>
          <p>
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.strided
            </span>
           </code>
           represents dense Tensors and is the memory layout that
is most commonly used. Each strided tensor has an associated
           <code class="xref py py-class docutils literal notranslate">
            <span class="pre">
             torch.Storage
            </span>
           </code>
           , which holds its data. These tensors provide
multi-dimensional,
           <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">
            strided
           </a>
           view of a storage. Strides are a list of integers: the k-th stride
represents the jump in the memory necessary to go from one element to the
next one in the k-th dimension of the Tensor. This concept makes it possible
to perform many tensor operations efficiently.
          </p>
          <p>
           Example:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(1, 5)</span>
</pre>
           </div>
          </div>
          <p>
           For more information on
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.sparse_coo
            </span>
           </code>
           tensors, see
           <a class="reference internal" href="sparse.html#sparse-docs">
            <span class="std std-ref">
             torch.sparse
            </span>
           </a>
           .
          </p>
         </div>
         <div class="section" id="torch-memory-format">
          <h2>
           torch.memory_format
           <a class="headerlink" href="#torch-memory-format" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py class">
           <dt id="torch.memory_format">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              memory_format
             </span>
            </code>
            <a class="headerlink" href="#torch.memory_format" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
           </dd>
          </dl>
          <p>
           A
           <a class="reference internal" href="#torch.memory_format" title="torch.memory_format">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.memory_format
             </span>
            </code>
           </a>
           is an object representing the memory format on which a
           <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
           </a>
           is
or will be allocated.
          </p>
          <p>
           Possible values are:
          </p>
          <ul class="simple">
           <li>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.contiguous_format
              </span>
             </code>
             :
Tensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order.
            </p>
           </li>
           <li>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.channels_last
              </span>
             </code>
             :
Tensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in
             <code class="docutils literal notranslate">
              <span class="pre">
               strides[0]
              </span>
              <span class="pre">
               &gt;
              </span>
              <span class="pre">
               strides[2]
              </span>
              <span class="pre">
               &gt;
              </span>
              <span class="pre">
               strides[3]
              </span>
              <span class="pre">
               &gt;
              </span>
              <span class="pre">
               strides[1]
              </span>
              <span class="pre">
               ==
              </span>
              <span class="pre">
               1
              </span>
             </code>
             aka NHWC order.
            </p>
           </li>
           <li>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.preserve_format
              </span>
             </code>
             :
Used in functions like
             <cite>
              clone
             </cite>
             to preserve the memory format of the input tensor. If input tensor is
allocated in dense non-overlapping memory, the output tensor strides will be copied from the input.
Otherwise output strides will follow
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.contiguous_format
              </span>
             </code>
            </p>
           </li>
          </ul>
         </div>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>