<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.backends — PyTorch 1.11.0 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-backends">
         <h1>
          torch.backends
          <a class="headerlink" href="#torch-backends" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <p>
          <cite>
           torch.backends
          </cite>
          controls the behavior of various backends that PyTorch supports.
         </p>
         <p>
          These backends include:
         </p>
         <ul class="simple">
          <li>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.backends.cuda
             </span>
            </code>
           </p>
          </li>
          <li>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.backends.cudnn
             </span>
            </code>
           </p>
          </li>
          <li>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.backends.mkl
             </span>
            </code>
           </p>
          </li>
          <li>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.backends.mkldnn
             </span>
            </code>
           </p>
          </li>
          <li>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.backends.openmp
             </span>
            </code>
           </p>
          </li>
         </ul>
         <div class="section" id="torch-backends-cuda">
          <h2>
           torch.backends.cuda
           <a class="headerlink" href="#torch-backends-cuda" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py function">
           <dt id="torch.backends.cuda.is_built">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cuda.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              is_built
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/cuda.html#is_built">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.cuda.is_built" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns whether PyTorch is built with CUDA support.  Note that this
doesn’t necessarily mean CUDA is available; just that if this PyTorch
binary were run a machine with working CUDA drivers and devices, we
would be able to use it.
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cuda.matmul.allow_tf32">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cuda.matmul.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              allow_tf32
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cuda.matmul.allow_tf32" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A
             <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
             </a>
             that controls whether TensorFloat-32 tensor cores may be used in matrix
multiplications on Ampere or newer GPUs. See
             <a class="reference internal" href="notes/cuda.html#tf32-on-ampere">
              <span class="std std-ref">
               TensorFloat-32(TF32) on Ampere devices
              </span>
             </a>
             .
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cuda.matmul.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              allow_fp16_reduced_precision_reduction
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A
             <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
             </a>
             that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cuda.cufft_plan_cache">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cuda.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              cufft_plan_cache
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cuda.cufft_plan_cache" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               cufft_plan_cache
              </span>
             </code>
             caches the cuFFT plans
            </p>
            <dl class="py attribute">
             <dt id="torch.backends.cuda.size">
              <code class="sig-name descname">
               <span class="pre">
                size
               </span>
              </code>
              <a class="headerlink" href="#torch.backends.cuda.size" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               A readonly
               <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  int
                 </span>
                </code>
               </a>
               that shows the number of plans currently in the cuFFT plan cache.
              </p>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt id="max_size">
              <code class="sig-name descname">
               <span class="pre">
                max_size
               </span>
              </code>
              <a class="headerlink" href="#max_size" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               A
               <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  int
                 </span>
                </code>
               </a>
               that controls cache capacity of cuFFT plan.
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt id="clear">
              <code class="sig-name descname">
               <span class="pre">
                clear
               </span>
              </code>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="headerlink" href="#clear" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Clears the cuFFT plan cache.
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="torch.backends.cuda.preferred_linalg_library">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cuda.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              preferred_linalg_library
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               backend
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/cuda.html#preferred_linalg_library">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.cuda.preferred_linalg_library" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <div class="admonition warning">
             <p class="admonition-title">
              Warning
             </p>
             <p>
              This flag is experimental and subject to change.
             </p>
            </div>
            <p>
             When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries,
and if both are available it decides which to use with a heuristic.
This flag (a
             <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                str
               </span>
              </code>
             </a>
             ) allows overriding those heuristics.
            </p>
            <ul class="simple">
             <li>
              <p>
               If
               <cite>
                “cusolver”
               </cite>
               is set then cuSOLVER will be used wherever possible.
              </p>
             </li>
             <li>
              <p>
               If
               <cite>
                “magma”
               </cite>
               is set then MAGMA will be used wherever possible.
              </p>
             </li>
             <li>
              <p>
               If
               <cite>
                “default”
               </cite>
               (the default) is set then heuristics will be used to pick between
cuSOLVER and MAGMA if both are available.
              </p>
             </li>
             <li>
              <p>
               When no input is given, this function returns the currently preferred library.
              </p>
             </li>
            </ul>
            <p>
             Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect
for your application’s inputs.
            </p>
            <p>
             Currently supported linalg operators:
            </p>
            <ul class="simple">
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.inv.html#torch.linalg.inv" title="torch.linalg.inv">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.inv()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex" title="torch.linalg.inv_ex">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.inv_ex()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.cholesky.html#torch.linalg.cholesky" title="torch.linalg.cholesky">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.cholesky()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex" title="torch.linalg.cholesky_ex">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.cholesky_ex()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.cholesky_solve()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.cholesky_inverse()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.lu.html#torch.lu" title="torch.lu">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.lu()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.qr.html#torch.linalg.qr" title="torch.linalg.qr">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.qr()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.eigh.html#torch.linalg.eigh" title="torch.linalg.eigh">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.eigh()
                 </span>
                </code>
               </a>
              </p>
             </li>
             <li>
              <p>
               <a class="reference internal" href="generated/torch.linalg.svd.html#torch.linalg.svd" title="torch.linalg.svd">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.linalg.svd()
                 </span>
                </code>
               </a>
              </p>
             </li>
            </ul>
           </dd>
          </dl>
         </div>
         <div class="section" id="torch-backends-cudnn">
          <h2>
           torch.backends.cudnn
           <a class="headerlink" href="#torch-backends-cudnn" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py function">
           <dt id="torch.backends.cudnn.version">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cudnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              version
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/cudnn.html#version">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.cudnn.version" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns the version of cuDNN
            </p>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="torch.backends.cudnn.is_available">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cudnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              is_available
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/cudnn.html#is_available">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.cudnn.is_available" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns a bool indicating if CUDNN is currently available.
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cudnn.enabled">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cudnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              enabled
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cudnn.enabled" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A
             <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
             </a>
             that controls whether cuDNN is enabled.
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cudnn.allow_tf32">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cudnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              allow_tf32
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cudnn.allow_tf32" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A
             <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
             </a>
             that controls where TensorFloat-32 tensor cores may be used in cuDNN
convolutions on Ampere or newer GPUs. See
             <a class="reference internal" href="notes/cuda.html#tf32-on-ampere">
              <span class="std std-ref">
               TensorFloat-32(TF32) on Ampere devices
              </span>
             </a>
             .
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cudnn.deterministic">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cudnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              deterministic
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cudnn.deterministic" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A
             <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
             </a>
             that, if True, causes cuDNN to only use deterministic convolution algorithms.
See also
             <a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                torch.are_deterministic_algorithms_enabled()
               </span>
              </code>
             </a>
             and
             <a class="reference internal" href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                torch.use_deterministic_algorithms()
               </span>
              </code>
             </a>
             .
            </p>
           </dd>
          </dl>
          <dl class="py attribute">
           <dt id="torch.backends.cudnn.benchmark">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.cudnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              benchmark
             </span>
            </code>
            <a class="headerlink" href="#torch.backends.cudnn.benchmark" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A
             <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
             </a>
             that, if True, causes cuDNN to benchmark multiple convolution algorithms
and select the fastest.
            </p>
           </dd>
          </dl>
         </div>
         <div class="section" id="torch-backends-mkl">
          <h2>
           torch.backends.mkl
           <a class="headerlink" href="#torch-backends-mkl" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py function">
           <dt id="torch.backends.mkl.is_available">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.mkl.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              is_available
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/mkl.html#is_available">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.mkl.is_available" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns whether PyTorch is built with MKL support.
            </p>
           </dd>
          </dl>
         </div>
         <div class="section" id="torch-backends-mkldnn">
          <h2>
           torch.backends.mkldnn
           <a class="headerlink" href="#torch-backends-mkldnn" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py function">
           <dt id="torch.backends.mkldnn.is_available">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.mkldnn.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              is_available
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/mkldnn.html#is_available">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.mkldnn.is_available" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns whether PyTorch is built with MKL-DNN support.
            </p>
           </dd>
          </dl>
         </div>
         <div class="section" id="torch-backends-openmp">
          <h2>
           torch.backends.openmp
           <a class="headerlink" href="#torch-backends-openmp" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py function">
           <dt id="torch.backends.openmp.is_available">
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.backends.openmp.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              is_available
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/backends/openmp.html#is_available">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.backends.openmp.is_available" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns whether PyTorch is built with OpenMP support.
            </p>
           </dd>
          </dl>
         </div>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>