<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.optim — PyTorch 1.12 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="module-torch.optim">
         <span id="torch-optim">
         </span>
         <h1>
          torch.optim
          <a class="headerlink" href="#module-torch.optim" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <p>
          <a class="reference internal" href="#module-torch.optim" title="torch.optim">
           <code class="xref py py-mod docutils literal notranslate">
            <span class="pre">
             torch.optim
            </span>
           </code>
          </a>
          is a package implementing various optimization algorithms.
Most commonly used methods are already supported, and the interface is general
enough, so that more sophisticated ones can be also easily integrated in the
future.
         </p>
         <div class="section" id="how-to-use-an-optimizer">
          <h2>
           How to use an optimizer
           <a class="headerlink" href="#how-to-use-an-optimizer" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           To use
           <a class="reference internal" href="#module-torch.optim" title="torch.optim">
            <code class="xref py py-mod docutils literal notranslate">
             <span class="pre">
              torch.optim
             </span>
            </code>
           </a>
           you have to construct an optimizer object, that will hold
the current state and will update the parameters based on the computed gradients.
          </p>
          <div class="section" id="constructing-it">
           <h3>
            Constructing it
            <a class="headerlink" href="#constructing-it" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            To construct an
            <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               Optimizer
              </span>
             </code>
            </a>
            you have to give it an iterable containing the
parameters (all should be
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              Variable
             </span>
            </code>
            s) to optimize. Then,
you can specify optimizer-specific options such as the learning rate, weight decay, etc.
           </p>
           <p>
            Example:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
</pre>
            </div>
           </div>
          </div>
          <div class="section" id="per-parameter-options">
           <h3>
            Per-parameter options
            <a class="headerlink" href="#per-parameter-options" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               Optimizer
              </span>
             </code>
            </a>
            s also support specifying per-parameter options. To do this, instead
of passing an iterable of
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              Variable
             </span>
            </code>
            s, pass in an iterable of
            <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               dict
              </span>
             </code>
            </a>
            s. Each of them will define a separate parameter group, and should contain
a
            <code class="docutils literal notranslate">
             <span class="pre">
              params
             </span>
            </code>
            key, containing a list of parameters belonging to it. Other keys
should match the keyword arguments accepted by the optimizers, and will be used
as optimization options for this group.
           </p>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             You can still pass options as keyword arguments. They will be used as
defaults, in the groups that didn’t override them. This is useful when you
only want to vary a single option, while keeping all others consistent
between parameter groups.
            </p>
           </div>
           <p>
            For example, this is very useful when one wants to specify per-layer learning rates:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
                <span class="p">{</span><span class="s1">'params'</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
                <span class="p">{</span><span class="s1">'params'</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">'lr'</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
            <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            This means that
            <code class="docutils literal notranslate">
             <span class="pre">
              model.base
             </span>
            </code>
            ’s parameters will use the default learning rate of
            <code class="docutils literal notranslate">
             <span class="pre">
              1e-2
             </span>
            </code>
            ,
            <code class="docutils literal notranslate">
             <span class="pre">
              model.classifier
             </span>
            </code>
            ’s parameters will use a learning rate of
            <code class="docutils literal notranslate">
             <span class="pre">
              1e-3
             </span>
            </code>
            , and a momentum of
            <code class="docutils literal notranslate">
             <span class="pre">
              0.9
             </span>
            </code>
            will be used for all parameters.
           </p>
          </div>
          <div class="section" id="taking-an-optimization-step">
           <h3>
            Taking an optimization step
            <a class="headerlink" href="#taking-an-optimization-step" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            All optimizers implement a
            <a class="reference internal" href="generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step">
             <code class="xref py py-func docutils literal notranslate">
              <span class="pre">
               step()
              </span>
             </code>
            </a>
            method, that updates the
parameters. It can be used in two ways:
           </p>
           <div class="section" id="optimizer-step">
            <h4>
             <code class="docutils literal notranslate">
              <span class="pre">
               optimizer.step()
              </span>
             </code>
             <a class="headerlink" href="#optimizer-step" title="Permalink to this headline">
              ¶
             </a>
            </h4>
            <p>
             This is a simplified version supported by most optimizers. The function can be
called once the gradients are computed using e.g.
             <code class="xref py py-func docutils literal notranslate">
              <span class="pre">
               backward()
              </span>
             </code>
             .
            </p>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre>
             </div>
            </div>
           </div>
           <div class="section" id="optimizer-step-closure">
            <h4>
             <code class="docutils literal notranslate">
              <span class="pre">
               optimizer.step(closure)
              </span>
             </code>
             <a class="headerlink" href="#optimizer-step-closure" title="Permalink to this headline">
              ¶
             </a>
            </h4>
            <p>
             Some optimization algorithms such as Conjugate Gradient and LBFGS need to
reevaluate the function multiple times, so you have to pass in a closure that
allows them to recompute your model. The closure should clear the gradients,
compute the loss, and return it.
            </p>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</pre>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div class="section" id="base-class">
          <span id="optimizer-algorithms">
          </span>
          <h2>
           Base class
           <a class="headerlink" href="#base-class" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <dl class="py class">
           <dt id="torch.optim.Optimizer">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              torch.optim.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              Optimizer
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               defaults
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/optim/optimizer.html#Optimizer">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.optim.Optimizer" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Base class for all optimizers.
            </p>
            <div class="admonition warning">
             <p class="admonition-title">
              Warning
             </p>
             <p>
              Parameters need to be specified as collections that have a deterministic
ordering that is consistent between runs. Examples of objects that don’t
satisfy those properties are sets and iterators over values of dictionaries.
             </p>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 (
                 <em>
                  iterable
                 </em>
                 ) – an iterable of
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 s or
                 <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    dict
                   </span>
                  </code>
                 </a>
                 s. Specifies what Tensors should be optimized.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  defaults
                 </strong>
                 – (dict): a dict containing default values of optimization
options (used when a parameter group doesn’t specify them).
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </dd>
          </dl>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group" title="torch.optim.Optimizer.add_param_group">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Optimizer.add_param_group
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Add a param group to the
               <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  Optimizer
                 </span>
                </code>
               </a>
               s
               <cite>
                param_groups
               </cite>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict" title="torch.optim.Optimizer.load_state_dict">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Optimizer.load_state_dict
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Loads the optimizer state.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Optimizer.state_dict
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the state of the optimizer as a
               <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  dict
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Optimizer.step
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Performs a single optimization step (parameter update).
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="torch.optim.Optimizer.zero_grad">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Optimizer.zero_grad
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the gradients of all optimized
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  torch.Tensor
                 </span>
                </code>
               </a>
               s to zero.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="algorithms">
          <h2>
           Algorithms
           <a class="headerlink" href="#algorithms" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.Adadelta">
               </p>
               <a class="reference internal" href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta" title="torch.optim.Adadelta">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Adadelta
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements Adadelta algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.optim.Adagrad">
               </p>
               <a class="reference internal" href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad" title="torch.optim.Adagrad">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Adagrad
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements Adagrad algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.Adam">
               </p>
               <a class="reference internal" href="generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Adam
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements Adam algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.optim.AdamW">
               </p>
               <a class="reference internal" href="generated/torch.optim.AdamW.html#torch.optim.AdamW" title="torch.optim.AdamW">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  AdamW
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements AdamW algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.SparseAdam">
               </p>
               <a class="reference internal" href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam" title="torch.optim.SparseAdam">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  SparseAdam
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements lazy version of Adam algorithm suitable for sparse tensors.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.optim.Adamax">
               </p>
               <a class="reference internal" href="generated/torch.optim.Adamax.html#torch.optim.Adamax" title="torch.optim.Adamax">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Adamax
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements Adamax algorithm (a variant of Adam based on infinity norm).
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.ASGD">
               </p>
               <a class="reference internal" href="generated/torch.optim.ASGD.html#torch.optim.ASGD" title="torch.optim.ASGD">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ASGD
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements Averaged Stochastic Gradient Descent.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.optim.LBFGS">
               </p>
               <a class="reference internal" href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS" title="torch.optim.LBFGS">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LBFGS
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements L-BFGS algorithm, heavily inspired by
               <a class="reference external" href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">
                minFunc
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.NAdam">
               </p>
               <a class="reference internal" href="generated/torch.optim.NAdam.html#torch.optim.NAdam" title="torch.optim.NAdam">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  NAdam
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements NAdam algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.optim.RAdam">
               </p>
               <a class="reference internal" href="generated/torch.optim.RAdam.html#torch.optim.RAdam" title="torch.optim.RAdam">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  RAdam
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements RAdam algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.RMSprop">
               </p>
               <a class="reference internal" href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop" title="torch.optim.RMSprop">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  RMSprop
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements RMSprop algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.optim.Rprop">
               </p>
               <a class="reference internal" href="generated/torch.optim.Rprop.html#torch.optim.Rprop" title="torch.optim.Rprop">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Rprop
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements the resilient backpropagation algorithm.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.optim.SGD">
               </p>
               <a class="reference internal" href="generated/torch.optim.SGD.html#torch.optim.SGD" title="torch.optim.SGD">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  SGD
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Implements stochastic gradient descent (optionally with momentum).
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="how-to-adjust-learning-rate">
          <h2>
           How to adjust learning rate
           <a class="headerlink" href="#how-to-adjust-learning-rate" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           <code class="xref py py-mod docutils literal notranslate">
            <span class="pre">
             torch.optim.lr_scheduler
            </span>
           </code>
           provides several methods to adjust the learning
rate based on the number of epochs.
           <a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="torch.optim.lr_scheduler.ReduceLROnPlateau">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.optim.lr_scheduler.ReduceLROnPlateau
             </span>
            </code>
           </a>
           allows dynamic learning rate reducing based on some validation measurements.
          </p>
          <p>
           Learning rate scheduling should be applied after optimizer’s update; e.g., you
should write your code this way:
          </p>
          <p>
           Example:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="p">[</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))]</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre>
           </div>
          </div>
          <p>
           Most learning rate schedulers can be called back-to-back (also referred to as
chaining schedulers). The result is that each scheduler is applied one after the
other on the learning rate obtained by the one preceding it.
          </p>
          <p>
           Example:
          </p>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="p">[</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))]</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">scheduler1</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler2</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">80</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler2</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre>
           </div>
          </div>
          <p>
           In many places in the documentation, we will use the following template to refer to schedulers
algorithms.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre>
           </div>
          </div>
          <div class="admonition warning">
           <p class="admonition-title">
            Warning
           </p>
           <p>
            Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before
the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use
the learning rate scheduler (calling
            <code class="docutils literal notranslate">
             <span class="pre">
              scheduler.step()
             </span>
            </code>
            ) before the optimizer’s update
(calling
            <code class="docutils literal notranslate">
             <span class="pre">
              optimizer.step()
             </span>
            </code>
            ), this will skip the first value of the learning rate schedule.
If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check
if you are calling
            <code class="docutils literal notranslate">
             <span class="pre">
              scheduler.step()
             </span>
            </code>
            at the wrong time.
           </p>
          </div>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR" title="torch.optim.lr_scheduler.LambdaLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.LambdaLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the learning rate of each parameter group to the initial lr times a given function.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR" title="torch.optim.lr_scheduler.MultiplicativeLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.MultiplicativeLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Multiply the learning rate of each parameter group by the factor given in the specified function.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR" title="torch.optim.lr_scheduler.StepLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.StepLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Decays the learning rate of each parameter group by gamma every step_size epochs.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR" title="torch.optim.lr_scheduler.MultiStepLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.MultiStepLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR" title="torch.optim.lr_scheduler.ConstantLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.ConstantLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Decays the learning rate of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR" title="torch.optim.lr_scheduler.LinearLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.LinearLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR" title="torch.optim.lr_scheduler.ExponentialLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.ExponentialLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Decays the learning rate of each parameter group by gamma every epoch.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.CosineAnnealingLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the learning rate of each parameter group using a cosine annealing schedule, where
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <msub>
                      <mi>
                       η
                      </mi>
                      <mrow>
                       <mi>
                        m
                       </mi>
                       <mi>
                        a
                       </mi>
                       <mi>
                        x
                       </mi>
                      </mrow>
                     </msub>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     \eta_{max}
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.625em;vertical-align:-0.1944em;">
                   </span>
                   <span class="mord">
                    <span class="mord mathnormal" style="margin-right:0.03588em;">
                     η
                    </span>
                    <span class="msupsub">
                     <span class="vlist-t vlist-t2">
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.1514em;">
                        <span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;">
                         <span class="pstrut" style="height:2.7em;">
                         </span>
                         <span class="sizing reset-size6 size3 mtight">
                          <span class="mord mtight">
                           <span class="mord mathnormal mtight">
                            ma
                           </span>
                           <span class="mord mathnormal mtight">
                            x
                           </span>
                          </span>
                         </span>
                        </span>
                       </span>
                       <span class="vlist-s">
                        ​
                       </span>
                      </span>
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.15em;">
                        <span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               is set to the initial lr and
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <msub>
                      <mi>
                       T
                      </mi>
                      <mrow>
                       <mi>
                        c
                       </mi>
                       <mi>
                        u
                       </mi>
                       <mi>
                        r
                       </mi>
                      </mrow>
                     </msub>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     T_{cur}
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.8333em;vertical-align:-0.15em;">
                   </span>
                   <span class="mord">
                    <span class="mord mathnormal" style="margin-right:0.13889em;">
                     T
                    </span>
                    <span class="msupsub">
                     <span class="vlist-t vlist-t2">
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.1514em;">
                        <span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;">
                         <span class="pstrut" style="height:2.7em;">
                         </span>
                         <span class="sizing reset-size6 size3 mtight">
                          <span class="mord mtight">
                           <span class="mord mathnormal mtight">
                            c
                           </span>
                           <span class="mord mathnormal mtight">
                            u
                           </span>
                           <span class="mord mathnormal mtight" style="margin-right:0.02778em;">
                            r
                           </span>
                          </span>
                         </span>
                        </span>
                       </span>
                       <span class="vlist-s">
                        ​
                       </span>
                      </span>
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.15em;">
                        <span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               is the number of epochs since the last restart in SGDR:
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler" title="torch.optim.lr_scheduler.ChainedScheduler">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.ChainedScheduler
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Chains list of learning rate schedulers.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR" title="torch.optim.lr_scheduler.SequentialLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.SequentialLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Receives the list of schedulers that is expected to be called sequentially during optimization process and milestone points that provides exact intervals to reflect which scheduler is supposed to be called at a given epoch.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="torch.optim.lr_scheduler.ReduceLROnPlateau">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.ReduceLROnPlateau
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Reduce learning rate when a metric has stopped improving.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR" title="torch.optim.lr_scheduler.CyclicLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.CyclicLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR" title="torch.optim.lr_scheduler.OneCycleLR">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.OneCycleLR
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the learning rate of each parameter group according to the 1cycle learning rate policy.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts" title="torch.optim.lr_scheduler.CosineAnnealingWarmRestarts">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lr_scheduler.CosineAnnealingWarmRestarts
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the learning rate of each parameter group using a cosine annealing schedule, where
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <msub>
                      <mi>
                       η
                      </mi>
                      <mrow>
                       <mi>
                        m
                       </mi>
                       <mi>
                        a
                       </mi>
                       <mi>
                        x
                       </mi>
                      </mrow>
                     </msub>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     \eta_{max}
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.625em;vertical-align:-0.1944em;">
                   </span>
                   <span class="mord">
                    <span class="mord mathnormal" style="margin-right:0.03588em;">
                     η
                    </span>
                    <span class="msupsub">
                     <span class="vlist-t vlist-t2">
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.1514em;">
                        <span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;">
                         <span class="pstrut" style="height:2.7em;">
                         </span>
                         <span class="sizing reset-size6 size3 mtight">
                          <span class="mord mtight">
                           <span class="mord mathnormal mtight">
                            ma
                           </span>
                           <span class="mord mathnormal mtight">
                            x
                           </span>
                          </span>
                         </span>
                        </span>
                       </span>
                       <span class="vlist-s">
                        ​
                       </span>
                      </span>
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.15em;">
                        <span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               is set to the initial lr,
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <msub>
                      <mi>
                       T
                      </mi>
                      <mrow>
                       <mi>
                        c
                       </mi>
                       <mi>
                        u
                       </mi>
                       <mi>
                        r
                       </mi>
                      </mrow>
                     </msub>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     T_{cur}
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.8333em;vertical-align:-0.15em;">
                   </span>
                   <span class="mord">
                    <span class="mord mathnormal" style="margin-right:0.13889em;">
                     T
                    </span>
                    <span class="msupsub">
                     <span class="vlist-t vlist-t2">
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.1514em;">
                        <span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;">
                         <span class="pstrut" style="height:2.7em;">
                         </span>
                         <span class="sizing reset-size6 size3 mtight">
                          <span class="mord mtight">
                           <span class="mord mathnormal mtight">
                            c
                           </span>
                           <span class="mord mathnormal mtight">
                            u
                           </span>
                           <span class="mord mathnormal mtight" style="margin-right:0.02778em;">
                            r
                           </span>
                          </span>
                         </span>
                        </span>
                       </span>
                       <span class="vlist-s">
                        ​
                       </span>
                      </span>
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.15em;">
                        <span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               is the number of epochs since the last restart and
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <msub>
                      <mi>
                       T
                      </mi>
                      <mi>
                       i
                      </mi>
                     </msub>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     T_{i}
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.8333em;vertical-align:-0.15em;">
                   </span>
                   <span class="mord">
                    <span class="mord mathnormal" style="margin-right:0.13889em;">
                     T
                    </span>
                    <span class="msupsub">
                     <span class="vlist-t vlist-t2">
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.3117em;">
                        <span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;">
                         <span class="pstrut" style="height:2.7em;">
                         </span>
                         <span class="sizing reset-size6 size3 mtight">
                          <span class="mord mtight">
                           <span class="mord mathnormal mtight">
                            i
                           </span>
                          </span>
                         </span>
                        </span>
                       </span>
                       <span class="vlist-s">
                        ​
                       </span>
                      </span>
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.15em;">
                        <span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               is the number of epochs between two warm restarts in SGDR:
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="stochastic-weight-averaging">
          <h2>
           Stochastic Weight Averaging
           <a class="headerlink" href="#stochastic-weight-averaging" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           <code class="xref py py-mod docutils literal notranslate">
            <span class="pre">
             torch.optim.swa_utils
            </span>
           </code>
           implements Stochastic Weight Averaging (SWA). In particular,
           <code class="xref py py-class docutils literal notranslate">
            <span class="pre">
             torch.optim.swa_utils.AveragedModel
            </span>
           </code>
           class implements SWA models,
           <code class="xref py py-class docutils literal notranslate">
            <span class="pre">
             torch.optim.swa_utils.SWALR
            </span>
           </code>
           implements the SWA learning rate scheduler and
           <code class="xref py py-func docutils literal notranslate">
            <span class="pre">
             torch.optim.swa_utils.update_bn()
            </span>
           </code>
           is a utility function used to update SWA batch
normalization statistics at the end of training.
          </p>
          <p>
           SWA has been proposed in
           <a class="reference external" href="https://arxiv.org/abs/1803.05407">
            Averaging Weights Leads to Wider Optima and Better Generalization
           </a>
           .
          </p>
          <div class="section" id="constructing-averaged-models">
           <h3>
            Constructing averaged models
            <a class="headerlink" href="#constructing-averaged-models" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            <cite>
             AveragedModel
            </cite>
            class serves to compute the weights of the SWA model. You can create an
averaged model by running:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">swa_model</span> <span class="o">=</span> <span class="n">AveragedModel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            Here the model
            <code class="docutils literal notranslate">
             <span class="pre">
              model
             </span>
            </code>
            can be an arbitrary
            <a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               torch.nn.Module
              </span>
             </code>
            </a>
            object.
            <code class="docutils literal notranslate">
             <span class="pre">
              swa_model
             </span>
            </code>
            will keep track of the running averages of the parameters of the
            <code class="docutils literal notranslate">
             <span class="pre">
              model
             </span>
            </code>
            . To update these
averages, you can use the
            <code class="xref py py-func docutils literal notranslate">
             <span class="pre">
              update_parameters()
             </span>
            </code>
            function:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">swa_model</span><span class="o">.</span><span class="n">update_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre>
            </div>
           </div>
          </div>
          <div class="section" id="swa-learning-rate-schedules">
           <h3>
            SWA learning rate schedules
            <a class="headerlink" href="#swa-learning-rate-schedules" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            Typically, in SWA the learning rate is set to a high constant value.
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              SWALR
             </span>
            </code>
            is a
learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it
constant. For example, the following code creates a scheduler that linearly anneals the
learning rate from its initial value to 0.05 in 5 epochs within each parameter group:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">swa_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">swa_utils</span><span class="o">.</span><span class="n">SWALR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> \
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">anneal_strategy</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">,</span> <span class="n">anneal_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">swa_lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            You can also use cosine annealing to a fixed value instead of linear annealing by setting
            <code class="docutils literal notranslate">
             <span class="pre">
              anneal_strategy="cos"
             </span>
            </code>
            .
           </p>
          </div>
          <div class="section" id="taking-care-of-batch-normalization">
           <h3>
            Taking care of batch normalization
            <a class="headerlink" href="#taking-care-of-batch-normalization" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            <code class="xref py py-func docutils literal notranslate">
             <span class="pre">
              update_bn()
             </span>
            </code>
            is a utility function that allows to compute the batchnorm statistics for the SWA model
on a given dataloader
            <code class="docutils literal notranslate">
             <span class="pre">
              loader
             </span>
            </code>
            at the end of training:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">swa_utils</span><span class="o">.</span><span class="n">update_bn</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">swa_model</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            <code class="xref py py-func docutils literal notranslate">
             <span class="pre">
              update_bn()
             </span>
            </code>
            applies the
            <code class="docutils literal notranslate">
             <span class="pre">
              swa_model
             </span>
            </code>
            to every element in the dataloader and computes the activation
statistics for each batch normalization layer in the model.
           </p>
           <div class="admonition warning">
            <p class="admonition-title">
             Warning
            </p>
            <p>
             <code class="xref py py-func docutils literal notranslate">
              <span class="pre">
               update_bn()
              </span>
             </code>
             assumes that each batch in the dataloader
             <code class="docutils literal notranslate">
              <span class="pre">
               loader
              </span>
             </code>
             is either a tensors or a list of
tensors where the first element is the tensor that the network
             <code class="docutils literal notranslate">
              <span class="pre">
               swa_model
              </span>
             </code>
             should be applied to.
If your dataloader has a different structure, you can update the batch normalization statistics of the
             <code class="docutils literal notranslate">
              <span class="pre">
               swa_model
              </span>
             </code>
             by doing a forward pass with the
             <code class="docutils literal notranslate">
              <span class="pre">
               swa_model
              </span>
             </code>
             on each element of the dataset.
            </p>
           </div>
          </div>
          <div class="section" id="custom-averaging-strategies">
           <h3>
            Custom averaging strategies
            <a class="headerlink" href="#custom-averaging-strategies" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            By default,
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              torch.optim.swa_utils.AveragedModel
             </span>
            </code>
            computes a running equal average of
the parameters that you provide, but you can also use custom averaging functions with the
            <code class="docutils literal notranslate">
             <span class="pre">
              avg_fn
             </span>
            </code>
            parameter. In the following example
            <code class="docutils literal notranslate">
             <span class="pre">
              ema_model
             </span>
            </code>
            computes an exponential moving average.
           </p>
           <p>
            Example:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ema_avg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">averaged_model_parameter</span><span class="p">,</span> <span class="n">model_parameter</span><span class="p">,</span> <span class="n">num_averaged</span><span class="p">:</span>\
<span class="gp">&gt;&gt;&gt; </span>        <span class="mf">0.1</span> <span class="o">*</span> <span class="n">averaged_model_parameter</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">model_parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ema_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">swa_utils</span><span class="o">.</span><span class="n">AveragedModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avg_fn</span><span class="o">=</span><span class="n">ema_avg</span><span class="p">)</span>
</pre>
            </div>
           </div>
          </div>
          <div class="section" id="putting-it-all-together">
           <h3>
            Putting it all together
            <a class="headerlink" href="#putting-it-all-together" title="Permalink to this headline">
             ¶
            </a>
           </h3>
           <p>
            In the example below,
            <code class="docutils literal notranslate">
             <span class="pre">
              swa_model
             </span>
            </code>
            is the SWA model that accumulates the averages of the weights.
We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule
and start to collect SWA averages of the parameters at epoch 160:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">swa_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">swa_utils</span><span class="o">.</span><span class="n">AveragedModel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">swa_start</span> <span class="o">=</span> <span class="mi">160</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">swa_scheduler</span> <span class="o">=</span> <span class="n">SWALR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">swa_lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>      <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>          <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>          <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>      <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="n">swa_start</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>          <span class="n">swa_model</span><span class="o">.</span><span class="n">update_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>          <span class="n">swa_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>      <span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>          <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update bn statistics for the swa_model at the end</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">swa_utils</span><span class="o">.</span><span class="n">update_bn</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">swa_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use swa_model to make predictions on test data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">preds</span> <span class="o">=</span> <span class="n">swa_model</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</pre>
            </div>
           </div>
          </div>
         </div>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>