<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Generic Join Context Manager — PyTorch 1.12 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="generic-join-context-manager">
         <h1>
          Generic Join Context Manager
          <a class="headerlink" href="#generic-join-context-manager" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <p>
          The generic join context manager facilitates distributed training on uneven
inputs. This page outlines the API of the relevant classes:
          <code class="xref py py-class docutils literal notranslate">
           <span class="pre">
            Join
           </span>
          </code>
          ,
          <code class="xref py py-class docutils literal notranslate">
           <span class="pre">
            Joinable
           </span>
          </code>
          , and
          <code class="xref py py-class docutils literal notranslate">
           <span class="pre">
            JoinHook
           </span>
          </code>
          . For a tutorial, see
          <a class="reference external" href="https://pytorch.org/tutorials/advanced/generic_join.html">
           Distributed Training with Uneven Inputs Using the Join Context Manager
          </a>
          .
         </p>
         <dl class="py class">
          <dt id="torch.distributed.algorithms.Join">
           <em class="property">
            <span class="pre">
             class
            </span>
           </em>
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.distributed.algorithms.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             Join
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              joinables
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              enable
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              True
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              throw_on_early_termination
             </span>
            </span>
            <span class="o">
             <span class="pre">
              =
             </span>
            </span>
            <span class="default_value">
             <span class="pre">
              False
             </span>
            </span>
           </em>
           ,
           <em class="sig-param">
            <span class="o">
             <span class="pre">
              **
             </span>
            </span>
            <span class="n">
             <span class="pre">
              kwargs
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#Join">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.distributed.algorithms.Join" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            This class defines the generic join context manager, which allows custom
hooks to be called after a process joins. These hooks should shadow the
collective communications of non-joined processes to prevent hanging and
erroring and to ensure algorithmic correctness. Refer to
            <a class="reference internal" href="#torch.distributed.algorithms.JoinHook" title="torch.distributed.algorithms.JoinHook">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               JoinHook
              </span>
             </code>
            </a>
            for details about the hook definition.
           </p>
           <div class="admonition warning">
            <p class="admonition-title">
             Warning
            </p>
            <p>
             The context manager requires each participating
             <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                Joinable
               </span>
              </code>
             </a>
             to
call the method
             <a class="reference internal" href="#torch.distributed.algorithms.Join.notify_join_context" title="torch.distributed.algorithms.Join.notify_join_context">
              <code class="xref py py-meth docutils literal notranslate">
               <span class="pre">
                notify_join_context()
               </span>
              </code>
             </a>
             before its own per-
iteration collective communications to ensure correctness.
            </p>
           </div>
           <div class="admonition warning">
            <p class="admonition-title">
             Warning
            </p>
            <p>
             The context manager requires that all
             <code class="docutils literal notranslate">
              <span class="pre">
               process_group
              </span>
             </code>
             attributes in
the
             <a class="reference internal" href="#torch.distributed.algorithms.JoinHook" title="torch.distributed.algorithms.JoinHook">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                JoinHook
               </span>
              </code>
             </a>
             objects are the same. If there are multiple
             <a class="reference internal" href="#torch.distributed.algorithms.JoinHook" title="torch.distributed.algorithms.JoinHook">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                JoinHook
               </span>
              </code>
             </a>
             objects, then the
             <code class="docutils literal notranslate">
              <span class="pre">
               device
              </span>
             </code>
             of the first is used.
The process group and device information is used for checking for non-
joined processes and for notifying processes to throw an exception if
             <code class="docutils literal notranslate">
              <span class="pre">
               throw_on_early_termination
              </span>
             </code>
             is enabled, both of which using an all-
reduce.
            </p>
           </div>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <ul class="simple">
              <li>
               <p>
                <strong>
                 joinables
                </strong>
                (
                <em>
                 List
                </em>
                <em>
                 [
                </em>
                <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
                 <em>
                  Joinable
                 </em>
                </a>
                <em>
                 ]
                </em>
                ) – a list of the participating
                <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   Joinable
                  </span>
                 </code>
                </a>
                s; their hooks are iterated over in the given
order.
               </p>
              </li>
              <li>
               <p>
                <strong>
                 enable
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                ) – a flag enabling uneven input detection; setting to
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                disables the context manager’s functionality and should
only be set when the user knows the inputs will not be uneven
(default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                ).
               </p>
              </li>
              <li>
               <p>
                <strong>
                 throw_on_early_termination
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                ) – a flag controlling whether to throw an
exception upon detecting uneven inputs (default:
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                ).
               </p>
              </li>
             </ul>
            </dd>
           </dl>
           <p>
            Example:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.nn.parallel.DistributedDataParallel</span> <span class="k">as</span> <span class="nn">DDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed.optim.ZeroRedundancyOptimizer</span> <span class="k">as</span> <span class="nn">ZeRO</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.algorithms.join</span> <span class="kn">import</span> <span class="n">Join</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># On each spawned worker</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optim</span> <span class="o">=</span> <span class="n">ZeRO</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Rank 1 gets one more input than rank 0</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">Join</span><span class="p">([</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">]):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># All ranks reach here without hanging/erroring</span>
</pre>
            </div>
           </div>
           <dl class="py method">
            <dt id="torch.distributed.algorithms.Join.notify_join_context">
             <em class="property">
              <span class="pre">
               static
              </span>
             </em>
             <code class="sig-name descname">
              <span class="pre">
               notify_join_context
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                joinable
               </span>
              </span>
             </em>
             <span class="sig-paren">
              )
             </span>
             <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#Join.notify_join_context">
              <span class="viewcode-link">
               <span class="pre">
                [source]
               </span>
              </span>
             </a>
             <a class="headerlink" href="#torch.distributed.algorithms.Join.notify_join_context" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Notifies the join context manager that the calling process has not yet
joined; then, if
              <code class="docutils literal notranslate">
               <span class="pre">
                throw_on_early_termination=True
               </span>
              </code>
              , checks if uneven
inputs have been detected (i.e. if one process has already joined) and
throws an exception if so.
             </p>
             <p>
              This method should be called from a
              <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Joinable
                </span>
               </code>
              </a>
              object before
its per-iteration collective communications. For example, this should
be called at the beginning of the forward pass in
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                DistributedDataParallel
               </span>
              </code>
              .
             </p>
             <p>
              Only the first
              <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Joinable
                </span>
               </code>
              </a>
              object passed into the context
manager performs the collective communications in this method, and
for the others, this method is vacuous.
             </p>
             <dl class="field-list simple">
              <dt class="field-odd">
               Parameters
              </dt>
              <dd class="field-odd">
               <p>
                <strong>
                 joinable
                </strong>
                (
                <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
                 <em>
                  Joinable
                 </em>
                </a>
                ) – the
                <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   Joinable
                  </span>
                 </code>
                </a>
                object calling this
method.
               </p>
              </dd>
              <dt class="field-even">
               Returns
              </dt>
              <dd class="field-even">
               <p>
                An async work handle for the all-reduce meant to notify the context
manager that the process has not yet joined if
                <code class="docutils literal notranslate">
                 <span class="pre">
                  joinable
                 </span>
                </code>
                is the
first one passed into the context manager;
                <code class="docutils literal notranslate">
                 <span class="pre">
                  None
                 </span>
                </code>
                otherwise.
               </p>
              </dd>
             </dl>
            </dd>
           </dl>
          </dd>
         </dl>
         <dl class="py class">
          <dt id="torch.distributed.algorithms.Joinable">
           <em class="property">
            <span class="pre">
             class
            </span>
           </em>
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.distributed.algorithms.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             Joinable
            </span>
           </code>
           <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#Joinable">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.distributed.algorithms.Joinable" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            This defines an abstract base class for joinable classes. A joinable class
(inheriting from
            <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               Joinable
              </span>
             </code>
            </a>
            ) should implement
            <a class="reference internal" href="#torch.distributed.algorithms.Joinable.join_hook" title="torch.distributed.algorithms.Joinable.join_hook">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               join_hook()
              </span>
             </code>
            </a>
            ,
which returns a
            <a class="reference internal" href="#torch.distributed.algorithms.JoinHook" title="torch.distributed.algorithms.JoinHook">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               JoinHook
              </span>
             </code>
            </a>
            instance, in addition to
            <a class="reference internal" href="#torch.distributed.algorithms.Joinable.join_device" title="torch.distributed.algorithms.Joinable.join_device">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               join_device()
              </span>
             </code>
            </a>
            and
            <a class="reference internal" href="#torch.distributed.algorithms.Joinable.join_process_group" title="torch.distributed.algorithms.Joinable.join_process_group">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               join_process_group()
              </span>
             </code>
            </a>
            that return device and
process group information, respectively.
           </p>
           <dl class="py method">
            <dt id="torch.distributed.algorithms.Joinable.join_device">
             <em class="property">
              <span class="pre">
               abstract
              </span>
              <span class="pre">
               property
              </span>
             </em>
             <code class="sig-name descname">
              <span class="pre">
               join_device
              </span>
             </code>
             <a class="headerlink" href="#torch.distributed.algorithms.Joinable.join_device" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Returns the device from which to perform collective communications
needed by the join context manager implementation itself.
             </p>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.distributed.algorithms.Joinable.join_hook">
             <em class="property">
              <span class="pre">
               abstract
              </span>
             </em>
             <code class="sig-name descname">
              <span class="pre">
               join_hook
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <em class="sig-param">
              <span class="o">
               <span class="pre">
                **
               </span>
              </span>
              <span class="n">
               <span class="pre">
                kwargs
               </span>
              </span>
             </em>
             <span class="sig-paren">
              )
             </span>
             <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#Joinable.join_hook">
              <span class="viewcode-link">
               <span class="pre">
                [source]
               </span>
              </span>
             </a>
             <a class="headerlink" href="#torch.distributed.algorithms.Joinable.join_hook" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Returns a
              <a class="reference internal" href="#torch.distributed.algorithms.JoinHook" title="torch.distributed.algorithms.JoinHook">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 JoinHook
                </span>
               </code>
              </a>
              instance for the given
              <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Joinable
                </span>
               </code>
              </a>
              .
             </p>
             <dl class="field-list simple">
              <dt class="field-odd">
               Parameters
              </dt>
              <dd class="field-odd">
               <p>
                <strong>
                 kwargs
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
                 <em>
                  dict
                 </em>
                </a>
                ) – a
                <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   dict
                  </span>
                 </code>
                </a>
                containing any keyword arguments
to modify the behavior of the join hook at run time; all
                <a class="reference internal" href="#torch.distributed.algorithms.Joinable" title="torch.distributed.algorithms.Joinable">
                 <code class="xref py py-class docutils literal notranslate">
                  <span class="pre">
                   Joinable
                  </span>
                 </code>
                </a>
                instances sharing the same join context
manager are forwarded the same value for
                <code class="docutils literal notranslate">
                 <span class="pre">
                  kwargs
                 </span>
                </code>
                .
               </p>
              </dd>
             </dl>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.distributed.algorithms.Joinable.join_process_group">
             <em class="property">
              <span class="pre">
               abstract
              </span>
              <span class="pre">
               property
              </span>
             </em>
             <code class="sig-name descname">
              <span class="pre">
               join_process_group
              </span>
             </code>
             <a class="headerlink" href="#torch.distributed.algorithms.Joinable.join_process_group" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              Returns the process group for the collective communications needed by
the join context manager itself.
             </p>
            </dd>
           </dl>
          </dd>
         </dl>
         <dl class="py class">
          <dt id="torch.distributed.algorithms.JoinHook">
           <em class="property">
            <span class="pre">
             class
            </span>
           </em>
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.distributed.algorithms.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             JoinHook
            </span>
           </code>
           <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#JoinHook">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.distributed.algorithms.JoinHook" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            This defines a join hook, which provides two entry points in the join
context manager: a main hook, which is called repeatedly while there exists
a non-joined process, and a post-hook, which is called once all processes
have joined.
           </p>
           <p>
            To implement a join hook for the generic join context manager, define a
class that inherits from
            <a class="reference internal" href="#torch.distributed.algorithms.JoinHook" title="torch.distributed.algorithms.JoinHook">
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               JoinHook
              </span>
             </code>
            </a>
            and override
            <code class="docutils literal notranslate">
             <span class="pre">
              main_hook()
             </span>
            </code>
            and
            <code class="docutils literal notranslate">
             <span class="pre">
              post_hook()
             </span>
            </code>
            as appropriate.
           </p>
           <dl class="py method">
            <dt id="torch.distributed.algorithms.JoinHook.main_hook">
             <code class="sig-name descname">
              <span class="pre">
               main_hook
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <span class="sig-paren">
              )
             </span>
             <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#JoinHook.main_hook">
              <span class="viewcode-link">
               <span class="pre">
                [source]
               </span>
              </span>
             </a>
             <a class="headerlink" href="#torch.distributed.algorithms.JoinHook.main_hook" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              This hook is called repeatedly while there exists a non-joined process
to shadow collective communications in one training iteration (i.e. in
one forward pass, backward pass, and optimizer step).
             </p>
            </dd>
           </dl>
           <dl class="py method">
            <dt id="torch.distributed.algorithms.JoinHook.post_hook">
             <code class="sig-name descname">
              <span class="pre">
               post_hook
              </span>
             </code>
             <span class="sig-paren">
              (
             </span>
             <em class="sig-param">
              <span class="n">
               <span class="pre">
                is_last_joiner
               </span>
              </span>
             </em>
             <span class="sig-paren">
              )
             </span>
             <a class="reference internal" href="_modules/torch/distributed/algorithms/join.html#JoinHook.post_hook">
              <span class="viewcode-link">
               <span class="pre">
                [source]
               </span>
              </span>
             </a>
             <a class="headerlink" href="#torch.distributed.algorithms.JoinHook.post_hook" title="Permalink to this definition">
              ¶
             </a>
            </dt>
            <dd>
             <p>
              This hook is called after all processes have joined. It is passed an
additional
              <code class="docutils literal notranslate">
               <span class="pre">
                bool
               </span>
              </code>
              argument
              <code class="docutils literal notranslate">
               <span class="pre">
                is_last_joiner
               </span>
              </code>
              , which indicates if the
rank is one of the last to join.
             </p>
             <dl class="field-list simple">
              <dt class="field-odd">
               Parameters
              </dt>
              <dd class="field-odd">
               <p>
                <strong>
                 is_last_joiner
                </strong>
                (
                <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">
                 <em>
                  bool
                 </em>
                </a>
                ) –
                <code class="docutils literal notranslate">
                 <span class="pre">
                  True
                 </span>
                </code>
                if the rank is one of the last to
join;
                <code class="docutils literal notranslate">
                 <span class="pre">
                  False
                 </span>
                </code>
                otherwise.
               </p>
              </dd>
             </dl>
            </dd>
           </dl>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>