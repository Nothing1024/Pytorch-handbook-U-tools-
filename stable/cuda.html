<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.cuda — PyTorch 1.12 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="module-torch.cuda">
         <span id="torch-cuda">
         </span>
         <h1>
          torch.cuda
          <a class="headerlink" href="#module-torch.cuda" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <p>
          This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.
         </p>
         <p>
          It is lazily initialized, so you can always import it, and use
          <a class="reference internal" href="generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available">
           <code class="xref py py-func docutils literal notranslate">
            <span class="pre">
             is_available()
            </span>
           </code>
          </a>
          to determine if your system supports CUDA.
         </p>
         <p>
          <a class="reference internal" href="notes/cuda.html#cuda-semantics">
           <span class="std std-ref">
            CUDA semantics
           </span>
          </a>
          has more details about working with CUDA.
         </p>
         <table class="longtable docutils colwidths-auto align-default">
          <tbody>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.StreamContext">
              </p>
              <a class="reference internal" href="generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext" title="torch.cuda.StreamContext">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 StreamContext
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Context-manager that selects a given stream.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.can_device_access_peer">
              </p>
              <a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer" title="torch.cuda.can_device_access_peer">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 can_device_access_peer
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Checks if peer access between two devices is possible.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.current_blas_handle">
              </p>
              <a class="reference internal" href="generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle" title="torch.cuda.current_blas_handle">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 current_blas_handle
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns cublasHandle_t pointer to current cuBLAS handle
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.current_device">
              </p>
              <a class="reference internal" href="generated/torch.cuda.current_device.html#torch.cuda.current_device" title="torch.cuda.current_device">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 current_device
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns the index of a currently selected device.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.current_stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.current_stream.html#torch.cuda.current_stream" title="torch.cuda.current_stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 current_stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns the currently selected
              <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Stream
                </span>
               </code>
              </a>
              for a given device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.default_stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.default_stream.html#torch.cuda.default_stream" title="torch.cuda.default_stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 default_stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns the default
              <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Stream
                </span>
               </code>
              </a>
              for a given device.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.device">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device.html#torch.cuda.device" title="torch.cuda.device">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Context-manager that changes the selected device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.device_count">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device_count
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns the number of GPUs available.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.device_of">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device_of.html#torch.cuda.device_of" title="torch.cuda.device_of">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device_of
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Context-manager that changes the current device to that of given object.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_arch_list">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list" title="torch.cuda.get_arch_list">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_arch_list
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns list CUDA architectures this library was compiled for.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.get_device_capability">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability" title="torch.cuda.get_device_capability">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_device_capability
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Gets the cuda capability of a device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_device_name">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name" title="torch.cuda.get_device_name">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_device_name
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Gets the name of a device.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.get_device_properties">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties" title="torch.cuda.get_device_properties">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_device_properties
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Gets the properties of a device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_gencode_flags">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags" title="torch.cuda.get_gencode_flags">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_gencode_flags
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns NVCC gencode flags this library was compiled with.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.get_sync_debug_mode">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode" title="torch.cuda.get_sync_debug_mode">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_sync_debug_mode
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns current value of debug mode for cuda synchronizing operations.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.init">
              </p>
              <a class="reference internal" href="generated/torch.cuda.init.html#torch.cuda.init" title="torch.cuda.init">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 init
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Initialize PyTorch’s CUDA state.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.ipc_collect">
              </p>
              <a class="reference internal" href="generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect" title="torch.cuda.ipc_collect">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 ipc_collect
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Force collects GPU memory after it has been released by CUDA IPC.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.is_available">
              </p>
              <a class="reference internal" href="generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 is_available
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns a bool indicating if CUDA is currently available.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.is_initialized">
              </p>
              <a class="reference internal" href="generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized" title="torch.cuda.is_initialized">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 is_initialized
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns whether PyTorch’s CUDA state has been initialized.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.memory_usage">
              </p>
              <a class="reference internal" href="generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage" title="torch.cuda.memory_usage">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 memory_usage
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns the percent of time over the past sample period during which global (device) memory was being read or written.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.set_device">
              </p>
              <a class="reference internal" href="generated/torch.cuda.set_device.html#torch.cuda.set_device" title="torch.cuda.set_device">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 set_device
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Sets the current device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.set_stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.set_stream.html#torch.cuda.set_stream" title="torch.cuda.set_stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 set_stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Sets the current stream.This is a wrapper API to set the stream.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.set_sync_debug_mode">
              </p>
              <a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode" title="torch.cuda.set_sync_debug_mode">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 set_sync_debug_mode
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Sets the debug mode for cuda synchronizing operations.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.stream.html#torch.cuda.stream" title="torch.cuda.stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Wrapper around the Context-manager StreamContext that selects a given stream.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.synchronize">
              </p>
              <a class="reference internal" href="generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 synchronize
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Waits for all kernels in all streams on a CUDA device to complete.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.utilization">
              </p>
              <a class="reference internal" href="generated/torch.cuda.utilization.html#torch.cuda.utilization" title="torch.cuda.utilization">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 utilization
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by
              <cite>
               nvidia-smi
              </cite>
              .
             </p>
            </td>
           </tr>
          </tbody>
         </table>
         <div class="section" id="random-number-generator">
          <h2>
           Random Number Generator
           <a class="headerlink" href="#random-number-generator" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.get_rng_state">
               </p>
               <a class="reference internal" href="generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state" title="torch.cuda.get_rng_state">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_rng_state
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the random number generator state of the specified GPU as a ByteTensor.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.get_rng_state_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all" title="torch.cuda.get_rng_state_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_rng_state_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a list of ByteTensor representing the random number states of all devices.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.set_rng_state">
               </p>
               <a class="reference internal" href="generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state" title="torch.cuda.set_rng_state">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  set_rng_state
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the random number generator state of the specified GPU.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.set_rng_state_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all" title="torch.cuda.set_rng_state_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  set_rng_state_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the random number generator state of all devices.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.manual_seed">
               </p>
               <a class="reference internal" href="generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed" title="torch.cuda.manual_seed">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  manual_seed
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the seed for generating random numbers for the current GPU.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.manual_seed_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  manual_seed_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the seed for generating random numbers on all GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.seed">
               </p>
               <a class="reference internal" href="generated/torch.cuda.seed.html#torch.cuda.seed" title="torch.cuda.seed">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  seed
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the seed for generating random numbers to a random number for the current GPU.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.seed_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.seed_all.html#torch.cuda.seed_all" title="torch.cuda.seed_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  seed_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sets the seed for generating random numbers to a random number on all GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.initial_seed">
               </p>
               <a class="reference internal" href="generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed" title="torch.cuda.initial_seed">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  initial_seed
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the current random seed of the current GPU.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="communication-collectives">
          <h2>
           Communication collectives
           <a class="headerlink" href="#communication-collectives" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast" title="torch.cuda.comm.broadcast">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.broadcast
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Broadcasts a tensor to specified GPU devices.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced" title="torch.cuda.comm.broadcast_coalesced">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.broadcast_coalesced
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Broadcasts a sequence tensors to the specified GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add" title="torch.cuda.comm.reduce_add">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.reduce_add
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sums tensors from multiple GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter" title="torch.cuda.comm.scatter">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.scatter
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Scatters tensor across multiple GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather" title="torch.cuda.comm.gather">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.gather
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Gathers tensors from multiple GPU devices.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="streams-and-events">
          <h2>
           Streams and events
           <a class="headerlink" href="#streams-and-events" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.Stream">
               </p>
               <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Stream
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around a CUDA stream.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.ExternalStream">
               </p>
               <a class="reference internal" href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream" title="torch.cuda.ExternalStream">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ExternalStream
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around an externally allocated CUDA stream.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.Event">
               </p>
               <a class="reference internal" href="generated/torch.cuda.Event.html#torch.cuda.Event" title="torch.cuda.Event">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Event
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around a CUDA event.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="graphs-beta">
          <h2>
           Graphs (beta)
           <a class="headerlink" href="#graphs-beta" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.is_current_stream_capturing">
               </p>
               <a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing" title="torch.cuda.is_current_stream_capturing">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  is_current_stream_capturing
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.graph_pool_handle">
               </p>
               <a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle" title="torch.cuda.graph_pool_handle">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  graph_pool_handle
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns an opaque token representing the id of a graph memory pool.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.CUDAGraph">
               </p>
               <a class="reference internal" href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  CUDAGraph
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around a CUDA graph.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.graph">
               </p>
               <a class="reference internal" href="generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  graph
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Context-manager that captures CUDA work into a
               <a class="reference internal" href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  torch.cuda.CUDAGraph
                 </span>
                </code>
               </a>
               object for later replay.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.make_graphed_callables">
               </p>
               <a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  make_graphed_callables
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Accepts callables (functions or
               <a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  nn.Module
                 </span>
                </code>
               </a>
               s) and returns graphed versions.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="memory-management">
          <h2>
           Memory management
           <a class="headerlink" href="#memory-management" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.empty_cache">
               </p>
               <a class="reference internal" href="generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache" title="torch.cuda.empty_cache">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  empty_cache
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in
               <cite>
                nvidia-smi
               </cite>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.list_gpu_processes">
               </p>
               <a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes" title="torch.cuda.list_gpu_processes">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  list_gpu_processes
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a human-readable printout of the running processes and their GPU memory use for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.mem_get_info">
               </p>
               <a class="reference internal" href="generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info" title="torch.cuda.mem_get_info">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  mem_get_info
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the global free and total GPU memory occupied for a given device using cudaMemGetInfo.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.memory_stats">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats" title="torch.cuda.memory_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a dictionary of CUDA memory allocator statistics for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_summary">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary" title="torch.cuda.memory_summary">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_summary
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a human-readable printout of the current memory allocator statistics for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.memory_snapshot">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot" title="torch.cuda.memory_snapshot">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_snapshot
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a snapshot of the CUDA memory allocator state across all devices.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_allocated">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated" title="torch.cuda.memory_allocated">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_allocated
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the current GPU memory occupied by tensors in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.max_memory_allocated">
               </p>
               <a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_memory_allocated
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the maximum GPU memory occupied by tensors in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.reset_max_memory_allocated">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated" title="torch.cuda.reset_max_memory_allocated">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_max_memory_allocated
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.memory_reserved">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_reserved
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the current GPU memory managed by the caching allocator in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.max_memory_reserved">
               </p>
               <a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_memory_reserved
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.set_per_process_memory_fraction">
               </p>
               <a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction" title="torch.cuda.set_per_process_memory_fraction">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  set_per_process_memory_fraction
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set memory fraction for a process.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_cached">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached" title="torch.cuda.memory_cached">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_cached
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Deprecated; see
               <a class="reference internal" href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  memory_reserved()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.max_memory_cached">
               </p>
               <a class="reference internal" href="generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_memory_cached
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Deprecated; see
               <a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  max_memory_reserved()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.reset_max_memory_cached">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached" title="torch.cuda.reset_max_memory_cached">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_max_memory_cached
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.reset_peak_memory_stats">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats" title="torch.cuda.reset_peak_memory_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_peak_memory_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Resets the “peak” stats tracked by the CUDA memory allocator.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.caching_allocator_alloc">
               </p>
               <a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc" title="torch.cuda.caching_allocator_alloc">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  caching_allocator_alloc
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Performs a memory allocation using the CUDA memory allocator.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.caching_allocator_delete">
               </p>
               <a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete" title="torch.cuda.caching_allocator_delete">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  caching_allocator_delete
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Deletes memory allocated using the CUDA memory allocator.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="nvidia-tools-extension-nvtx">
          <h2>
           NVIDIA Tools Extension (NVTX)
           <a class="headerlink" href="#nvidia-tools-extension-nvtx" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark" title="torch.cuda.nvtx.mark">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.mark
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Describe an instantaneous event that occurred at some point.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push" title="torch.cuda.nvtx.range_push">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.range_push
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Pushes a range onto a stack of nested range span.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop" title="torch.cuda.nvtx.range_pop">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.range_pop
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Pops a range off of a stack of nested range spans.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="jiterator-beta">
          <h2>
           Jiterator (beta)
           <a class="headerlink" href="#jiterator-beta" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <table class="longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn" title="torch.cuda.jiterator._create_jit_fn">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  jiterator._create_jit_fn
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Create a jiterator-generated cuda kernel for an elementwise op.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>