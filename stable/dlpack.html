<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.utils.dlpack — PyTorch 1.11.0 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- End Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="torch-utils-dlpack">
         <h1>
          torch.utils.dlpack
          <a class="headerlink" href="#torch-utils-dlpack" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <dl class="py function">
          <dt id="torch.utils.dlpack.from_dlpack">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.utils.dlpack.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             from_dlpack
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              ext_tensor
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           →
           <span class="pre">
            Tensor
           </span>
           <a class="reference internal" href="_modules/torch/utils/dlpack.html#from_dlpack">
            <span class="viewcode-link">
             <span class="pre">
              [source]
             </span>
            </span>
           </a>
           <a class="headerlink" href="#torch.utils.dlpack.from_dlpack" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Converts a tensor from an external library into a
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.Tensor
             </span>
            </code>
            .
           </p>
           <p>
            The returned PyTorch tensor will share the memory with the input tensor
(which may have come from another library). Note that in-place operations
will therefore also affect the data of the input tensor. This may lead to
unexpected issues (e.g., other libraries may have read-only flags or
immutable data structures), so the user should only do this if they know
for sure that this is fine.
           </p>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <p>
              <strong>
               ext_tensor
              </strong>
              (object with
              <code class="docutils literal notranslate">
               <span class="pre">
                __dlpack__
               </span>
              </code>
              attribute, or a DLPack capsule) –
              <p>
               The tensor or DLPack capsule to convert.
              </p>
              <p>
               If
               <code class="docutils literal notranslate">
                <span class="pre">
                 ext_tensor
                </span>
               </code>
               is a tensor (or ndarray) object, it must support
the
               <code class="docutils literal notranslate">
                <span class="pre">
                 __dlpack__
                </span>
               </code>
               protocol (i.e., have a
               <code class="docutils literal notranslate">
                <span class="pre">
                 ext_tensor.__dlpack__
                </span>
               </code>
               method). Otherwise
               <code class="docutils literal notranslate">
                <span class="pre">
                 ext_tensor
                </span>
               </code>
               may be a DLPack capsule, which is
an opaque
               <code class="docutils literal notranslate">
                <span class="pre">
                 PyCapsule
                </span>
               </code>
               instance, typically produced by a
               <code class="docutils literal notranslate">
                <span class="pre">
                 to_dlpack
                </span>
               </code>
               function or method.
              </p>
             </p>
            </dd>
           </dl>
           <p>
            Examples:
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="go"># Convert a tensor directly (supported in PyTorch &gt;= 1.10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># show that memory is shared</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span>
<span class="go">tensor([-1, -1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span>
<span class="go">tensor([-1, -1,  2,  3])</span>

<span class="go"># The old-style DLPack usage, with an intermediate capsule object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">capsule</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">capsule</span>
<span class="go">&lt;capsule object "dltensor" at 0x7f6017d14300&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">capsule</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t3</span>
<span class="go">tensor([-1, -1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t3</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">9</span>  <span class="c1"># now we're sharing memory between 3 tensors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t3</span>
<span class="go">tensor([-9, -1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span>
<span class="go">tensor([-9, -1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span>
<span class="go">tensor([-9, -1,  2,  3])</span>
</pre>
            </div>
           </div>
          </dd>
         </dl>
         <dl class="py function">
          <dt id="torch.utils.dlpack.to_dlpack">
           <code class="sig-prename descclassname">
            <span class="pre">
             torch.utils.dlpack.
            </span>
           </code>
           <code class="sig-name descname">
            <span class="pre">
             to_dlpack
            </span>
           </code>
           <span class="sig-paren">
            (
           </span>
           <em class="sig-param">
            <span class="n">
             <span class="pre">
              tensor
             </span>
            </span>
           </em>
           <span class="sig-paren">
            )
           </span>
           →
           <span class="pre">
            PyCapsule
           </span>
           <a class="headerlink" href="#torch.utils.dlpack.to_dlpack" title="Permalink to this definition">
            ¶
           </a>
          </dt>
          <dd>
           <p>
            Returns an opaque object (a “DLPack capsule”) representing the tensor.
           </p>
           <div class="admonition note">
            <p class="admonition-title">
             Note
            </p>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               to_dlpack
              </span>
             </code>
             is a legacy DLPack interface. The capsule it returns
cannot be used for anything in Python other than use it as input to
             <code class="docutils literal notranslate">
              <span class="pre">
               from_dlpack
              </span>
             </code>
             . The more idiomatic use of DLPack is to call
             <code class="docutils literal notranslate">
              <span class="pre">
               from_dlpack
              </span>
             </code>
             directly on the tensor object - this works when that
object has a
             <code class="docutils literal notranslate">
              <span class="pre">
               __dlpack__
              </span>
             </code>
             method, which PyTorch and most other
libraries indeed have now.
            </p>
           </div>
           <div class="admonition warning">
            <p class="admonition-title">
             Warning
            </p>
            <p>
             Only call
             <code class="docutils literal notranslate">
              <span class="pre">
               from_dlpack
              </span>
             </code>
             once per capsule produced with
             <code class="docutils literal notranslate">
              <span class="pre">
               to_dlpack
              </span>
             </code>
             .
Behavior when a capsule is consumed multiple times is undefined.
            </p>
           </div>
           <dl class="field-list simple">
            <dt class="field-odd">
             Parameters
            </dt>
            <dd class="field-odd">
             <p>
              <strong>
               tensor
              </strong>
              – a tensor to be exported
             </p>
            </dd>
           </dl>
           <p>
            The DLPack capsule shares the tensor’s memory.
           </p>
          </dd>
         </dl>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>