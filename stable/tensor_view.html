<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Tensor Views — PyTorch 1.10 documentation
  </title>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <!-- Google Analytics -->
  <!-- Preload the theme fonts -->
  <!-- Preload the katex fonts -->
 </head>
 <body class="pytorch-body">
  <div class="pytorch-container">
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="tensor-views">
         <span id="tensor-view-doc">
         </span>
         <h1>
          Tensor Views
          <a class="headerlink" href="#tensor-views" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <p>
          PyTorch allows a tensor to be a
          <code class="docutils literal notranslate">
           <span class="pre">
            View
           </span>
          </code>
          of an existing tensor. View tensor shares the same underlying data
with its base tensor. Supporting
          <code class="docutils literal notranslate">
           <span class="pre">
            View
           </span>
          </code>
          avoids explicit data copy, thus allows us to do fast and memory efficient
reshaping, slicing and element-wise operations.
         </p>
         <p>
          For example, to get a view of an existing tensor
          <code class="docutils literal notranslate">
           <span class="pre">
            t
           </span>
          </code>
          , you can call
          <code class="docutils literal notranslate">
           <span class="pre">
            t.view(...)
           </span>
          </code>
          .
         </p>
         <div class="highlight-default notranslate">
          <div class="highlight">
           <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>  <span class="c1"># `t` and `b` share the same underlying data.</span>
<span class="go">True</span>
<span class="go"># Modifying view tensor changes base tensor as well.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.14</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="go">tensor(3.14)</span>
</pre>
          </div>
         </div>
         <p>
          Since views share underlying data with its base tensor, if you edit the data
in the view, it will be reflected in the base tensor as well.
         </p>
         <p>
          Typically a PyTorch op returns a new tensor as output, e.g.
          <a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add">
           <code class="xref py py-meth docutils literal notranslate">
            <span class="pre">
             add()
            </span>
           </code>
          </a>
          .
But in case of view ops, outputs are views of input tensors to avoid unncessary data copy.
No data movement occurs when creating a view, view tensor just changes the way
it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.
Users should be pay additional attention as contiguity might have implicit performance impact.
          <a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose">
           <code class="xref py py-meth docutils literal notranslate">
            <span class="pre">
             transpose()
            </span>
           </code>
          </a>
          is a common example.
         </p>
         <div class="highlight-default notranslate">
          <div class="highlight">
           <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">base</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># `t` is a view of `base`. No data movement happened here.</span>
<span class="go"># View tensors might be non-contiguous.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
<span class="go"># To get a contiguous tensor, call `.contiguous()` to enforce</span>
<span class="go"># copying data when `t` is not contiguous.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</pre>
          </div>
         </div>
         <p>
          For reference, here’s a full list of view ops in PyTorch:
         </p>
         <ul class="simple">
          <li>
           <p>
            Basic slicing and indexing op, e.g.
            <code class="docutils literal notranslate">
             <span class="pre">
              tensor[0,
             </span>
             <span class="pre">
              2:,
             </span>
             <span class="pre">
              1:7:2]
             </span>
            </code>
            returns a view of base
            <code class="docutils literal notranslate">
             <span class="pre">
              tensor
             </span>
            </code>
            , see note below.
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" title="torch.Tensor.as_strided">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               as_strided()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               detach()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal" title="torch.Tensor.diagonal">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               diagonal()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               expand()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as" title="torch.Tensor.expand_as">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               expand_as()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.movedim.html#torch.Tensor.movedim" title="torch.Tensor.movedim">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               movedim()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.narrow.html#torch.Tensor.narrow" title="torch.Tensor.narrow">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               narrow()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.permute.html#torch.Tensor.permute" title="torch.Tensor.permute">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               permute()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.select.html#torch.Tensor.select" title="torch.Tensor.select">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               select()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" title="torch.Tensor.squeeze">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               squeeze()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               transpose()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.t.html#torch.Tensor.t" title="torch.Tensor.t">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               t()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="tensors.html#torch.Tensor.T" title="torch.Tensor.T">
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               T
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <p id="torch.Tensor.real">
            </p>
            <a class="reference internal" href="generated/torch.Tensor.real.html#torch.Tensor.real" title="torch.Tensor.real">
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               real
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <p id="torch.Tensor.imag">
            </p>
            <a class="reference internal" href="generated/torch.Tensor.imag.html#torch.Tensor.imag" title="torch.Tensor.imag">
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               imag
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <code class="xref py py-meth docutils literal notranslate">
             <span class="pre">
              view_as_real()
             </span>
            </code>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="named_tensor.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               unflatten()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.unfold.html#torch.Tensor.unfold" title="torch.Tensor.unfold">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               unfold()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               unsqueeze()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.view.html#torch.Tensor.view" title="torch.Tensor.view">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               view()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.view_as.html#torch.Tensor.view_as" title="torch.Tensor.view_as">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               view_as()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.unbind.html#torch.Tensor.unbind" title="torch.Tensor.unbind">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               unbind()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.split.html#torch.Tensor.split" title="torch.Tensor.split">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               split()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit" title="torch.Tensor.hsplit">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               hsplit()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit" title="torch.Tensor.vsplit">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               vsplit()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split" title="torch.Tensor.tensor_split">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               tensor_split()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <code class="xref py py-meth docutils literal notranslate">
             <span class="pre">
              split_with_sizes()
             </span>
            </code>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes" title="torch.Tensor.swapaxes">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               swapaxes()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims" title="torch.Tensor.swapdims">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               swapdims()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.chunk.html#torch.Tensor.chunk" title="torch.Tensor.chunk">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               chunk()
              </span>
             </code>
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.indices.html#torch.Tensor.indices" title="torch.Tensor.indices">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               indices()
              </span>
             </code>
            </a>
            (sparse tensor only)
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.values.html#torch.Tensor.values" title="torch.Tensor.values">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               values()
              </span>
             </code>
            </a>
            (sparse tensor only)
           </p>
          </li>
         </ul>
         <div class="admonition note">
          <p class="admonition-title">
           Note
          </p>
          <p>
           When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors
that basic indexing returns views, while advanced indexing returns a copy.
Assignment via either basic or advanced indexing is in-place. See more examples in
           <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html">
            Numpy indexing documentation
           </a>
           .
          </p>
         </div>
         <p>
          It’s also worth mentioning a few ops with special behaviors:
         </p>
         <ul class="simple">
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.reshape.html#torch.Tensor.reshape" title="torch.Tensor.reshape">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               reshape()
              </span>
             </code>
            </a>
            ,
            <a class="reference internal" href="generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as" title="torch.Tensor.reshape_as">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               reshape_as()
              </span>
             </code>
            </a>
            and
            <a class="reference internal" href="generated/torch.Tensor.flatten.html#torch.Tensor.flatten" title="torch.Tensor.flatten">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               flatten()
              </span>
             </code>
            </a>
            can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not.
           </p>
          </li>
          <li>
           <p>
            <a class="reference internal" href="generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous" title="torch.Tensor.contiguous">
             <code class="xref py py-meth docutils literal notranslate">
              <span class="pre">
               contiguous()
              </span>
             </code>
            </a>
            returns
            <strong>
             itself
            </strong>
            if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.
           </p>
          </li>
         </ul>
         <p>
          For a more detailed walk-through of PyTorch internal implementation,
please refer to
          <a class="reference external" href="http://blog.ezyang.com/2019/05/pytorch-internals/">
           ezyang’s blogpost about PyTorch Internals
          </a>
          .
         </p>
        </div>
       </article>
      </div>
     </div>
    </div>
   </section>
  </div>
 </body>
</html>